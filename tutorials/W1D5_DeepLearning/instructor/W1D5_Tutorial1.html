
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 1: Decoding Neural Responses — Neuromatch Academy: Computational Neuroscience (instructor's version)</title>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/custom.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-logo-square-4xp.png" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="W1D5_Tutorial2.html" rel="next" title="Tutorial 2: Convolutional Neural Networks"/>
<link href="W1D5_Intro.html" rel="prev" title="Intro"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../../../_static/nma-logo-square-4xp.png"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Computational Neuroscience (instructor's version)</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</input></li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using discord
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../TechnicalHelp/Links_Policy.html">
   Quick links and policies
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../prereqs/ComputationalNeuroscience.html">
   Prerequisites and preparatory materials for NMA Computational Neuroscience
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../tatraining/TA_Training_CN.html">
   TA Training - Computational Neuroscience
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Pre-reqs Refresher
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/chapter_title.html">
   Neuro Video Series (W0D0)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial1.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial2.html">
     Human Psychophysics
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial3.html">
     Behavioral Readout
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial4.html">
     Live in Lab
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial5.html">
     Brain Signals: Spiking Activity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial6.html">
     Brain Signals: LFP
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial7.html">
     Brain Signals: EEG &amp; MEG
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial8.html">
     Brain Signals: fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial9.html">
     Brain Signals: Calcium Imaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial10.html">
     Stimulus Representation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial11.html">
     Neurotransmitters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial12.html">
     Neurons to Consciousness
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/chapter_title.html">
   Python Workshop 1 (W0D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/instructor/W0D1_Tutorial1.html">
     Tutorial: LIF Neuron Part I
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/chapter_title.html">
   Python Workshop 2 (W0D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/instructor/W0D2_Tutorial1.html">
     Tutorial 1: LIF Neuron Part II
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D3_LinearAlgebra/chapter_title.html">
   Linear Algebra (W0D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial1.html">
     Tutorial 1: Vectors
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial2.html">
     Tutorial 2: Matrices
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial3.html">
     Bonus Tutorial: Discrete Dynamical Systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D4_Calculus/chapter_title.html">
   Calculus (W0D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial1.html">
     Tutorial 1: Differentiation and Integration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial2.html">
     Tutorial 2: Differential Equations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial3.html">
     Tutorial 3: Numerical Methods
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D5_Statistics/chapter_title.html">
   Statistics (W0D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_Tutorial1.html">
     Tutorial 1: Probability Distributions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_Tutorial2.html">
     Tutorial 2: Statistical Inference
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Intro to Modeling
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_ModelTypes/chapter_title.html">
   Model Types (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial1.html">
     Tutorial 1: “What” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial2.html">
     Tutorial 2: “How” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial3.html">
     Tutorial 3: “Why” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial4.html">
     Tutorial 4: Model Discussions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ModelingPractice/chapter_title.html">
   Modeling Practice (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Tutorial1.html">
     Tutorial 1: Framing the Question
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_ModelFitting/chapter_title.html">
   Model Fitting (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial1.html">
     Tutorial 1: Linear regression with MSE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial2.html">
     Tutorial 2: Linear regression with MLE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial3.html">
     Tutorial 3: Confidence intervals and bootstrapping
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial4.html">
     Tutorial 4: Multiple linear regression and polynomial regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial5.html">
     Tutorial 5: Model Selection: Bias-variance trade-off
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial6.html">
     Tutorial 6: Model Selection: Cross-validation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/chapter_title.html">
   Generalized Linear Models (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial1.html">
     Tutorial 1: GLMs for Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial2.html">
     Tutorial 2: Classifiers and regularizers
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/chapter_title.html">
   Dimensionality Reduction (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial1.html">
     Tutorial 1: Geometric view of data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial2.html">
     Tutorial 2: Principal Component Analysis
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial3.html">
     Tutorial 3: Dimensionality Reduction &amp; Reconstruction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial4.html">
     Tutorial 4:  Nonlinear Dimensionality Reduction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Deep Learning (W1D5)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="W1D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 1: Decoding Neural Responses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W1D5_Tutorial2.html">
     Tutorial 2: Convolutional Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W1D5_Tutorial3.html">
     Tutorial 3: Building and Evaluating Normative Encoding Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W1D5_Tutorial4.html">
     Bonus Tutorial: Diving Deeper into Decoding &amp; Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W1D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W1D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Bonus_Autoencoders/chapter_title.html">
   Autoencoders (Bonus)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial1.html">
     Tutorial 1: Intro to Autoencoders
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial2.html">
     Tutorial 2: Autoencoder extensions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial3.html">
     Tutorial 3: Autoencoders applications
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/MachineLearning.html">
   Machine Learning Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Dynamical Systems
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_LinearSystems/chapter_title.html">
   Linear Systems (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial1.html">
     Tutorial 1: Linear dynamical systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial2.html">
     Tutorial 2: Markov Processes
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial3.html">
     Tutorial 3: Combining determinism and stochasticity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial4.html">
     Tutorial 4: Autoregressive models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/chapter_title.html">
   Biological Neuron Models (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial1.html">
     Tutorial 1: The Leaky Integrate-and-Fire (LIF) Neuron Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial2.html">
     Tutorial 2: Effects of Input Correlation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial3.html">
     Tutorial 3: Synaptic transmission - Models of static and dynamic synapses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial4.html">
     Bonus Tutorial: Spike-timing dependent plasticity (STDP)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_DynamicNetworks/chapter_title.html">
   Dynamic Networks (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial1.html">
     Tutorial 1: Neural Rate Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial2.html">
     Tutorial 2: Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial3.html">
     Bonus Tutorial: Extending the Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/DynamicalSystems.html">
   Dynamical Systems Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Stochastic Processes
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_BayesianDecisions/chapter_title.html">
   Bayesian Decisions (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial1.html">
     Tutorial 1: Bayes with a binary hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial2.html">
     Tutorial 2: Bayesian inference and decisions with continuous hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial3.html">
     Bonus Tutorial : Fitting to data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_HiddenDynamics/chapter_title.html">
   Hidden Dynamics (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial1.html">
     Tutorial 1: Sequential Probability Ratio Test
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial2.html">
     Tutorial 2: Hidden Markov Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial3.html">
     Tutorial 3: The Kalman Filter
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial4.html">
     Bonus Tutorial 4: The Kalman Filter, part 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial5.html">
     Bonus Tutorial 5: Expectation Maximization for spiking neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_OptimalControl/chapter_title.html">
   Optimal Control (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Tutorial1.html">
     Tutorial 1: Optimal Control for Discrete States
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Tutorial2.html">
     Tutorial 2: Optimal Control for Continuous State
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/chapter_title.html">
   Reinforcement Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial1.html">
     Tutorial 1: Learning to Predict
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial2.html">
     Tutorial 2: Learning to Act: Multi-Armed Bandits
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial3.html">
     Tutorial 3: Learning to Act: Q-Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial4.html">
     Tutorial 4: Model-Based Reinforcement Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D5_NetworkCausality/chapter_title.html">
   Network Causality (W3D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
<label for="toctree-checkbox-24">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial1.html">
     Tutorial 1: Interventions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial2.html">
     Tutorial 2: Correlations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial3.html">
     Tutorial 3: Simultaneous fitting/regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial4.html">
     Tutorial 4: Instrumental Variables
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/StochasticProcesses.html">
   Stochastic Processes Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
<label for="toctree-checkbox-25">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through4.html">
     Modeling Steps 1 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through10.html">
     Modeling Steps 5 - 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModel.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProject.html">
     Example Data Project: the Train Illusion
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/datasets_overview.html">
   Datasets
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
<label for="toctree-checkbox-26">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/neurons.html">
     Neurons
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
<label for="toctree-checkbox-27">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/neurons_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/fMRI.html">
     fMRI
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
<label for="toctree-checkbox-28">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/fMRI_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/ECoG.html">
     ECoG
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
<label for="toctree-checkbox-29">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/ECoG_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/behavior.html">
     Behavior
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
<label for="toctree-checkbox-30">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/behavior_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/theory.html">
     Theory
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
<label for="toctree-checkbox-31">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/theory/README.html">
       Guide
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_templates.html">
   Project Templates
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/project_2020_highlights.html">
   Projects 2020
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/>
<label for="toctree-checkbox-32">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/neurons.html">
     Neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/theory.html">
     Theory
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/behavior.html">
     Behavior
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/fMRI.html">
     fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/eeg.html">
     EEG
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
<button aria-label="Launch interactive content" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-rocket"></i>
</button>
<div class="menu-dropdown__content">
<ul>
</ul>
</div>
</div>
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="menu-dropdown menu-dropdown-repository-buttons">
<button aria-label="Source repositories" class="headerbtn menu-dropdown__trigger">
<i class="fab fa-github"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/instructor-course-content" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="headerbtn__text-container">repository</span>
</a>
</li>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/instructor-course-content/issues/new?title=Issue%20on%20page%20%2Ftutorials/W1D5_DeepLearning/instructor/W1D5_Tutorial1.html&amp;body=Your%20issue%20content%20here." title="Open an issue">
<span class="headerbtn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="headerbtn__text-container">open issue</span>
</a>
</li>
</ul>
</div>
</div>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../../../_sources/tutorials/W1D5_DeepLearning/instructor/W1D5_Tutorial1.ipynb" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Decoding Neural Responses
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-decoding-from-neural-data-using-feed-forward-networks-in-pytorch">
     Video 1: Decoding from neural data using feed-forward networks in pytorch
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-retrieval-and-loading">
     Data retrieval and loading
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure Settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper Functions
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-load-and-visualize-data">
   Section 1: Load and visualize data
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
</a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-deep-feed-forward-networks-in-pytorch">
   Section 2: Deep feed-forward networks in
   <em>
    pytorch
   </em>
</a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-introduction-to-pytorch">
     Section 2.1: Introduction to PyTorch
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-activation-functions">
     Section 2.2: Activation functions
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-nonlinear-activation-functions">
       Video 2: Nonlinear activation functions
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-2-nonlinear-activations">
         Coding Exercise 2.2: Nonlinear Activations
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-loss-functions-and-gradient-descent">
   Section 3: Loss functions and gradient descent
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-loss-functions-gradient-descent">
     Video 3: Loss functions &amp; gradient descent
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-loss-functions">
       Section 3.1: Loss functions
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-optimization-with-gradient-descent">
       Section 3.2: Optimization with gradient descent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-2-gradient-descent-in-pytorch">
         Coding Exercise 3.2: Gradient descent in PyTorch
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-1-neural-network-depth-width-and-expressivity">
     Bonus Section 1: Neural network
     <em>
      depth
     </em>
     ,
     <em>
      width
     </em>
     and
     <em>
      expressivity
     </em>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-2-gradient-descent">
     Bonus Section 2: Gradient descent
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-2-1-gradient-descent-equations">
       Bonus Section 2.1: Gradient descent equations
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-2-2-stochastic-gradient-descent-sgd-vs-gradient-descent-gd">
       Bonus Section 2.2:
       <em>
        Stochastic
       </em>
       gradient descent (SGD) vs. gradient descent (GD)
      </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>Tutorial 1: Decoding Neural Responses</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Decoding Neural Responses
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-decoding-from-neural-data-using-feed-forward-networks-in-pytorch">
     Video 1: Decoding from neural data using feed-forward networks in pytorch
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-retrieval-and-loading">
     Data retrieval and loading
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure Settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions">
     Helper Functions
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-load-and-visualize-data">
   Section 1: Load and visualize data
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
</a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-deep-feed-forward-networks-in-pytorch">
   Section 2: Deep feed-forward networks in
   <em>
    pytorch
   </em>
</a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-introduction-to-pytorch">
     Section 2.1: Introduction to PyTorch
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-activation-functions">
     Section 2.2: Activation functions
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-nonlinear-activation-functions">
       Video 2: Nonlinear activation functions
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-2-nonlinear-activations">
         Coding Exercise 2.2: Nonlinear Activations
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-loss-functions-and-gradient-descent">
   Section 3: Loss functions and gradient descent
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-loss-functions-gradient-descent">
     Video 3: Loss functions &amp; gradient descent
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-loss-functions">
       Section 3.1: Loss functions
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-optimization-with-gradient-descent">
       Section 3.2: Optimization with gradient descent
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-2-gradient-descent-in-pytorch">
         Coding Exercise 3.2: Gradient descent in PyTorch
        </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-1-neural-network-depth-width-and-expressivity">
     Bonus Section 1: Neural network
     <em>
      depth
     </em>
     ,
     <em>
      width
     </em>
     and
     <em>
      expressivity
     </em>
</a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-2-gradient-descent">
     Bonus Section 2: Gradient descent
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-2-1-gradient-descent-equations">
       Bonus Section 2.1: Gradient descent equations
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-2-2-stochastic-gradient-descent-sgd-vs-gradient-descent-gd">
       Bonus Section 2.2:
       <em>
        Stochastic
       </em>
       gradient descent (SGD) vs. gradient descent (GD)
      </a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/main/tutorials/W1D5_DeepLearning/instructor/W1D5_Tutorial1.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/tutorials/W1D5_DeepLearning/instructor/W1D5_Tutorial1.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<div class="section" id="tutorial-1-decoding-neural-responses">
<h1>Tutorial 1: Decoding Neural Responses<a class="headerlink" href="#tutorial-1-decoding-neural-responses" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 1, Day 5: Deep Learning</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators</strong>: Jorge A. Menendez, Carsen Stringer</p>
<p><strong>Content reviewers</strong>: Roozbeh Farhoodi,  Madineh Sarvestani, Kshitij Dwivedi, Spiros Chavlis, Ella Batty, Michael Waskom</p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p><em>Estimated timing of tutorial: 1 hr, 20 minutes</em></p>
<p>In this tutorial, we’ll use deep learning to decode stimulus information from the responses of sensory neurons. Specifically, we’ll look at the activity of ~20,000 neurons in the mouse primary visual cortex responding to oriented gratings recorded in <a class="reference external" href="https://www.biorxiv.org/content/10.1101/679324v2.abstract">this study</a>. Our task will be to decode the orientation of the presented stimulus from the responses of the whole population of neurons. We could do this in a number of ways, but here we’ll use deep learning. Deep learning is particularly well-suited to this problem for a number of reasons:</p>
<ul class="simple">
<li><p>The data are very high-dimensional: the neural response to a stimulus is a ~20,000 dimensional vector. Many machine learning techniques fail in such high dimensions, but deep learning actually thrives in this regime, as long as you have enough data (which we do here!).</p></li>
<li><p>As you’ll be able to see below, different neurons can respond quite differently to stimuli. This complex pattern of responses will, therefore, require non-linear methods to be decoded, which we can easily do with non-linear activation functions in deep networks.</p></li>
<li><p>Deep learning architectures are highly flexible, meaning we can easily adapt the architecture of our decoding model to optimize decoding. Here, we’ll focus on a single architecture, but you’ll see that it can easily be modified with few changes to the code.</p></li>
</ul>
<p>More concretely, our goal will be learn how to:</p>
<ul class="simple">
<li><p>Build a deep feed-forward network using PyTorch</p></li>
<li><p>Evaluate the network’s outputs using PyTorch built-in loss functions</p></li>
<li><p>Compute gradients of the loss with respect to each parameter of the network using automatic differentiation</p></li>
<li><p>Implement gradient descent to optimize the network’s parameters</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Tutorial slides</span>
<span class="c1"># @markdown These are the slides for all videos in this tutorial.</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">IFrame</span>
<span class="n">link_id</span> <span class="o">=</span> <span class="s2">"vb7c4"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"If you want to download the slides: https://osf.io/download/</span><span class="si">{</span><span class="n">link_id</span><span class="si">}</span><span class="s2">/"</span><span class="p">)</span>
<span class="n">IFrame</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="sa">f</span><span class="s2">"https://mfr.ca-1.osf.io/render?url=https://osf.io/</span><span class="si">{</span><span class="n">link_id</span><span class="si">}</span><span class="s2">/?direct%26mode=render%26action=download%26mode=render"</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="video-1-decoding-from-neural-data-using-feed-forward-networks-in-pytorch">
<h2>Video 1: Decoding from neural data using feed-forward networks in pytorch<a class="headerlink" href="#video-1-decoding-from-neural-data-using-feed-forward-networks-in-pytorch" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>This video covers the decoding task we will use in these tutorials, a linear network with one hidden layer, and how to build this in Pytorch.</p>
<p>Generalized linear models were used as decoding and encoding models in W1D4 Machine Learning. A model that decodes a variable from neural activity can tell us <em>how much information</em> a brain area contains about that variable. An encoding model is a model from an input variable, like visual stimulus, to neural activity. The encoding model is meant to approximate the same transformation that the brain performs on input variables and therefore help us understand <em>how the brain represents information</em>. Today we will use deep neural networks to build these models because deep neural networks can approximate a wide range of non-linear functions and can be easily fit.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="data-retrieval-and-loading">
<h2>Data retrieval and loading<a class="headerlink" href="#data-retrieval-and-loading" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Data retrieval and loading</span>
<span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">fname</span> <span class="o">=</span> <span class="s2">"W3D4_stringer_oribinned1.npz"</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">"https://osf.io/683xc/download"</span>
<span class="n">expected_md5</span> <span class="o">=</span> <span class="s2">"436599dfd8ebe6019f066c38aed20580"</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
  <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">ConnectionError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="n">requests</span><span class="o">.</span><span class="n">codes</span><span class="o">.</span><span class="n">ok</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span> <span class="o">!=</span> <span class="n">expected_md5</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Data download appears corrupted !!!"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fid</span><span class="p">:</span>
        <span class="n">fid</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure Settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Figure Settings</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-functions">
<h2>Plotting Functions<a class="headerlink" href="#plotting-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plotting Functions</span>

<span class="k">def</span> <span class="nf">plot_data_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">ax</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Visualize data matrix of neural responses using a heatmap</span>

<span class="sd">  Args:</span>
<span class="sd">    X (torch.Tensor or np.ndarray): matrix of neural responses to visualize</span>
<span class="sd">        with a heatmap</span>
<span class="sd">    ax (matplotlib axes): where to plot</span>

<span class="sd">  """</span>

  <span class="n">cax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">pink</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">vmax</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">99</span><span class="p">))</span>
  <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cax</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'normalized neural response'</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">'auto'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>

<span class="k">def</span> <span class="nf">plot_decoded_results</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">,</span> <span class="n">predicted_test_labels</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Plot decoding results in the form of network training loss and test predictions</span>

<span class="sd">  Args:</span>
<span class="sd">    train_loss (list): training error over iterations</span>
<span class="sd">    test_labels (torch.Tensor): n_test x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data, in radians</span>
<span class="sd">    predicted_test_labels (torch.Tensor): n_test x 1 tensor with predicted orientations of the</span>
<span class="sd">      stimuli from decoding neural network</span>

<span class="sd">  """</span>

  <span class="c1"># Plot results</span>
  <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

  <span class="c1"># Plot the training loss over iterations of GD</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
  <span class="c1"># Plot the testing loss over iterations of GD</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">'train loss'</span><span class="p">,</span> <span class="s1">'test loss'</span><span class="p">])</span>

  <span class="c1"># Plot true stimulus orientation vs. predicted class</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">stimuli_test</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">predicted_test_labels</span><span class="p">,</span> <span class="s1">'.'</span><span class="p">)</span>

  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'iterations of gradient descent'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'negative log likelihood'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'true stimulus orientation ($^o$)'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'decoded orientation bin'</span><span class="p">)</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_classes</span><span class="p">))</span>
  <span class="n">class_bins</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">360</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">n_classes</span><span class="si">:</span><span class="s1"> .0f</span><span class="si">}</span><span class="s1">$^o$ - </span><span class="si">{</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">360</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">n_classes</span><span class="si">:</span><span class="s1"> .0f</span><span class="si">}</span><span class="s1">$^o$'</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)]</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">class_bins</span><span class="p">);</span>

  <span class="c1"># Draw bin edges as vertical lines</span>
  <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">ax2</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">())</span>  <span class="c1"># fix y-axis limits</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
    <span class="n">lower</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">360</span> <span class="o">/</span> <span class="n">n_classes</span>
    <span class="n">upper</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">360</span> <span class="o">/</span> <span class="n">n_classes</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">lower</span><span class="p">,</span> <span class="n">lower</span><span class="p">],</span> <span class="n">ax2</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">(),</span> <span class="s1">'-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"0.7"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">upper</span><span class="p">,</span> <span class="n">upper</span><span class="p">],</span> <span class="n">ax2</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">(),</span> <span class="s1">'-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"0.7"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">plot_train_loss</span><span class="p">(</span><span class="n">train_loss</span><span class="p">):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">])</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'iterations of gradient descent'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'mean squared error'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions">
<h2>Helper Functions<a class="headerlink" href="#helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper Functions</span>

<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">data_name</span><span class="o">=</span><span class="n">fname</span><span class="p">,</span> <span class="n">bin_width</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Load mouse V1 data from Stringer et al. (2019)</span>

<span class="sd">  Data from study reported in this preprint:</span>
<span class="sd">  https://www.biorxiv.org/content/10.1101/679324v2.abstract</span>

<span class="sd">  These data comprise time-averaged responses of ~20,000 neurons</span>
<span class="sd">  to ~4,000 stimulus gratings of different orientations, recorded</span>
<span class="sd">  through Calcium imaging. The responses have been normalized by</span>
<span class="sd">  spontaneous levels of activity and then z-scored over stimuli, so</span>
<span class="sd">  expect negative numbers. They have also been binned and averaged</span>
<span class="sd">  to each degree of orientation.</span>

<span class="sd">  This function returns the relevant data (neural responses and</span>
<span class="sd">  stimulus orientations) in a torch.Tensor of data type torch.float32</span>
<span class="sd">  in order to match the default data type for nn.Parameters in</span>
<span class="sd">  Google Colab.</span>

<span class="sd">  This function will actually average responses to stimuli with orientations</span>
<span class="sd">  falling within bins specified by the bin_width argument. This helps</span>
<span class="sd">  produce individual neural "responses" with smoother and more</span>
<span class="sd">  interpretable tuning curves.</span>

<span class="sd">  Args:</span>
<span class="sd">    bin_width (float): size of stimulus bins over which to average neural</span>
<span class="sd">      responses</span>

<span class="sd">  Returns:</span>
<span class="sd">    resp (torch.Tensor): n_stimuli x n_neurons matrix of neural responses,</span>
<span class="sd">        each row contains the responses of each neuron to a given stimulus.</span>
<span class="sd">        As mentioned above, neural "response" is actually an average over</span>
<span class="sd">        responses to stimuli with similar angles falling within specified bins.</span>
<span class="sd">    stimuli: (torch.Tensor): n_stimuli x 1 column vector with orientation</span>
<span class="sd">        of each stimulus, in degrees. This is actually the mean orientation</span>
<span class="sd">        of all stimuli in each bin.</span>

<span class="sd">  """</span>
  <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_name</span><span class="p">)</span> <span class="k">as</span> <span class="n">dobj</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="o">**</span><span class="n">dobj</span><span class="p">)</span>
  <span class="n">resp</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'resp'</span><span class="p">]</span>
  <span class="n">stimuli</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'stimuli'</span><span class="p">]</span>

  <span class="k">if</span> <span class="n">bin_width</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="c1"># Bin neural responses and stimuli</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">stimuli</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span> <span class="o">+</span> <span class="n">bin_width</span><span class="p">,</span> <span class="n">bin_width</span><span class="p">))</span>
    <span class="n">stimuli_binned</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">stimuli</span><span class="p">[</span><span class="n">bins</span> <span class="o">==</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">bins</span><span class="p">)])</span>
    <span class="n">resp_binned</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">resp</span><span class="p">[</span><span class="n">bins</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">bins</span><span class="p">)])</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">resp_binned</span> <span class="o">=</span> <span class="n">resp</span>
    <span class="n">stimuli_binned</span> <span class="o">=</span> <span class="n">stimuli</span>

  <span class="c1"># Return as torch.Tensor</span>
  <span class="n">resp_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">resp_binned</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">stimuli_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">stimuli_binned</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># add singleton dimension to make a column vector</span>

  <span class="k">return</span> <span class="n">resp_tensor</span><span class="p">,</span> <span class="n">stimuli_tensor</span>


<span class="k">def</span> <span class="nf">identityLine</span><span class="p">():</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  Plot the identity line y=x</span>
<span class="sd">  """</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
  <span class="n">lims</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">ax</span><span class="o">.</span><span class="n">get_xlim</span><span class="p">(),</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_ylim</span><span class="p">()])</span>
  <span class="n">minval</span> <span class="o">=</span> <span class="n">lims</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
  <span class="n">maxval</span> <span class="o">=</span> <span class="n">lims</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
  <span class="n">equal_lims</span> <span class="o">=</span> <span class="p">[</span><span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">]</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">equal_lims</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">equal_lims</span><span class="p">)</span>
  <span class="n">line</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">],</span> <span class="p">[</span><span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">"0.7"</span><span class="p">)</span>
  <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_zorder</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">n_stim</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Return n_stim randomly drawn stimuli/resp pairs</span>

<span class="sd">  Args:</span>
<span class="sd">    n_stim (scalar): number of stimuli to draw</span>
<span class="sd">    resp (torch.Tensor):</span>
<span class="sd">    train_data (torch.Tensor): n_train x n_neurons tensor with neural</span>
<span class="sd">      responses to train on</span>
<span class="sd">    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data, in radians</span>

<span class="sd">  Returns:</span>
<span class="sd">    (torch.Tensor, torch.Tensor): n_stim x n_neurons tensor of neural responses and n_stim x 1 of orientations respectively</span>
<span class="sd">  """</span>
  <span class="n">n_stimuli</span> <span class="o">=</span> <span class="n">train_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">istim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_stimuli</span><span class="p">,</span> <span class="n">n_stim</span><span class="p">)</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">istim</span><span class="p">]</span>  <span class="c1"># neural responses to this stimulus</span>
  <span class="n">ori</span> <span class="o">=</span> <span class="n">train_labels</span><span class="p">[</span><span class="n">istim</span><span class="p">]</span>  <span class="c1"># true stimulus orientation</span>

  <span class="k">return</span> <span class="n">r</span><span class="p">,</span> <span class="n">ori</span>

<span class="k">def</span> <span class="nf">stimulus_class</span><span class="p">(</span><span class="n">ori</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Get stimulus class from stimulus orientation</span>

<span class="sd">  Args:</span>
<span class="sd">    ori (torch.Tensor): orientations of stimuli to return classes for</span>
<span class="sd">    n_classes (int): total number of classes</span>

<span class="sd">  Returns:</span>
<span class="sd">    torch.Tensor: 1D tensor with the classes for each stimulus</span>

<span class="sd">  """</span>
  <span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="n">n_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">digitize</span><span class="p">(</span><span class="n">ori</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">bins</span><span class="p">))</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># minus 1 to accomodate Python indexing</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-load-and-visualize-data">
<h1>Section 1: Load and visualize data<a class="headerlink" href="#section-1-load-and-visualize-data" title="Permalink to this headline">¶</a></h1>
<details>
<summary> <font color="blue">Click here for text recap of relevant part of  video </font></summary>
<p>We will be exploring neural activity in mice while the mice is viewing oriented grating stimuli on a screen in front of it. We record neural activity using a technique called two-photon calcium imaging, which allows us to record many thousands of neurons simultanously. The neurons light up when they fire. We then convert this imaging data to a matrix of neural responses by stimuli presented. For the purposes of this tutorial we are going to bin the neural responses and compute each neuron’s tuning curve. We used bins of 1 degree. We will use the response of all neurons in a single bin to try to predict which stimulus was shown. So we are going to be using the responses of 24000 neurons to try to predict 360 different possible stimulus conditions corresponding to each degree of orientation - which means we’re in the regime of big data!</p>
</details>
<p>In the next cell, we have provided code to load the data and plot the matrix of neural responses.</p>
<p>Next to it, we plot the tuning curves of three randomly selected neurons. These tuning curves are the averaged response of each neuron to oriented stimuli within 1<span class="math notranslate nohighlight">\(^\circ\)</span>, and since there are 360<span class="math notranslate nohighlight">\(^\circ\)</span> in total, we have 360 responses.</p>
<p>In the recording, there were actually thousands of stimuli shown, but in practice we often create these tuning curves because we want to visualize averaged responses with respect to the variable we varied in the experiment, in this case stimulus orientation.</p>
<div class="section" id="id1">
<h2><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Execute this cell to load and visualize data</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>

<span class="c1">#@markdown Execute this cell to load and visualize data</span>

<span class="c1"># Load data</span>
<span class="n">resp_all</span><span class="p">,</span> <span class="n">stimuli_all</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>  <span class="c1"># argument to this function specifies bin width</span>
<span class="n">n_stimuli</span><span class="p">,</span> <span class="n">n_neurons</span> <span class="o">=</span> <span class="n">resp_all</span><span class="o">.</span><span class="n">shape</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">n_neurons</span><span class="si">}</span><span class="s1"> neurons in response to </span><span class="si">{</span><span class="n">n_stimuli</span><span class="si">}</span><span class="s1"> stimuli'</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Visualize data matrix</span>
<span class="n">plot_data_matrix</span><span class="p">(</span><span class="n">resp_all</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">ax1</span><span class="p">)</span>  <span class="c1"># plot responses of first 100 neurons</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'stimulus'</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'neuron'</span><span class="p">)</span>

<span class="c1"># Plot tuning curves of three random neurons</span>
<span class="n">ineurons</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># pick three random neurons</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">stimuli_all</span><span class="p">,</span> <span class="n">resp_all</span><span class="p">[:,</span> <span class="n">ineurons</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'stimulus orientation ($^o$)'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'neural response'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">360</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We will split our data into a training set and test set. In particular, we will have a training set of orientations (<code class="docutils literal notranslate"><span class="pre">stimuli_train</span></code>) and the corresponding responses (<code class="docutils literal notranslate"><span class="pre">resp_train</span></code>). Our testing set will have held-out orientations (<code class="docutils literal notranslate"><span class="pre">stimuli_test</span></code>) and the corresponding responses (<code class="docutils literal notranslate"><span class="pre">resp_test</span></code>).</p>
</div>
<div class="section" id="id2">
<h2><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>Execute this cell to split into training and test sets</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Execute this cell to split into training and test sets</span>

<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Split data into training set and testing set</span>
<span class="n">n_train</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.6</span> <span class="o">*</span> <span class="n">n_stimuli</span><span class="p">)</span>  <span class="c1"># use 60% of all data for training set</span>
<span class="n">ishuffle</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">n_stimuli</span><span class="p">)</span>
<span class="n">itrain</span> <span class="o">=</span> <span class="n">ishuffle</span><span class="p">[:</span><span class="n">n_train</span><span class="p">]</span>  <span class="c1"># indices of data samples to include in training set</span>
<span class="n">itest</span> <span class="o">=</span> <span class="n">ishuffle</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>  <span class="c1"># indices of data samples to include in testing set</span>
<span class="n">stimuli_test</span> <span class="o">=</span> <span class="n">stimuli_all</span><span class="p">[</span><span class="n">itest</span><span class="p">]</span>
<span class="n">resp_test</span> <span class="o">=</span> <span class="n">resp_all</span><span class="p">[</span><span class="n">itest</span><span class="p">]</span>
<span class="n">stimuli_train</span> <span class="o">=</span> <span class="n">stimuli_all</span><span class="p">[</span><span class="n">itrain</span><span class="p">]</span>
<span class="n">resp_train</span> <span class="o">=</span> <span class="n">resp_all</span><span class="p">[</span><span class="n">itrain</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-deep-feed-forward-networks-in-pytorch">
<h1>Section 2: Deep feed-forward networks in <em>pytorch</em><a class="headerlink" href="#section-2-deep-feed-forward-networks-in-pytorch" title="Permalink to this headline">¶</a></h1>
<details>
<summary> <font color="blue">Click here for text recap of relevant part of video </font></summary>
<p>We can build a linear network with no hidden layers, where the stimulus prediction <span class="math notranslate nohighlight">\(y\)</span> is a product of weights <span class="math notranslate nohighlight">\(\mathbf{W}_{out}\)</span> and neural responses <span class="math notranslate nohighlight">\(\mathbf{r}\)</span> with an added term <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> which is called the bias term. When you fit a linear model such as this you minimize the squared error between the predicted stimulus <span class="math notranslate nohighlight">\(y\)</span> and the true stimulus <span class="math notranslate nohighlight">\(\tilde{y}\)</span>, this is the “loss function”.</p>
<div class="amsmath math notranslate nohighlight" id="equation-662a2ea8-c1e7-46a3-a5a5-78b332985abd">
<span class="eqno">(224)<a class="headerlink" href="#equation-662a2ea8-c1e7-46a3-a5a5-78b332985abd" title="Permalink to this equation">¶</a></span>\[\begin{align}
L &amp;= (y - \tilde{y})^2 \\
&amp;= ((\mathbf{W}^{out} \mathbf{r} + \mathbf{b}) - \tilde{y})^2
\end{align}\]</div>
<p>The solution to minimizing this loss function in a linear model can be found in closed form, and you learned how to solve this linear regression problem in the first week if you remember. If we use a simple linear model for this data we are able to predict the stimulus within 2-3 degrees. Let’s see if we can predict the neural activity better with a deep network.</p>
<p>Let’s add a hidden layer with <span class="math notranslate nohighlight">\(M\)</span> units to this linear model, where now the output <span class="math notranslate nohighlight">\(y\)</span> is as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-aecb8b37-93e1-4706-96ca-19ec74c8bd9a">
<span class="eqno">(225)<a class="headerlink" href="#equation-aecb8b37-93e1-4706-96ca-19ec74c8bd9a" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathbf{h} &amp;= \mathbf{W}^{in} \mathbf{r} + \mathbf{b}^{in}, &amp;&amp; [\mathbf{W}^{in}: M \times N \, , \, \mathbf{b}^{in}: M \times 1] \, , \\
y &amp;= \mathbf{W}^{out} \mathbf{h} + \mathbf{b}^{out},  &amp;&amp; [\mathbf{W}^{out}: 1 \times M\, , \, \mathbf{b}^{in}: 1 \times 1] \, ,
\end{align}\]</div>
<p>Note this linear network with one hidden layer where <span class="math notranslate nohighlight">\(M\)</span> hidden units is less than <span class="math notranslate nohighlight">\(N\)</span> inputs is equivalent to performing <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3444519/">reduced rank regression</a>, a technique that is useful for regularizing your regression model.</p>
<p>Adding this hidden layer means the model now has a depth of <span class="math notranslate nohighlight">\(1\)</span>. The number of units <span class="math notranslate nohighlight">\(M\)</span> is termed the width of the network. Increasing the depth and the width of the network can increase the expressivity of the model – in other words how well it can fit complex non-linear functions. Many state-of-the-art models now have close to 100 layers! But for now let’s start with a model with a depth of <span class="math notranslate nohighlight">\(1\)</span> and see if we can improve our prediction of the stimulus.  See <a class="reference external" href="#b1">bonus section 1</a> for a deeper discussion of what this choice entails, and when one might want to use deeper/shallower and wider/narrower architectures.</p>
<p>The <span class="math notranslate nohighlight">\(M\)</span>-dimensional vector <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> denotes the activations of the <strong>hidden layer</strong> of the network. The blue components of this diagram denote the <strong>parameters</strong> of the network, which we will later optimize with gradient descent. These include all the weights and biases <span class="math notranslate nohighlight">\(\mathbf{W}^{in}, \mathbf{b}^{in}, \mathbf{W}^{out}, \mathbf{b}^{out}\)</span>. The <strong>weights</strong> are matrices of size (# of outputs, # of inputs) that are multiplied by the input of each layer, like the regression coefficients in linear regression. The <strong>biases</strong> are vectors of size (# of outputs, 1), like the intercept term in linear regression (see W1D3 for more details on multivariate linear regression).</p>
<p align="center">
<img src="https://github.com/NeuromatchAcademy/course-content/blob/main/tutorials/static/one-layer-network.png?raw=true" width="450"/>
</p>
</details>
<p>We’ll now build a simple deep neural network that takes as input a vector of neural responses and outputs a single number representing the decoded stimulus orientation.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{r}^{(n)} = \begin{bmatrix} r_1^{(n)} &amp; r_2^{(n)} &amp; \ldots &amp; r_N^{(n)} \end{bmatrix}^\top\)</span> denote the vector of neural responses (of neurons <span class="math notranslate nohighlight">\(1, \ldots, N\)</span>) to the <span class="math notranslate nohighlight">\(n\)</span>th stimulus. The network we will use is described by the following set of equations:</p>
<div class="amsmath math notranslate nohighlight" id="equation-116b2776-50fd-49fc-9e98-1125a7918003">
<span class="eqno">(226)<a class="headerlink" href="#equation-116b2776-50fd-49fc-9e98-1125a7918003" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathbf{h}^{(n)} &amp;= \mathbf{W}^{in} \mathbf{r}^{(n)} + \mathbf{b}^{in}, &amp;&amp; [\mathbf{W}^{in}: M \times N \, , \, \mathbf{b}^{in}: M \times 1] \, , \\
y^{(n)} &amp;= \mathbf{W}^{out} \mathbf{h}^{(n)} + \mathbf{b}^{out},  &amp;&amp; [\mathbf{W}^{out}: 1 \times M \, , \, \mathbf{b}^{in}: 1 \times 1] \, ,
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(y^{(n)}\)</span> denotes the scalar output of the network: the decoded orientation of the <span class="math notranslate nohighlight">\(n\)</span>-th stimulus.</p>
<div class="section" id="section-2-1-introduction-to-pytorch">
<h2>Section 2.1: Introduction to PyTorch<a class="headerlink" href="#section-2-1-introduction-to-pytorch" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial:  16 min</em></p>
<p>Here, we’ll use the <strong>PyTorch</strong> package to build, run, and train deep networks of this form in Python. PyTorch uses a data type called a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>. They are effectively just like <code class="docutils literal notranslate"><span class="pre">numpy</span></code> arrays, except that they have some important attributes and methods needed for automatic differentiation (to be discussed below). They also come along with infrastructure for easily storing and computing with them on GPU’s, a capability we won’t touch on here but which can be really useful in practice.</p>
<details>
<summary> <font color="blue">Click here for text recap of relevant part of video </font></summary>
<p>First we import the pytorch library called <code class="docutils literal notranslate"><span class="pre">torch</span></code> and its neural network module <code class="docutils literal notranslate"><span class="pre">nn</span></code>. Next we will create a class for the deep network called DeepNet. A class has functions which are called methods. A class in python is initialized using a method called <code class="docutils literal notranslate"><span class="pre">__init__</span></code>. In this case the init method is declared to takes two inputs (other than the <code class="docutils literal notranslate"><span class="pre">self</span></code> input which represents the class itself), which are <code class="docutils literal notranslate"><span class="pre">n_inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">n_hidden</span></code>. In our case <code class="docutils literal notranslate"><span class="pre">n_inputs</span></code> is the number of neurons we are using to do the prediction, and <code class="docutils literal notranslate"><span class="pre">n_hidden</span></code> is the number of hidden units. We first call the super function to invoke the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>’s init function. Next we add the hidden layer <code class="docutils literal notranslate"><span class="pre">in_layer</span></code> as an attribute of the class. It is a linear layer called <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> with size <code class="docutils literal notranslate"><span class="pre">n_inputs</span></code> by <code class="docutils literal notranslate"><span class="pre">n_hidden</span></code>. Then we add a second linear layer <code class="docutils literal notranslate"><span class="pre">out_layer</span></code> of size <code class="docutils literal notranslate"><span class="pre">n_hidden</span></code> by <code class="docutils literal notranslate"><span class="pre">1</span></code>, because we are predicting one output - the orientation of the stimulus. PyTorch will initialize all weights and biases randomly.</p>
<p>Note the number of hidden units <code class="docutils literal notranslate"><span class="pre">n_hidden</span></code> is a parameter that we are free to vary in deciding how to build our network. See <a class="reference external" href="#b1">Bonus Section 1</a> for a discussion of how this architectural choice affects the computations the network can perform.</p>
<p>Next we add another method to the class called <code class="docutils literal notranslate"><span class="pre">forward</span></code>. This is the method that runs when you call the class as a function. It takes as input <code class="docutils literal notranslate"><span class="pre">r</span></code> which is the neural responses. Then <code class="docutils literal notranslate"><span class="pre">r</span></code> is sent through the linear layers <code class="docutils literal notranslate"><span class="pre">in_layer</span></code> and <code class="docutils literal notranslate"><span class="pre">out_layer</span></code> and returns our prediction <code class="docutils literal notranslate"><span class="pre">y</span></code>. Let’s create an instantiation of this class called <code class="docutils literal notranslate"><span class="pre">net</span></code> with 200 hidden units with <code class="docutils literal notranslate"><span class="pre">net</span> <span class="pre">=</span> <span class="pre">DeepNet(n_neurons,</span> <span class="pre">200)</span></code>. Now we can run the neural response through the network to predict the stimulus (<code class="docutils literal notranslate"><span class="pre">net(r)</span></code>); running the “net” this way calls the forward method.</p>
</details>
<p>The next cell contains code for building the deep network we defined above and in the video using the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> base class for deep neural network models (documentation <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=nn%20module#torch.nn.Module">here</a>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DeepNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Deep Network with one hidden layer</span>

<span class="sd">  Args:</span>
<span class="sd">    n_inputs (int): number of input units</span>
<span class="sd">    n_hidden (int): number of units in hidden layer</span>

<span class="sd">  Attributes:</span>
<span class="sd">    in_layer (nn.Linear): weights and biases of input layer</span>
<span class="sd">    out_layer (nn.Linear): weights and biases of output layer</span>

<span class="sd">  """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>  <span class="c1"># needed to invoke the properties of the parent class nn.Module</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span> <span class="c1"># neural activity --&gt; hidden units</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># hidden units --&gt; output</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Decode stimulus orientation from neural responses</span>

<span class="sd">    Args:</span>
<span class="sd">      r (torch.Tensor): vector of neural responses to decode, must be of</span>
<span class="sd">        length n_inputs. Can also be a tensor of shape n_stimuli x n_inputs,</span>
<span class="sd">        containing n_stimuli vectors of neural responses</span>

<span class="sd">    Returns:</span>
<span class="sd">      torch.Tensor: network outputs for each input provided in r. If</span>
<span class="sd">        r is a vector, then y is a 1D tensor of length 1. If r is a 2D</span>
<span class="sd">        tensor then y is a 2D tensor of shape n_stimuli x 1.</span>

<span class="sd">    """</span>
    <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>  <span class="c1"># hidden representation</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-2-activation-functions">
<h2>Section 2.2: Activation functions<a class="headerlink" href="#section-2-2-activation-functions" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 25 min</em></p>
<div class="section" id="video-2-nonlinear-activation-functions">
<h3>Video 2: Nonlinear activation functions<a class="headerlink" href="#video-2-nonlinear-activation-functions" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
<p>This video covers adding a nonlinear activation function, specifically a Rectified Linear Unit (ReLU), to the linear network.</p>
<details>
<summary> <font color="blue">Click here for text recap of video </font></summary>
<p>Note that the deep network we constructed above comprises solely <strong>linear</strong> operations on each layer: each layer is just a weighted sum of all the elements in the previous layer. It turns out that linear hidden layers like this aren’t particularly useful, since a sequence of linear transformations is actually essentially the same as a single linear transformation. We can see this from the above equations by plugging in the first one into the second one to obtain</p>
<div class="amsmath math notranslate nohighlight" id="equation-2ce9df98-ae2e-4a8b-b11d-a6ad6f5c9d06">
<span class="eqno">(227)<a class="headerlink" href="#equation-2ce9df98-ae2e-4a8b-b11d-a6ad6f5c9d06" title="Permalink to this equation">¶</a></span>\[\begin{equation}
y^{(n)} = \mathbf{W}^{out} \left( \mathbf{W}^{in} \mathbf{r}^{(n)} + \mathbf{b}^{in} \right) + \mathbf{b}^{out} = \mathbf{W}^{out}\mathbf{W}^{in} \mathbf{r}^{(n)} + \left( \mathbf{W}^{out}\mathbf{b}^{in} + \mathbf{b}^{out} \right)
\end{equation}\]</div>
<p>In other words, the output is still just a weighted sum of elements in the input – the hidden layer has done nothing to change this.</p>
<p>To extend the set of computable input/output transformations to more than just weighted sums, we’ll incorporate a <strong>non-linear activation function</strong> in the hidden units. This is done by simply modifying the equation for the hidden layer activations to be</p>
<div class="amsmath math notranslate nohighlight" id="equation-4d64aac7-adf0-4b33-b72a-eb98b9982dd2">
<span class="eqno">(228)<a class="headerlink" href="#equation-4d64aac7-adf0-4b33-b72a-eb98b9982dd2" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{h}^{(n)} = \phi(\mathbf{W}^{in} \mathbf{r}^{(n)} + \mathbf{b}^{in})
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi\)</span> is referred to as the activation function. Using a non-linear activation function will ensure that the hidden layer performs a non-linear transformation of the input, which will make our network much more powerful (or <em>expressive</em>, see <a class="reference external" href="#b1">Bonus Section 1</a>). In practice, deep networks <em>always</em> use non-linear activation functions.</p>
<p>The most common non-linearity used is the rectified linear unit (or ReLU), which is a max(0, x) function. At the beginning of neural network development, researchers experimented with different non-linearities such as sigmoid and tanh functions, but in the end they found that RELU activation functions worked the best. It works well because the gradient is able to back-propagate through the network as long as the input is positive - the gradient is 1 for all values of x greater than 0. If you use a saturating non-linearity then the gradients will be very small in the saturating regimes, reducing the effective computing regime of the unit.</p>
</details>
<div class="section" id="coding-exercise-2-2-nonlinear-activations">
<h4>Coding Exercise 2.2: Nonlinear Activations<a class="headerlink" href="#coding-exercise-2-2-nonlinear-activations" title="Permalink to this headline">¶</a></h4>
<p>Create a new class <code class="docutils literal notranslate"><span class="pre">DeepNetReLU</span></code> by modifying our above deep network model to add a <strong>non-linear activation</strong> function <span class="math notranslate nohighlight">\(\phi\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-9e04f7e5-9bbf-47e8-a25c-1f213aea9435">
<span class="eqno">(229)<a class="headerlink" href="#equation-9e04f7e5-9bbf-47e8-a25c-1f213aea9435" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathbf{h}^{(n)} = \phi(\mathbf{W}^{in} \mathbf{r}^{(n)} + \mathbf{b}^{in})
\end{equation}\]</div>
<p>We’ll use the linear rectification function:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3fb64fcf-9c9d-4bb2-b23f-e3439a0b3101">
<span class="eqno">(230)<a class="headerlink" href="#equation-3fb64fcf-9c9d-4bb2-b23f-e3439a0b3101" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\phi(x) = 
\begin{cases}
x &amp; \text{if } x &gt; 0 \\
0 &amp; \text{else}
\end{cases}
\end{equation}\]</div>
<p>which can be implemented in PyTorch using <code class="docutils literal notranslate"><span class="pre">torch.relu()</span></code>. Hidden layers with this activation function are typically referred to as “<strong>Re</strong>ctified <strong>L</strong>inear <strong>U</strong>nits”, or <strong>ReLU</strong>’s.</p>
<p>Initialize this network with 10 hidden units and run on an example stimulus.</p>
<p><strong>Hint</strong>: you only need to modify the <code class="docutils literal notranslate"><span class="pre">forward()</span></code> method of the above <code class="docutils literal notranslate"><span class="pre">DeepNet()</span></code> class to include <code class="docutils literal notranslate"><span class="pre">torch.relu()</span></code>.</p>
<br/>
<p>We then initialize and run this network. We use it to decode stimulus orientation (true stimulus given by <code class="docutils literal notranslate"><span class="pre">ori</span></code>) from a vector of neural responses <code class="docutils literal notranslate"><span class="pre">r</span></code> to the very first stimulus. Note that when the initialized network class is called as a function on an input (e.g., <code class="docutils literal notranslate"><span class="pre">net(r)</span></code>), its <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> method is called. This is a special property of the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class.</p>
<p>Note that the decoded orientations at this point will be nonsense, since the network has been initialized with random weights. Below, we’ll learn how to optimize these weights for good stimulus decoding.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DeepNetReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" network with a single hidden layer h with a RELU """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>  <span class="c1"># needed to invoke the properties of the parent class nn.Module</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span> <span class="c1"># neural activity --&gt; hidden units</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># hidden units --&gt; output</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>

    <span class="c1">############################################################################</span>
    <span class="c1">## TO DO for students: write code for computing network output using a</span>
    <span class="c1">## rectified linear activation function for the hidden units</span>
    <span class="c1"># Fill out function and remove</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: complete DeepNetReLU forward"</span><span class="p">)</span>
    <span class="c1">############################################################################</span>

    <span class="n">h</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># h is size (n_inputs, n_hidden)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># y is size (n_inputs, 1)</span>


    <span class="k">return</span> <span class="n">y</span>


<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize a deep network with M=200 hidden units</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DeepNetReLU</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="c1"># Get neural responses (r) to and orientation (ori) to one stimulus in dataset</span>
<span class="n">r</span><span class="p">,</span> <span class="n">ori</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">resp_train</span><span class="p">,</span> <span class="n">stimuli_train</span><span class="p">)</span>  <span class="c1"># using helper function get_data</span>

<span class="c1"># Decode orientation from these neural responses using initialized network</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>  <span class="c1"># compute output from network, equivalent to net.forward(r)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'decoded orientation: </span><span class="si">%.2f</span><span class="s1"> degrees'</span> <span class="o">%</span> <span class="n">out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'true orientation: </span><span class="si">%.2f</span><span class="s1"> degrees'</span> <span class="o">%</span> <span class="n">ori</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">class</span> <span class="nc">DeepNetReLU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" network with a single hidden layer h with a RELU """</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>  <span class="c1"># needed to invoke the properties of the parent class nn.Module</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_inputs</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span> <span class="c1"># neural activity --&gt; hidden units</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># hidden units --&gt; output</span>

  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>

    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">r</span><span class="p">))</span> <span class="c1"># h is size (n_inputs, n_hidden)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="c1"># y is size (n_inputs, 1)</span>

    <span class="k">return</span> <span class="n">y</span>


<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize a deep network with M=200 hidden units</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DeepNetReLU</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="c1"># Get neural responses (r) to and orientation (ori) to one stimulus in dataset</span>
<span class="n">r</span><span class="p">,</span> <span class="n">ori</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">resp_train</span><span class="p">,</span> <span class="n">stimuli_train</span><span class="p">)</span>  <span class="c1"># using helper function get_data</span>

<span class="c1"># Decode orientation from these neural responses using initialized network</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>  <span class="c1"># compute output from network, equivalent to net.forward(r)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'decoded orientation: </span><span class="si">%.2f</span><span class="s1"> degrees'</span> <span class="o">%</span> <span class="n">out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'true orientation: </span><span class="si">%.2f</span><span class="s1"> degrees'</span> <span class="o">%</span> <span class="n">ori</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You should see that the decoded orientation is 0.17 <span class="math notranslate nohighlight">\(^{\circ}\)</span> while the true orientation is 139.00 <span class="math notranslate nohighlight">\(^{\circ}\)</span>.</p>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-loss-functions-and-gradient-descent">
<h1>Section 3: Loss functions and gradient descent<a class="headerlink" href="#section-3-loss-functions-and-gradient-descent" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-3-loss-functions-gradient-descent">
<h2>Video 3: Loss functions &amp; gradient descent<a class="headerlink" href="#video-3-loss-functions-gradient-descent" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>This video covers loss functions, gradient descent, and how to implement these in Pytorch.</p>
<div class="section" id="section-3-1-loss-functions">
<h3>Section 3.1: Loss functions<a class="headerlink" href="#section-3-1-loss-functions" title="Permalink to this headline">¶</a></h3>
<p><em>Estimated timing to here from start of tutorial: 40 min</em></p>
<p>Because the weights of the network are currently randomly chosen, the outputs of the network are nonsense: the decoded stimulus orientation is nowhere close to the true stimulus orientation. We’ll shortly write some code to change these weights so that the network does a better job of decoding.</p>
<p>But to do so, we first need to define what we mean by “better”. One simple way of defining this is to use the squared error</p>
<div class="amsmath math notranslate nohighlight" id="equation-a2fa21c0-3a6c-4325-b6cf-44cb4f593464">
<span class="eqno">(231)<a class="headerlink" href="#equation-a2fa21c0-3a6c-4325-b6cf-44cb4f593464" title="Permalink to this equation">¶</a></span>\[\begin{equation}
L = (y - \tilde{y})^2
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is the network output and <span class="math notranslate nohighlight">\(\tilde{y}\)</span> is the true stimulus orientation. When the decoded stimulus orientation is far from the true stimulus orientation, <span class="math notranslate nohighlight">\(L\)</span> will be large. We thus refer to <span class="math notranslate nohighlight">\(L\)</span> as the <strong>loss function</strong>, as it quantifies how <em>bad</em> the network is at decoding stimulus orientation.</p>
<details>
<summary> <font color="blue">Click here for text recap of relevant part of video </font></summary>
<p>First we run the neural responses through the network <code class="docutils literal notranslate"><span class="pre">net</span></code> to get the output <code class="docutils literal notranslate"><span class="pre">out</span></code>. Then we declare our loss function, we will use the built in <code class="docutils literal notranslate"><span class="pre">nn.MSELoss</span></code> function for this purpose: <code class="docutils literal notranslate"><span class="pre">loss_fn</span> <span class="pre">=</span> <span class="pre">nn.MSELoss()</span></code>. This loss function takes two inputs, the network output <code class="docutils literal notranslate"><span class="pre">out</span></code> and the true stimulus orientations <code class="docutils literal notranslate"><span class="pre">ori</span></code> and finds the mean squared error: <code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">loss_fn(out,</span> <span class="pre">ori)</span></code>.  Specifically, it will take as arguments a <strong>batch</strong> of network outputs <span class="math notranslate nohighlight">\(y_1, y_2, \ldots, y_P\)</span> and corresponding target outputs <span class="math notranslate nohighlight">\(\tilde{y}_1, \tilde{y}_2, \ldots, \tilde{y}_P\)</span>, and compute the <strong>mean squared error (MSE)</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-5fa72adb-776c-4df4-af90-4ce4d4e1a943">
<span class="eqno">(232)<a class="headerlink" href="#equation-5fa72adb-776c-4df4-af90-4ce4d4e1a943" title="Permalink to this equation">¶</a></span>\[\begin{equation}
L = \frac{1}{P}\sum_{n=1}^P \left(y^{(n)} - \tilde{y}^{(n)}\right)^2
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(P\)</span> is the number of different stimuli in a batch, called the <em>batch size</em>.</p>
<p><strong>Computing MSE</strong></p>
<p>Evaluate the mean squared error for a deep network with <span class="math notranslate nohighlight">\(M=10\)</span> rectified linear units, on the decoded orientations from neural responses to 20 random stimuli.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize a deep network with M=10 hidden units</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DeepNetReLU</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Get neural responses to first 20 stimuli in the data set</span>
<span class="n">r</span><span class="p">,</span> <span class="n">ori</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">resp_train</span><span class="p">,</span> <span class="n">stimuli_train</span><span class="p">)</span>

<span class="c1"># Decode orientation from these neural responses</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>

<span class="c1"># Initialize PyTorch mean squared error loss function (Hint: look at nn.MSELoss)</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Evaluate mean squared error</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">ori</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'mean squared error: </span><span class="si">%.2f</span><span class="s1">'</span> <span class="o">%</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You should see a mean squared error of <span class="math notranslate nohighlight">\(42949.14\)</span>.</p>
</details></div>
<div class="section" id="section-3-2-optimization-with-gradient-descent">
<h3>Section 3.2: Optimization with gradient descent<a class="headerlink" href="#section-3-2-optimization-with-gradient-descent" title="Permalink to this headline">¶</a></h3>
<p><em>Estimated timing to here from start of tutorial: 50 min</em></p>
<details>
<summary> <font color="blue">Click here for text recap of relevant part of video </font></summary>
<p>Next we minimize this loss function using gradient descent. In <strong>gradient descent</strong> we compute the gradient of the loss function with respect to each parameter (all <span class="math notranslate nohighlight">\(W\)</span>’s and <span class="math notranslate nohighlight">\(b\)</span>’s). We then update the parameters by subtracting the learning rate times the gradient.</p>
<p>Let’s visualize this loss function <span class="math notranslate nohighlight">\(L\)</span> with respect to a weight <span class="math notranslate nohighlight">\(w\)</span>. If the gradient is positive (the slope <span class="math notranslate nohighlight">\(\frac{dL}{dw}\)</span> &gt; 0) as in this case then we want to move in the opposite direction which is negative. So we update the <span class="math notranslate nohighlight">\(w\)</span> accordingly in the negative direction on each iteration. Once the iterations complete the weight will ideally be at a value that minimizes the cost function.</p>
<p>In reality these cost functions are not convex like this one and depend on hundreds of thousands of parameters. There are tricks to help navigate this rocky cost landscape such as adding momentum or changing the optimizer but we won’t have time to get into that today. There are also ways to change the architecture of the network to improve optimization, such as including skip connections. These skip connections are used in residual networks and allow for the optimization of many layer networks.</p>
</details><p>Execute this cell to view gradient descent gif</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@markdown Execute this cell to view gradient descent gif</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s1">'https://github.com/NeuromatchAcademy/course-content/blob/main/tutorials/static/grad_descent.gif?raw=true'</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll use the <strong>gradient descent (GD)</strong> algorithm to modify our weights to reduce the loss function, which consists of iterating three steps.</p>
<ol class="simple">
<li><p><strong>Evaluate the loss</strong> on the training data,</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">train_data</span></code> are the network inputs in the training data (in our case, neural responses), and <code class="docutils literal notranslate"><span class="pre">train_labels</span></code> are the target outputs for each input (in our case, true stimulus orientations).
2. <strong>Compute the gradient of the loss</strong> with respect to each of the network weights. In PyTorch, we can do this with the <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> method of the loss <code class="docutils literal notranslate"><span class="pre">loss</span></code>. Note that the gradients of each parameter need to be cleared before calling <code class="docutils literal notranslate"><span class="pre">.backward()</span></code>, or else PyTorch will try to accumulate gradients across iterations. This can again be done using built-in optimizers via the method <code class="docutils literal notranslate"><span class="pre">.zero_grad()</span></code>. Putting these together we have</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<ol class="simple">
<li><p><strong>Update the network weights</strong> by descending the gradient. In Pytorch, we can do this using built-in optimizers. We’ll use the <code class="docutils literal notranslate"><span class="pre">optim.SGD</span></code> optimizer (documentation <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#torch.optim.SGD">here</a>) which updates parameters along the negative gradient, scaled by a learning rate. To initialize this optimizer, we have to tell it</p></li>
</ol>
<ul class="simple">
<li><p>which parameters to update, and</p></li>
<li><p>what learning rate to use</p></li>
</ul>
<p>For example, to optimize <em>all</em> the parameters of a network <code class="docutils literal notranslate"><span class="pre">net</span></code> using a learning rate of .001, the optimizer would be initialized as follows</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">.001</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">.parameters()</span></code> is a method of the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class that returns a <a class="reference external" href="https://wiki.python.org/moin/Generators">Python generator object</a> over all the parameters of that <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class (in our case, <span class="math notranslate nohighlight">\(\mathbf{W}^{in}, \mathbf{b}^{in}, \mathbf{W}^{out}, \mathbf{b}^{out}\)</span>).</p>
<p>After computing all the parameter gradients in step 2, we can then update each of these parameters using the <code class="docutils literal notranslate"><span class="pre">.step()</span></code> method of this optimizer,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<p>In the next exercise, we’ll give you a code skeleton for implementing the GD algorithm. Your job will be to fill in the blanks.</p>
<p>For the mathematical details of the GD algorithm, see <a class="reference external" href="#b21">bonus section 2.1</a>.</p>
<p>In this case we are using gradient descent (not <em>stochastic</em> gradient descent) because we are computing the gradient over ALL training data at once. Normally there is too much training data to do this in practice, and for instance the neural responses may be divided into sets of 20 stimuli. An <strong>epoch</strong> in deep learning is defined as the forward and backward pass of all the training data through the network. We will run the forward and backward pass of the network here for 20 <strong>epochs</strong>, in practice training may require thousands of epochs.</p>
<p>See <a class="reference external" href="#b22">bonus section 2.2</a> for a more detailed discussion of stochastic gradient descent.</p>
<div class="section" id="coding-exercise-3-2-gradient-descent-in-pytorch">
<h4>Coding Exercise 3.2: Gradient descent in PyTorch<a class="headerlink" href="#coding-exercise-3-2-gradient-descent-in-pytorch" title="Permalink to this headline">¶</a></h4>
<p>Complete the function <code class="docutils literal notranslate"><span class="pre">train()</span></code> that uses the gradient descent algorithm to optimize the weights of a given network. This function takes as input arguments</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">net</span></code>: the PyTorch network whose weights to optimize</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">loss_fn</span></code>: the PyTorch loss function to use to evaluate the loss</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_data</span></code>: the training data to evaluate the loss on (i.e., neural responses to decode)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_labels</span></code>: the target outputs for each data point in <code class="docutils literal notranslate"><span class="pre">train_data</span></code> (i.e., true stimulus orientations)</p></li>
</ul>
<p>We will then train a neural network on our data and plot the loss (mean squared error) over time. When we run this function, behind the scenes PyTorch is actually changing the parameters inside this network to make the network better at decoding, so its weights will now be different than they were at initialization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span>
          <span class="n">n_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Run gradient descent to optimize parameters of a given network</span>

<span class="sd">  Args:</span>
<span class="sd">    net (nn.Module): PyTorch network whose parameters to optimize</span>
<span class="sd">    loss_fn: built-in PyTorch loss function to minimize</span>
<span class="sd">    train_data (torch.Tensor): n_train x n_neurons tensor with neural</span>
<span class="sd">      responses to train on</span>
<span class="sd">    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data</span>
<span class="sd">    n_epochs (int, optional): number of epochs of gradient descent to run</span>
<span class="sd">    learning_rate (float, optional): learning rate to use for gradient descent</span>

<span class="sd">  Returns:</span>
<span class="sd">    (list): training loss over iterations</span>

<span class="sd">  """</span>

  <span class="c1"># Initialize PyTorch SGD optimizer</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

  <span class="c1"># Placeholder to save the loss at each iteration</span>
  <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Loop over epochs</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>

    <span class="c1">######################################################################</span>
    <span class="c1">## TO DO for students: fill in missing code for GD iteration</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: write code for GD iterations"</span><span class="p">)</span>
    <span class="c1">######################################################################</span>

    <span class="c1"># compute network output from inputs in train_data</span>
    <span class="n">out</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># compute network output from inputs in train_data</span>

    <span class="c1"># evaluate loss function</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>

    <span class="c1"># Clear previous gradients</span>
    <span class="o">...</span>

    <span class="c1"># Compute gradients</span>
    <span class="o">...</span>

    <span class="c1"># Update weights</span>
    <span class="o">...</span>

    <span class="c1"># Store current value of loss</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># .item() needed to transform the tensor output of loss_fn to a scalar</span>

    <span class="c1"># Track progress</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_epochs</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'iteration </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="s1"> | loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loss</span>


<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize network with 10 hidden units</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DeepNetReLU</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Initialize built-in PyTorch MSE loss function</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Run gradient descent on data</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">resp_train</span><span class="p">,</span> <span class="n">stimuli_train</span><span class="p">)</span>

<span class="c1"># Plot the training loss over iterations of GD</span>
<span class="n">plot_train_loss</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span>
          <span class="n">n_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Run gradient descent to optimize parameters of a given network</span>

<span class="sd">  Args:</span>
<span class="sd">    net (nn.Module): PyTorch network whose parameters to optimize</span>
<span class="sd">    loss_fn: built-in PyTorch loss function to minimize</span>
<span class="sd">    train_data (torch.Tensor): n_train x n_neurons tensor with neural</span>
<span class="sd">      responses to train on</span>
<span class="sd">    train_labels (torch.Tensor): n_train x 1 tensor with orientations of the</span>
<span class="sd">      stimuli corresponding to each row of train_data</span>
<span class="sd">    n_epochs (int, optional): number of epochs of gradient descent to run</span>
<span class="sd">    learning_rate (float, optional): learning rate to use for gradient descent</span>

<span class="sd">  Returns:</span>
<span class="sd">    (list): training loss over iterations</span>

<span class="sd">  """</span>

  <span class="c1"># Initialize PyTorch SGD optimizer</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

  <span class="c1"># Placeholder to save the loss at each iteration</span>
  <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="c1"># Loop over epochs</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>

    <span class="c1"># compute network output from inputs in train_data</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>  <span class="c1"># compute network output from inputs in train_data</span>

    <span class="c1"># evaluate loss function</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">)</span>

    <span class="c1"># Clear previous gradients</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Compute gradients</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="c1"># Update weights</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Store current value of loss</span>
    <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># .item() needed to transform the tensor output of loss_fn to a scalar</span>


    <span class="c1"># Track progress</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_epochs</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'iteration </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">/</span><span class="si">{</span><span class="n">n_epochs</span><span class="si">}</span><span class="s1"> | loss: </span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">train_loss</span>


<span class="c1"># Set random seeds for reproducibility</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Initialize network with 10 hidden units</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">DeepNetReLU</span><span class="p">(</span><span class="n">n_neurons</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Initialize built-in PyTorch MSE loss function</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>

<span class="c1"># Run gradient descent on data</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">resp_train</span><span class="p">,</span> <span class="n">stimuli_train</span><span class="p">)</span>

<span class="c1"># Plot the training loss over iterations of GD</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_train_loss</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>We can further improve our model - please see the Bonus Tutorial when you have time to dive deeper into this model by evaluating and improving its performance by visualizing the weights, looking at performance on test data, switching to a new loss function and adding regularization.</strong></p>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p><em>Estimated timing of tutorial: 1 hour, 20 minutes</em></p>
<p>We have now covered a number of common and powerful techniques for applying deep learning to decoding from neural data, some of which are common to almost any machine learning problem:</p>
<ul class="simple">
<li><p>Building and training deep networks using the <strong>PyTorch</strong> <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class and built-in <strong>optimizers</strong></p></li>
<li><p>Choosing <strong>loss functions</strong></p></li>
</ul>
<p>An important aspect of this tutorial was the <code class="docutils literal notranslate"><span class="pre">train()</span></code> function we wrote in coding exercise 3.2. Note that it can be used to train <em>any</em> network to minimize <em>any</em> loss function on <em>any</em> training data. This is the power of using PyTorch to train neural networks and, for that matter, <strong>any other model</strong>! There is nothing in the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class that forces us to use <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> layers that implement neural network operations. You can actually put anything you want inside the <code class="docutils literal notranslate"><span class="pre">.__init__()</span></code> and <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> methods of this class. As long as its parameters and computations involve only <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>’s, and the model is differentiable, you’ll then be able to optimize the parameters of this model in exactly the same way we optimized the deep networks here.</p>
<p>What kinds of conclusions can we draw from these sorts of analyses? If we can decode the stimulus well from visual cortex activity, that means that there is information about this stimulus available in the visual cortex. Whether or not the animal uses that information to make decisions is not determined from an analysis like this. In fact mice perform poorly in orientation discrimination tasks compared to monkeys and humans, even though they have information about these stimuli in their visual cortex. Why do you think they perform poorly in orientation discrimination tasks?</p>
<p>See <a class="reference external" href="https://doi.org/10.1016/j.cell.2021.03.042">Stringer, <em>et al.</em>, 2021</a> for some potential hypotheses, but this is totally an open question!</p>
</div>
<hr class="docutils"/>
<div class="section" id="bonus">
<h1>Bonus<a class="headerlink" href="#bonus" title="Permalink to this headline">¶</a></h1>
<p><a name="b1"></a></p>
<div class="section" id="bonus-section-1-neural-network-depth-width-and-expressivity">
<h2>Bonus Section 1: Neural network <em>depth</em>, <em>width</em> and <em>expressivity</em><a class="headerlink" href="#bonus-section-1-neural-network-depth-width-and-expressivity" title="Permalink to this headline">¶</a></h2>
<p>Two important architectural choices that always have to be made when constructing deep feed-forward networks like those used here are</p>
<ul class="simple">
<li><p>the number of hidden layers, or the network’s <em>depth</em></p></li>
<li><p>the number of units in each layer, or the layer <em>widths</em></p></li>
</ul>
<p>Here, we restricted ourselves to networks with a single hidden layer with a width of <span class="math notranslate nohighlight">\(M\)</span> units, but it is easy to see how this code could be adapted to arbitrary depths. Adding another hidden layer simply requires adding another <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> module to the <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method and incorporating it into the <code class="docutils literal notranslate"><span class="pre">.forward()</span></code> method.</p>
<p>The depth and width of a network determine the set of input/output transformations that it can perform, often referred to as its <em>expressivity</em>. The deeper and wider the network, the more <em>expressive</em> it is; that is, the larger the class of input/output transformations it can compute. In fact, it turns out that an infinitely wide <em>or</em> infinitely deep networks can in principle <a class="reference external" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">compute (almost) <em>any</em> input/output transformation</a>.</p>
<p>A classic mathematical demonstration of the power of depth is given by the so-called <a class="reference external" href="https://medium.com/@jayeshbahire/the-xor-problem-in-neural-networks-50006411840b#:~:text=The%20XOr%2C%20or%20%E2%80%9Cexclusive%20or,value%20if%20they%20are%20equal.">XOR problem</a>. This toy problem demonstrates how even a single hidden layer can drastically expand the set of input/output transformations a network can perform, relative to a shallow network with no hidden layers. The key intuition is that the hidden layer allows you to represent the input in a new format, which can then allow you to do almost anything you want with it. The <em>wider</em> this hidden layer, the more flexibility you have in this representation. In particular, if you have more hidden units than input units, then the hidden layer representation of the input is higher-dimensional than the raw data representation. This higher dimensionality effectively gives you more “room” to perform arbitrary computations in. It turns out that even with just this one hidden layer, if you make it wide enough you can actually approximate any input/output transformation you want. See <a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap4.html">here</a> for a neat visual demonstration of this.</p>
<p>In practice, however, it turns out that increasing depth seems to grant more expressivity with fewer units than increasing width does (for reasons that are not well understood). It is for this reason that truly <em>deep</em> networks are almost always used in machine learning, which is why this set of techniques is often referred to as <em>deep</em> learning.</p>
<p>That said, there is a cost to making networks deeper and wider. The bigger your network, the more parameters (i.e., weights and biases) it has, which need to be optimized! The extra expressivity afforded by higher width and/or depth thus carries with it (at least) two problems:</p>
<ul class="simple">
<li><p>optimizing more parameters usually requires more data</p></li>
<li><p>a more highly parameterized network is more prone to overfit to the training data, so requires more sophisticated optimization algorithms to ensure generalization</p></li>
</ul>
</div>
<div class="section" id="bonus-section-2-gradient-descent">
<h2>Bonus Section 2: Gradient descent<a class="headerlink" href="#bonus-section-2-gradient-descent" title="Permalink to this headline">¶</a></h2>
<p><a name="b21"></a></p>
<div class="section" id="bonus-section-2-1-gradient-descent-equations">
<h3>Bonus Section 2.1: Gradient descent equations<a class="headerlink" href="#bonus-section-2-1-gradient-descent-equations" title="Permalink to this headline">¶</a></h3>
<p>Here we provide the equations for the three steps of the gradient descent algorithm, as applied to our decoding problem:</p>
<ol class="simple">
<li><p><strong>Evaluate the loss</strong> on the training data. For a mean squared error loss, this is given by</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-a6b11d47-6da1-4aa2-9c8b-c893c525c368">
<span class="eqno">(233)<a class="headerlink" href="#equation-a6b11d47-6da1-4aa2-9c8b-c893c525c368" title="Permalink to this equation">¶</a></span>\[\begin{equation}
L = \frac{1}{P}\sum_{n=1}^P \left( y^{(n)} - \tilde{y}^{(n)} \right)^2
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(y^{(n)}\)</span> denotes the stimulus orientation decoded from the population response <span class="math notranslate nohighlight">\(\mathbf{r}^{(n)}\)</span> to the <span class="math notranslate nohighlight">\(n\)</span>th stimulus in the training data, and <span class="math notranslate nohighlight">\(\tilde{y}^{(n)}\)</span> is the true orientation of that stimulus. <span class="math notranslate nohighlight">\(P\)</span> denotes the total number of data samples in the training set. In the syntax of our <code class="docutils literal notranslate"><span class="pre">train()</span></code> function above, <span class="math notranslate nohighlight">\(\mathbf{r}^{(n)}\)</span> is given by <code class="docutils literal notranslate"><span class="pre">train_data[n,</span> <span class="pre">:]</span></code> and <span class="math notranslate nohighlight">\(\tilde{y}^{(n)}\)</span> by <code class="docutils literal notranslate"><span class="pre">train_labels[n]</span></code>.</p>
<ol class="simple">
<li><p><strong>Compute the gradient of the loss</strong> with respect to each of the network weights. In our case, this entails computing the quantities</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-81c6ade5-fcdc-4317-9dd0-eae23fc13559">
<span class="eqno">(234)<a class="headerlink" href="#equation-81c6ade5-fcdc-4317-9dd0-eae23fc13559" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{\partial L}{\partial \mathbf{W}^{in}}, \frac{\partial L}{\partial \mathbf{b}^{in}}, \frac{\partial L}{\partial \mathbf{W}^{out}}, \frac{\partial L}{\partial \mathbf{b}^{out}}
\end{equation}\]</div>
<p>Usually, we would require lots of math in order to derive each of these gradients, and lots of code to compute them. But this is where PyTorch comes to the rescue! Using a cool technique called <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a>, PyTorch automatically calculates these gradients when the <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> function is called.</p>
<p>More specifically, when this function is called on a particular variable (e.g., <code class="docutils literal notranslate"><span class="pre">loss</span></code>, as above), PyTorch will compute the gradients with respect to each network parameter. These are computed and stored behind the scenes, and can be accessed through the <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attribute of each of the network’s parameters. As we saw above, however, we actually never need to look at or call these gradients when implementing gradient descent, as this can be taken care of by PyTorch’s built-in optimizers, like <code class="docutils literal notranslate"><span class="pre">optim.SGD</span></code>.</p>
<ol class="simple">
<li><p><strong>Update the network weights</strong> by descending the gradient:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight" id="equation-1b45d23d-d20a-4bf2-98af-731c0018de48">
<span class="eqno">(235)<a class="headerlink" href="#equation-1b45d23d-d20a-4bf2-98af-731c0018de48" title="Permalink to this equation">¶</a></span>\[\begin{align}
\mathbf{W}^{in} &amp;\leftarrow \mathbf{W}^{in} - \alpha \frac{\partial L}{\partial \mathbf{W}^{in}} \\
\mathbf{b}^{in} &amp;\leftarrow \mathbf{b}^{in} - \alpha \frac{\partial L}{\partial \mathbf{b}^{in}} \\
\mathbf{W}^{out} &amp;\leftarrow \mathbf{W}^{out} - \alpha \frac{\partial L}{\partial \mathbf{W}^{out}} \\
\mathbf{b}^{out} &amp;\leftarrow \mathbf{b}^{out} - \alpha \frac{\partial L}{\partial \mathbf{b}^{out}}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is called the <strong>learning rate</strong>. This <strong>hyperparameter</strong> of the SGD algorithm controls how far we descend the gradient on each iteration. It should be as large as possible so that fewer iterations are needed, but not too large so as to avoid parameter updates from skipping over minima in the loss landscape.</p>
<p>While the equations written down here are specific to the network and loss function considered in this tutorial, the code provided above for implementing these three steps is completely general: no matter what loss function or network you are using, exactly the same commands can be used to implement these three steps.</p>
<p>The way that the gradients are calculated is called <strong>backpropagation</strong>. We have a loss function:</p>
<div class="amsmath math notranslate nohighlight" id="equation-afd4fdd0-2005-4607-9897-702db184aec3">
<span class="eqno">(236)<a class="headerlink" href="#equation-afd4fdd0-2005-4607-9897-702db184aec3" title="Permalink to this equation">¶</a></span>\[\begin{align}
L &amp;= (y - \tilde{y})^2 \\
&amp;= (\mathbf{W}^{out} \mathbf{h} - \tilde{y})^2
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{h} = \phi(\mathbf{W}^{in} \mathbf{r} + \mathbf{b}^{in})\)</span>, and <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span> is the activation function, e.g., RELU.
You may see that <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial \mathbf{W}^{out}}\)</span> is simple to calculate as it is on the outside of the equation (it is also a vector in this case, not a matrix, so the derivative is standard):</p>
<div class="amsmath math notranslate nohighlight" id="equation-17343c34-2e86-49de-bb7d-c874e5dfca15">
<span class="eqno">(237)<a class="headerlink" href="#equation-17343c34-2e86-49de-bb7d-c874e5dfca15" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{\partial L}{\partial \mathbf{W}^{out}} = 2 (\mathbf{W}^{out} \mathbf{h} - \tilde{y})\mathbf{h}^\top
\end{equation}\]</div>
<p>Now let’s compute the derivative with respect to <span class="math notranslate nohighlight">\(\mathbf{W}^{in}\)</span> using the chain rule. Note it is only positive if the output is positive due to the RELU activation function <span class="math notranslate nohighlight">\(\phi\)</span>. For the chain rule we need the derivative of the loss with respect to <span class="math notranslate nohighlight">\(\mathbf{h}\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-755afe4c-bfac-406b-891b-4fe30f0594ea">
<span class="eqno">(238)<a class="headerlink" href="#equation-755afe4c-bfac-406b-891b-4fe30f0594ea" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{\partial L}{\partial \mathbf{h}} = 2 \mathbf{W}^{out \top} (\mathbf{W}^{out} \mathbf{h} - \tilde{y})
\end{equation}\]</div>
<p>Thus,</p>
<div class="amsmath math notranslate nohighlight" id="equation-3118bbc8-6426-4eb9-80ef-5646f8f9e963">
<span class="eqno">(239)<a class="headerlink" href="#equation-3118bbc8-6426-4eb9-80ef-5646f8f9e963" title="Permalink to this equation">¶</a></span>\[\begin{align}
\frac{\partial L}{\partial \mathbf{W}^{in}} &amp;= \begin{cases}
\frac{\partial L}{\partial \mathbf{h}} \frac{\partial \mathbf{h}}{\partial \mathbf{W}^{in}}  &amp; \text{if }  \mathbf{h} &gt; 0 \\
0 &amp; \text{otherwise}
\end{cases} \\
&amp;= \begin{cases}
2 \mathbf{W}^{out \top} (\mathbf{W}^{out} \mathbf{h} - \tilde{y}) \mathbf{r}^\top  &amp; \text{if }  \mathbf{h} &gt; 0 \\
0 &amp; \text{otherwise}
\end{cases}
\end{align}\]</div>
<p>Notice that:</p>
<div class="amsmath math notranslate nohighlight" id="equation-445b32ca-39bc-45c6-bbdf-934d3e6df504">
<span class="eqno">(240)<a class="headerlink" href="#equation-445b32ca-39bc-45c6-bbdf-934d3e6df504" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{\partial \mathbf{h}}{\partial \mathbf{W}^{in}}=\mathbf{r}^\top \odot \phi^\prime
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> denotes the Hadamard product (i.e., elementwise multiplication) and <span class="math notranslate nohighlight">\(\phi^\prime\)</span> is the derivative of the activation function. In case of RELU:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c1f03ee0-9750-4a83-852c-d504a5246669">
<span class="eqno">(241)<a class="headerlink" href="#equation-c1f03ee0-9750-4a83-852c-d504a5246669" title="Permalink to this equation">¶</a></span>\[\begin{align}
\phi^\prime &amp;= \begin{cases}
1  &amp; \text{if }  \mathbf{h} &gt; 0 \\
0 &amp; \text{otherwise}
\end{cases}
\end{align}\]</div>
<p>It is most efficient to compute the derivative once for the last layer, then once for the next layer and multiply by the previous layer’s derivative and so on using the chain rule. Each of these operations is relatively fast, making training of deep networks feasible.</p>
<p>The command <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> computes these gradients for the defined <code class="docutils literal notranslate"><span class="pre">loss</span></code> with respect to each network parameter. The computation is done using <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a>, which implements backpropagation. Note that this works no matter how big/small the network is, allowing us to perform gradient descent for any deep network model built using PyTorch.</p>
<p><a name="b22"></a></p>
</div>
<div class="section" id="bonus-section-2-2-stochastic-gradient-descent-sgd-vs-gradient-descent-gd">
<h3>Bonus Section 2.2: <em>Stochastic</em> gradient descent (SGD) vs. gradient descent (GD)<a class="headerlink" href="#bonus-section-2-2-stochastic-gradient-descent-sgd-vs-gradient-descent-gd" title="Permalink to this headline">¶</a></h3>
<p>In this tutorial, we used the gradient descent algorithm, which differs in a subtle yet very important way from the more commonly used <strong>stochastic gradient descent (SGD)</strong> algorithm. The key difference is in the very first step of each iteration, where in the GD algorithm we evaluate the loss <em>at every data sample in the training set</em>. In SGD, on the other hand, we evaluate the loss only at a random subset of data samples from the full training set, called a <strong>mini-batch</strong>. At each iteration, we randomly sample a mini-batch to perform steps 1-3 on. All the above equations still hold, but now the <span class="math notranslate nohighlight">\(P\)</span> data samples <span class="math notranslate nohighlight">\(\mathbf{r}^{(n)}, \tilde{y}^{(n)}\)</span> denote a mini-batch of <span class="math notranslate nohighlight">\(P\)</span> random samples from the training set, rather than the whole training set.</p>
<p>There are several reasons why one might want to use SGD instead of GD. The first is that the training set might be too big, so that we actually can’t actually evaluate the loss on every single data sample in it. In this case, GD is simply infeasible, so we have no choice but to turn to SGD, which bypasses the restrictive memory demands of GD by subsampling the training set into smaller mini-batches.</p>
<p>But, even when GD is feasible, SGD turns out to often be better. The stochasticity induced by the extra random sampling step in SGD effectively adds some noise in the search for local minima of the loss function. This can be really useful for avoiding potential local minima, and enforce that whatever minimum is converged to is a good one. This is particularly important when networks are wider and/or deeper, in which case the large number of parameters can lead to overfitting.</p>
<p>Here, we used only GD because (1) it is simpler, and (2) it suffices for the problem being considered here. Because we have so many neurons in our data set, decoding is not too challenging and doesn’t require a particularly deep or wide network. The small number of parameters in our deep networks therefore can be optimized without a problem using GD.</p>
</div>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W1D5_DeepLearning/instructor"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="W1D5_Intro.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Intro</p>
</div>
</a>
<a class="right-next" href="W1D5_Tutorial2.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Tutorial 2: Convolutional Neural Networks</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By Neuromatch<br>
<div class="extra_footer">
<div>
<a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png"/></a>
<a href="https://opensource.org/licenses/BSD-3-Clause"><img src="https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667"/></a>
The contents of this repository are shared under the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Software elements are additionally licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD (3-Clause) License</a>.
</div>
</div>
</br></p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>