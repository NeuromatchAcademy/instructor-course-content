
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 2: Classifiers and regularizers — Neuromatch Academy: Computational Neuroscience (instructor's version)</title>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/custom.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-logo-square-4xp.png" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="W1D3_Outro.html" rel="next" title="Outro"/>
<link href="W1D3_Tutorial1.html" rel="prev" title="Tutorial 1: GLMs for Encoding"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../../../_static/nma-logo-square-4xp.png"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Computational Neuroscience (instructor's version)</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../tatraining/TA_Training_CN.html">
   TA Training: Computational Neuroscience (CN)
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</input></li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using discord
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../TechnicalHelp/Links_Policy.html">
   Quick links and policies
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../prereqs/ComputationalNeuroscience.html">
   Prerequisites and preparatory materials for NMA Computational Neuroscience
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Pre-reqs Refresher
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/chapter_title.html">
   Neuro Video Series (W0D0)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial1.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial2.html">
     Human Psychophysics
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial3.html">
     Behavioral Readout
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial4.html">
     Live in Lab
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial5.html">
     Brain Signals: Spiking Activity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial6.html">
     Brain Signals: LFP
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial7.html">
     Brain Signals: EEG &amp; MEG
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial8.html">
     Brain Signals: fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial9.html">
     Brain Signals: Calcium Imaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial10.html">
     Stimulus Representation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial11.html">
     Neurotransmitters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial12.html">
     Neurons to Consciousness
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/chapter_title.html">
   Python Workshop 1 (W0D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/instructor/W0D1_Tutorial1.html">
     Tutorial: LIF Neuron Part I
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/chapter_title.html">
   Python Workshop 2 (W0D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/instructor/W0D2_Tutorial1.html">
     Tutorial 1: LIF Neuron Part II
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D3_LinearAlgebra/chapter_title.html">
   Linear Algebra (W0D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial1.html">
     Tutorial 1: Vectors
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial2.html">
     Tutorial 2: Matrices
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial3.html">
     Bonus Tutorial: Discrete Dynamical Systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D4_Calculus/chapter_title.html">
   Calculus (W0D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial1.html">
     Tutorial 1: Differentiation and Integration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial2.html">
     Tutorial 2: Differential Equations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial3.html">
     Tutorial 3: Numerical Methods
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D5_Statistics/chapter_title.html">
   Statistics (W0D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_Tutorial1.html">
     Tutorial 1: Probability Distributions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_Tutorial2.html">
     Tutorial 2: Statistical Inference
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Intro to Modeling
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_ModelTypes/chapter_title.html">
   Model Types (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial1.html">
     Tutorial 1: “What” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial2.html">
     Tutorial 2: “How” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial3.html">
     Tutorial 3: “Why” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial4.html">
     Tutorial 4: Model Discussions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ModelingPractice/chapter_title.html">
   Modeling Practice (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Tutorial1.html">
     Tutorial 1: Framing the Question
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_ModelFitting/chapter_title.html">
   Model Fitting (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial1.html">
     Tutorial 1: Linear regression with MSE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial2.html">
     Tutorial 2: Linear regression with MLE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial3.html">
     Tutorial 3: Confidence intervals and bootstrapping
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial4.html">
     Tutorial 4: Multiple linear regression and polynomial regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial5.html">
     Tutorial 5: Model Selection: Bias-variance trade-off
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial6.html">
     Tutorial 6: Model Selection: Cross-validation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Generalized Linear Models (W1D3)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="W1D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W1D3_Tutorial1.html">
     Tutorial 1: GLMs for Encoding
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 2: Classifiers and regularizers
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W1D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W1D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/chapter_title.html">
   Dimensionality Reduction (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial1.html">
     Tutorial 1: Geometric view of data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial2.html">
     Tutorial 2: Principal Component Analysis
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial3.html">
     Tutorial 3: Dimensionality Reduction &amp; Reconstruction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial4.html">
     Tutorial 4:  Nonlinear Dimensionality Reduction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_DeepLearning/chapter_title.html">
   Deep Learning (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial1.html">
     Tutorial 1: Decoding Neural Responses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial2.html">
     Tutorial 2: Convolutional Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial3.html">
     Tutorial 3: Building and Evaluating Normative Encoding Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial4.html">
     Bonus Tutorial: Diving Deeper into Decoding &amp; Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Bonus_Autoencoders/chapter_title.html">
   Autoencoders (Bonus)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial1.html">
     Tutorial 1: Intro to Autoencoders
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial2.html">
     Tutorial 2: Autoencoder extensions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial3.html">
     Tutorial 3: Autoencoders applications
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/MachineLearning.html">
   Machine Learning Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Dynamical Systems
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_LinearSystems/chapter_title.html">
   Linear Systems (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial1.html">
     Tutorial 1: Linear dynamical systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial2.html">
     Tutorial 2: Markov Processes
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial3.html">
     Tutorial 3: Combining determinism and stochasticity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial4.html">
     Tutorial 4: Autoregressive models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/chapter_title.html">
   Biological Neuron Models (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial1.html">
     Tutorial 1: The Leaky Integrate-and-Fire (LIF) Neuron Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial2.html">
     Tutorial 2: Effects of Input Correlation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial3.html">
     Tutorial 3: Synaptic transmission - Models of static and dynamic synapses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial4.html">
     Bonus Tutorial: Spike-timing dependent plasticity (STDP)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_DynamicNetworks/chapter_title.html">
   Dynamic Networks (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial1.html">
     Tutorial 1: Neural Rate Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial2.html">
     Tutorial 2: Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial3.html">
     Bonus Tutorial: Extending the Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/DynamicalSystems.html">
   Dynamical Systems Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Stochastic Processes
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_BayesianDecisions/chapter_title.html">
   Bayesian Decisions (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial1.html">
     Tutorial 1: Bayes with a binary hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial2.html">
     Tutorial 2: Bayesian inference and decisions with continuous hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial3.html">
     Bonus Tutorial : Fitting to data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_HiddenDynamics/chapter_title.html">
   Hidden Dynamics (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial1.html">
     Tutorial 1: Sequential Probability Ratio Test
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial2.html">
     Tutorial 2: Hidden Markov Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial3.html">
     Tutorial 3: The Kalman Filter
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial4.html">
     Bonus Tutorial 4: The Kalman Filter, part 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial5.html">
     Bonus Tutorial 5: Expectation Maximization for spiking neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_OptimalControl/chapter_title.html">
   Optimal Control (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Tutorial1.html">
     Tutorial 1: Optimal Control for Discrete States
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Tutorial2.html">
     Tutorial 2: Optimal Control for Continuous State
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/chapter_title.html">
   Reinforcement Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial1.html">
     Tutorial 1: Learning to Predict
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial2.html">
     Tutorial 2: Learning to Act: Multi-Armed Bandits
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial3.html">
     Tutorial 3: Learning to Act: Q-Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial4.html">
     Tutorial 4: Model-Based Reinforcement Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D5_NetworkCausality/chapter_title.html">
   Network Causality (W3D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
<label for="toctree-checkbox-24">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial1.html">
     Tutorial 1: Interventions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial2.html">
     Tutorial 2: Correlations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial3.html">
     Tutorial 3: Simultaneous fitting/regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial4.html">
     Tutorial 4: Instrumental Variables
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/StochasticProcesses.html">
   Stochastic Processes Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
<label for="toctree-checkbox-25">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through4.html">
     Modeling Steps 1 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through10.html">
     Modeling Steps 5 - 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModel.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProject.html">
     Example Data Project: the Train Illusion
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/datasets_overview.html">
   Datasets
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
<label for="toctree-checkbox-26">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/neurons.html">
     Neurons
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
<label for="toctree-checkbox-27">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/neurons_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/fMRI.html">
     fMRI
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
<label for="toctree-checkbox-28">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/fMRI_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/ECoG.html">
     ECoG
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
<label for="toctree-checkbox-29">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/ECoG_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/behavior.html">
     Behavior
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
<label for="toctree-checkbox-30">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/behavior_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/theory.html">
     Theory
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
<label for="toctree-checkbox-31">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/theory/README.html">
       Guide
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_templates.html">
   Project Templates
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/project_2020_highlights.html">
   Projects 2020
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/>
<label for="toctree-checkbox-32">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/neurons.html">
     Neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/theory.html">
     Theory
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/behavior.html">
     Behavior
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/fMRI.html">
     fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/eeg.html">
     EEG
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
<button aria-label="Launch interactive content" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-rocket"></i>
</button>
<div class="menu-dropdown__content">
<ul>
</ul>
</div>
</div>
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="menu-dropdown menu-dropdown-repository-buttons">
<button aria-label="Source repositories" class="headerbtn menu-dropdown__trigger">
<i class="fab fa-github"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/instructor-course-content" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="headerbtn__text-container">repository</span>
</a>
</li>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/instructor-course-content/issues/new?title=Issue%20on%20page%20%2Ftutorials/W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial2.html&amp;body=Your%20issue%20content%20here." title="Open an issue">
<span class="headerbtn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="headerbtn__text-container">open issue</span>
</a>
</li>
</ul>
</div>
</div>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../../../_sources/tutorials/W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial2.ipynb" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 2: Classifiers and regularizers
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-retrieval-and-loading">
     Data retrieval and loading
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-logistic-regression">
   Section 1: Logistic regression
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-logistic-regression">
     Video 1: Logistic regression
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-the-logistic-regression-model">
     Section 1.1: The logistic regression model
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-1-implement-the-sigmoid-function">
       Coding Exercise 1.1: Implement the sigmoid function
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-using-scikit-learn">
     Section 1.2: Using scikit-learn
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-decoding-neural-data-with-logistic-regression">
   Section 2: Decoding neural data with logistic regression
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-setting-up-the-data">
     Section 2.1: Setting up the data
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-format">
       Data format
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-fitting-the-model">
     Section 2.2: Fitting the model
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-3-classifying-the-training-data">
     Section 2.3: Classifying the training data
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-4-evaluating-the-model">
     Section 2.4: Evaluating the model
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-4-classifier-accuracy">
       Coding Exercise 2.4: Classifier accuracy
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-5-cross-validating-the-classifier">
     Section 2.5: Cross-validating the classifier
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#cross-validating-using-scikit-learn-helper-functions">
       Cross-validating using
       <code class="docutils literal notranslate">
<span class="pre">
         scikit-learn
        </span>
</code>
       helper functions
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#why-more-features-than-samples-leads-to-overfitting">
       Why more features than samples leads to overfitting
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#what-we-can-do-about-it">
       What we can do about it
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-regularization">
   Section 3: Regularization
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-regularization">
     Video 2: Regularization
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-l-2-regularization">
     Section 3.1:
     <span class="math notranslate nohighlight">
      \(L_2\)
     </span>
     regularization
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-3-1-the-effect-of-varying-c-on-parameter-size">
       Interactive Demo 3.1: The effect of varying C on parameter size
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-l-1-regularization">
     Section 3.2:
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     regularization
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-3-the-key-difference-between-l-1-and-l-2-regularization-sparsity">
     Section 3.3: The key difference between
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(L_2\)
     </span>
     regularization: sparsity
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-3-the-effect-of-l-1-regularization-on-parameter-sparsity">
       Coding Exercise 3.3: The effect of
       <span class="math notranslate nohighlight">
        \(L_1\)
       </span>
       regularization on parameter sparsity
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-4-choosing-the-regularization-penalty">
     Section 3.4: Choosing the regularization penalty
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-4-model-selection">
       Coding Exercise 3.4: Model selection
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#notation">
   Notation
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-1-the-logistic-regression-model-in-full">
     Bonus Section 1: The Logistic Regression model in full
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-2-more-detail-about-model-selection">
     Bonus Section 2: More detail about model selection
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>Tutorial 2: Classifiers and regularizers</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 2: Classifiers and regularizers
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial Objectives
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-retrieval-and-loading">
     Data retrieval and loading
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-logistic-regression">
   Section 1: Logistic regression
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-logistic-regression">
     Video 1: Logistic regression
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-the-logistic-regression-model">
     Section 1.1: The logistic regression model
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-1-implement-the-sigmoid-function">
       Coding Exercise 1.1: Implement the sigmoid function
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-using-scikit-learn">
     Section 1.2: Using scikit-learn
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-decoding-neural-data-with-logistic-regression">
   Section 2: Decoding neural data with logistic regression
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-setting-up-the-data">
     Section 2.1: Setting up the data
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#data-format">
       Data format
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-fitting-the-model">
     Section 2.2: Fitting the model
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-3-classifying-the-training-data">
     Section 2.3: Classifying the training data
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-4-evaluating-the-model">
     Section 2.4: Evaluating the model
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-4-classifier-accuracy">
       Coding Exercise 2.4: Classifier accuracy
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-5-cross-validating-the-classifier">
     Section 2.5: Cross-validating the classifier
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#cross-validating-using-scikit-learn-helper-functions">
       Cross-validating using
       <code class="docutils literal notranslate">
<span class="pre">
         scikit-learn
        </span>
</code>
       helper functions
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#why-more-features-than-samples-leads-to-overfitting">
       Why more features than samples leads to overfitting
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#what-we-can-do-about-it">
       What we can do about it
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-regularization">
   Section 3: Regularization
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-regularization">
     Video 2: Regularization
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-l-2-regularization">
     Section 3.1:
     <span class="math notranslate nohighlight">
      \(L_2\)
     </span>
     regularization
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-3-1-the-effect-of-varying-c-on-parameter-size">
       Interactive Demo 3.1: The effect of varying C on parameter size
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-l-1-regularization">
     Section 3.2:
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     regularization
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-3-the-key-difference-between-l-1-and-l-2-regularization-sparsity">
     Section 3.3: The key difference between
     <span class="math notranslate nohighlight">
      \(L_1\)
     </span>
     and
     <span class="math notranslate nohighlight">
      \(L_2\)
     </span>
     regularization: sparsity
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-3-the-effect-of-l-1-regularization-on-parameter-sparsity">
       Coding Exercise 3.3: The effect of
       <span class="math notranslate nohighlight">
        \(L_1\)
       </span>
       regularization on parameter sparsity
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-4-choosing-the-regularization-penalty">
     Section 3.4: Choosing the regularization penalty
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-3-4-model-selection">
       Coding Exercise 3.4: Model selection
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#notation">
   Notation
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-1-the-logistic-regression-model-in-full">
     Bonus Section 1: The Logistic Regression model in full
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-section-2-more-detail-about-model-selection">
     Bonus Section 2: More detail about model selection
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/main/tutorials/W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial2.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/tutorials/W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial2.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<div class="section" id="tutorial-2-classifiers-and-regularizers">
<h1>Tutorial 2: Classifiers and regularizers<a class="headerlink" href="#tutorial-2-classifiers-and-regularizers" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 1, Day 3: Generalized Linear Models</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Pierre-Etienne H. Fiquet, Ari Benjamin, Jakob Macke</p>
<p><strong>Content reviewers:</strong> Davide Valeriani, Alish Dipani, Michael Waskom, Ella Batty</p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial Objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p><em>Estimated timing of tutorial: 1 hour, 35 minutes</em></p>
<p>This is part 2 of a 2-part series about Generalized Linear Models (GLMs), which are a fundamental framework for supervised learning. In part 1, we learned about and implemented GLMs. In this tutorial, we’ll implement logistic regression, a special case of GLMs used to model binary outcomes.
Oftentimes the variable you would like to predict takes only one of two possible values. Left or right? Awake or asleep? Car or bus? In this tutorial, we will decode a mouse’s left/right decisions from spike train data. Our objectives are to:</p>
<ol class="simple">
<li><p>Learn about logistic regression, how it is derived within the GLM theory, and how it is implemented in scikit-learn</p></li>
<li><p>Apply logistic regression to decode choies from neural responses</p></li>
<li><p>Learn about regularization, including the different approaches and the influence of hyperparameters</p></li>
</ol>
<hr class="docutils"/>
<p>We would like to acknowledge <a class="reference external" href="https://www.nature.com/articles/s41586-019-1787-x">Steinmetz <em>et al.</em>, 2019</a> for sharing their data, a subset of which is used here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Tutorial slides</span>
<span class="c1"># @markdown These are the slides for the videos in all tutorials today</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">IFrame</span>
<span class="n">link_id</span> <span class="o">=</span> <span class="s2">"upyjz"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"If you want to download the slides: https://osf.io/download/</span><span class="si">{</span><span class="n">link_id</span><span class="si">}</span><span class="s2">/"</span><span class="p">)</span>
<span class="n">IFrame</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="sa">f</span><span class="s2">"https://mfr.ca-1.osf.io/render?url=https://osf.io/</span><span class="si">{</span><span class="n">link_id</span><span class="si">}</span><span class="s2">/?direct%26mode=render%26action=download%26mode=render"</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Figure settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-functions">
<h2>Plotting Functions<a class="headerlink" href="#plotting-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plotting Functions</span>

<span class="k">def</span> <span class="nf">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Draw a stem plot of weights for each model in models dict."""</span>
  <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">2.5</span> <span class="o">*</span> <span class="n">n</span><span class="p">))</span>
  <span class="n">axs</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="n">sharey</span><span class="p">)</span>
  <span class="n">axs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">axs</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">models</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">margins</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mf">.02</span><span class="p">)</span>
    <span class="n">stem</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_marker</span><span class="p">(</span><span class="s2">"."</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s2">".2"</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_linewidths</span><span class="p">(</span><span class="mf">.5</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s2">".2"</span><span class="p">)</span>
    <span class="n">stem</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">"C3"</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s2">"Weight"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s2">"Neuron (a.k.a. feature)"</span><span class="p">)</span>
  <span class="n">f</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_function</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">points</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)):</span>
<span class="w">    </span><span class="sd">"""Evaluate f() on linear space between points and plot.</span>

<span class="sd">    Args:</span>
<span class="sd">      f (callable): function that maps scalar -&gt; scalar</span>
<span class="sd">      name (string): Function name for axis labels</span>
<span class="sd">      var (string): Variable name for axis labels.</span>
<span class="sd">      points (tuple): Args for np.linspace to create eval grid.</span>
<span class="sd">    """</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">*</span><span class="n">points</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
      <span class="n">xlabel</span><span class="o">=</span><span class="sa">f</span><span class="s1">'$</span><span class="si">{</span><span class="n">var</span><span class="si">}</span><span class="s1">$'</span><span class="p">,</span>
      <span class="n">ylabel</span><span class="o">=</span><span class="sa">f</span><span class="s1">'$</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">(</span><span class="si">{</span><span class="n">var</span><span class="si">}</span><span class="s1">)$'</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_model_selection</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Plot the accuracy curve over log-spaced C values."""</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">"log"</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">"o"</span><span class="p">)</span>
  <span class="n">best_C</span> <span class="o">=</span> <span class="n">C_values</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)]</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
      <span class="n">xticks</span><span class="o">=</span><span class="n">C_values</span><span class="p">,</span>
      <span class="n">xlabel</span><span class="o">=</span><span class="s2">"$C$"</span><span class="p">,</span>
      <span class="n">ylabel</span><span class="o">=</span><span class="s2">"Cross-validated accuracy"</span><span class="p">,</span>
      <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">"Best C: </span><span class="si">{</span><span class="n">best_C</span><span class="si">:</span><span class="s2">1g</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">accuracies</span><span class="p">)</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">)"</span><span class="p">,</span>
  <span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_non_zero_coefs</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">non_zero_l1</span><span class="p">,</span> <span class="n">n_voxels</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Plot the accuracy curve over log-spaced C values."""</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s2">"log"</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">non_zero_l1</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">"o"</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
    <span class="n">xticks</span><span class="o">=</span><span class="n">C_values</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">"$C$"</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="s2">"Number of non-zero coefficients"</span><span class="p">,</span>
  <span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">n_voxels</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">".1"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">":"</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">"Total</span><span class="se">\n</span><span class="s2"># Neurons"</span><span class="p">,</span> <span class="p">(</span><span class="n">C_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_voxels</span> <span class="o">*</span> <span class="mf">.98</span><span class="p">),</span> <span class="n">va</span><span class="o">=</span><span class="s2">"top"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data-retrieval-and-loading">
<h2>Data retrieval and loading<a class="headerlink" href="#data-retrieval-and-loading" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title Data retrieval and loading</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">hashlib</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">"https://osf.io/r9gh8/download"</span>
<span class="n">fname</span> <span class="o">=</span> <span class="s2">"W1D4_steinmetz_data.npz"</span>
<span class="n">expected_md5</span> <span class="o">=</span> <span class="s2">"d19716354fed0981267456b80db07ea8"</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">fname</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
  <span class="k">except</span> <span class="n">requests</span><span class="o">.</span><span class="n">ConnectionError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">r</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="n">requests</span><span class="o">.</span><span class="n">codes</span><span class="o">.</span><span class="n">ok</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Failed to download data !!!"</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span> <span class="o">!=</span> <span class="n">expected_md5</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">"!!! Data download appears corrupted !!!"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fid</span><span class="p">:</span>
        <span class="n">fid</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">load_steinmetz_data</span><span class="p">(</span><span class="n">data_fname</span><span class="o">=</span><span class="n">fname</span><span class="p">):</span>

  <span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_fname</span><span class="p">)</span> <span class="k">as</span> <span class="n">dobj</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="o">**</span><span class="n">dobj</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">data</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-logistic-regression">
<h1>Section 1: Logistic regression<a class="headerlink" href="#section-1-logistic-regression" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-1-logistic-regression">
<h2>Video 1: Logistic regression<a class="headerlink" href="#video-1-logistic-regression" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>Logistic Regression is a binary classification model. It is a GLM with a <em>logistic</em> link function and a <em>Bernoulli</em> (i.e. coinflip) noise model.</p>
<p>Like in the last notebook, logistic regression invokes a standard procedure:</p>
<ol class="simple">
<li><p>Define a <em>model</em> of how inputs relate to outputs.</p></li>
<li><p>Adjust the parameters to maximize (log) probability of your data given your model</p></li>
</ol>
</div>
<div class="section" id="section-1-1-the-logistic-regression-model">
<h2>Section 1.1: The logistic regression model<a class="headerlink" href="#section-1-1-the-logistic-regression-model" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 8 min</em></p>
<details>
<summary> <font color="blue">Click here for text recap of relevant part of video </font></summary>
<p>The fundamental input/output equation of logistic regression is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-5ef861b6-ee82-443f-ade0-a6a01506d40b">
<span class="eqno">(190)<a class="headerlink" href="#equation-5ef861b6-ee82-443f-ade0-a6a01506d40b" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\hat{y} \equiv p(y=1|x,\theta) = \sigma(\theta^\top x)
\end{equation}\]</div>
<p>Note that we interpret the output of logistic regression, <span class="math notranslate nohighlight">\(\hat{y}\)</span>, as the <strong>probability that y = 1</strong> given inputs <span class="math notranslate nohighlight">\(x\)</span> and parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Here <span class="math notranslate nohighlight">\(\sigma(\cdot)\)</span> is a “squashing” function called the <strong>sigmoid function</strong> or <strong>logistic function</strong>. Its output is in the range <span class="math notranslate nohighlight">\(0 \leq y \leq 1\)</span>. It looks like this:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b0ef4826-0fa2-4ad6-a12d-c138444bc243">
<span class="eqno">(191)<a class="headerlink" href="#equation-b0ef4826-0fa2-4ad6-a12d-c138444bc243" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\sigma(z) = \frac{1}{1 + \textrm{exp}(-z)}
\end{equation}\]</div>
<p>Recall that <span class="math notranslate nohighlight">\(z = \theta^\top x\)</span>. The parameters decide whether <span class="math notranslate nohighlight">\(\theta^\top x\)</span> will be very negative, in which case <span class="math notranslate nohighlight">\(\sigma(\theta^\top x)\approx 0\)</span>, or very positive, meaning  <span class="math notranslate nohighlight">\(\sigma(\theta^\top x)\approx 1\)</span>.</p>
</details><div class="section" id="coding-exercise-1-1-implement-the-sigmoid-function">
<h3>Coding Exercise 1.1: Implement the sigmoid function<a class="headerlink" href="#coding-exercise-1-1-implement-the-sigmoid-function" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Return the logistic transform of z."""</span>
  <span class="c1">##############################################################################</span>
  <span class="c1"># TODO for students: Fill in the missing code (...) and remove the error</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: implement the sigmoid function"</span><span class="p">)</span>
  <span class="c1">##############################################################################</span>

  <span class="n">sigmoid</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">sigmoid</span>


<span class="c1"># Visualize</span>
<span class="n">plot_function</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">,</span> <span class="s2">"\sigma"</span><span class="p">,</span> <span class="s2">"z"</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Return the logistic transform of z."""</span>

  <span class="n">sigmoid</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">sigmoid</span>


<span class="c1"># Visualize</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_function</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">,</span> <span class="s2">"\sigma"</span><span class="p">,</span> <span class="s2">"z"</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-1-2-using-scikit-learn">
<h2>Section 1.2: Using scikit-learn<a class="headerlink" href="#section-1-2-using-scikit-learn" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 13 min</em></p>
<p>Unlike the previous notebook, we’re not going to write the code that implements all of the Logistic Regression model itself. Instead, we’re going to use the implementation in <a class="reference external" href="https://scikit-learn.org/stable/">scikit-learn</a>, a very popular library for Machine Learning.</p>
<p>The goal of this next section is to introduce <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> classifiers and understand how to apply it to real neural data.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-decoding-neural-data-with-logistic-regression">
<h1>Section 2: Decoding neural data with logistic regression<a class="headerlink" href="#section-2-decoding-neural-data-with-logistic-regression" title="Permalink to this headline">¶</a></h1>
<div class="section" id="section-2-1-setting-up-the-data">
<h2>Section 2.1: Setting up the data<a class="headerlink" href="#section-2-1-setting-up-the-data" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 15 min</em></p>
<p>In this notebook we’ll use the Steinmetz dataset that you have seen previously. Recall that this dataset includes recordings of neurons as mice perform a decision task.</p>
<p>Mice had the task of turning a wheel to indicate whether they perceived a Gabor stimulus to the left, to the right, or not at all. Neuropixel probes measured spikes across the cortex. Check out the following task schematic below from the BiorXiv preprint.</p>
<p>Execute to see schematic</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute to see schematic</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="s2">"http://kordinglab.com/images/others/steinmetz-task.png"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Today we’re going to <strong>decode the decision from neural data</strong> using Logistic Regression. We will only consider trials where the mouse chose “Left” or “Right” and ignore NoGo trials.</p>
<div class="section" id="data-format">
<h3>Data format<a class="headerlink" href="#data-format" title="Permalink to this headline">¶</a></h3>
<p>In the hidden <code class="docutils literal notranslate"><span class="pre">Data</span> <span class="pre">retrieval</span> <span class="pre">and</span> <span class="pre">loading</span></code> cell, there is a function that loads the data:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">spikes</span></code>: an array of normalized spike rates with shape <code class="docutils literal notranslate"><span class="pre">(n_trials,</span> <span class="pre">n_neurons)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">choices</span></code>: a vector of 0s and 1s, indicating the animal’s behavioural response, with length <code class="docutils literal notranslate"><span class="pre">n_trials</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">load_steinmetz_data</span><span class="p">()</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As with the GLMs you’ve seen in the previous tutorial (Linear and Poisson Regression), we will need two data structures:</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">X</span></code> matrix with shape <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></code></p></li>
<li><p>a <code class="docutils literal notranslate"><span class="pre">y</span></code> vector with length <code class="docutils literal notranslate"><span class="pre">n_samples</span></code>.</p></li>
</ul>
<p>In the previous notebook, <code class="docutils literal notranslate"><span class="pre">y</span></code> corresponded to the neural data, and <code class="docutils literal notranslate"><span class="pre">X</span></code> corresponded to something about the experiment. Here, we are going to invert those relationships. That’s what makes this a <em>decoding</em> model: we are going to predict behaviour (<code class="docutils literal notranslate"><span class="pre">y</span></code>) from the neural responses (<code class="docutils literal notranslate"><span class="pre">X</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"choices"</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">"spikes"</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-2-fitting-the-model">
<h2>Section 2.2: Fitting the model<a class="headerlink" href="#section-2-2-fitting-the-model" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 25 min</em></p>
<p>Using a Logistic Regression model within <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> is very simple.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the model</span>
<span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span>

<span class="c1"># Fit it to data</span>
<span class="n">log_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There’s two steps here:</p>
<ul class="simple">
<li><p>We <em>initialized</em> the model with a hyperparameter, telling it what penalty to use (we’ll focus on this in the second part of the notebook)</p></li>
<li><p>We <em>fit</em> the model by passing it the <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> objects.</p></li>
</ul>
</div>
<div class="section" id="section-2-3-classifying-the-training-data">
<h2>Section 2.3: Classifying the training data<a class="headerlink" href="#section-2-3-classifying-the-training-data" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 27 min</em></p>
<p>Fitting the model performs maximum likelihood optimization, learning a set of <em>feature weights</em>. We can use those learned weights to <em>classify</em> new data, or predict the labels for each sample:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">log_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-4-evaluating-the-model">
<h2>Section 2.4: Evaluating the model<a class="headerlink" href="#section-2-4-evaluating-the-model" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 30 min</em></p>
<p>Now we need to evaluate the model’s predictions. We’ll do that with an <em>accuracy</em> score. The accuracy of the classifier is the proportion of trials where the predicted label matches the true label.</p>
<div class="section" id="coding-exercise-2-4-classifier-accuracy">
<h3>Coding Exercise 2.4: Classifier accuracy<a class="headerlink" href="#coding-exercise-2-4-classifier-accuracy" title="Permalink to this headline">¶</a></h3>
<p>For the first exercise, implement a function to evaluate a classifier using the accuracy score. Use it to get the accuracy of the classifier on the <em>training</em> data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Compute accuracy of classifier predictions.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    model (sklearn estimator): Classifier with trained weights.</span>

<span class="sd">  Returns:</span>
<span class="sd">    accuracy (float): Proportion of correct predictions.</span>
<span class="sd">  """</span>
  <span class="c1">#############################################################################</span>
  <span class="c1"># TODO Complete the function, then remove the next line to test it</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Implement the compute_accuracy function"</span><span class="p">)</span>
  <span class="c1">#############################################################################</span>

  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

  <span class="n">accuracy</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">accuracy</span>


<span class="c1"># Compute train accuracy</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">compute_accuracy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">log_reg</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy on the training data: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">def</span> <span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Compute accuracy of classifier predictions.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    model (sklearn estimator): Classifier with trained weights.</span>

<span class="sd">  Returns:</span>
<span class="sd">    accuracy (float): Proportion of correct predictions.</span>
<span class="sd">  """</span>

  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

  <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">accuracy</span>


<span class="c1"># Compute train accuracy</span>
<span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">compute_accuracy</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">log_reg</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy on the training data: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-5-cross-validating-the-classifier">
<h2>Section 2.5: Cross-validating the classifier<a class="headerlink" href="#section-2-5-cross-validating-the-classifier" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 40 min</em></p>
<p>Classification accuracy on the training data is 100%! That might sound impressive, but you should recall from yesterday the concept of <em>overfitting</em>: the classifier may have learned something idiosyncratic about the training data. If that’s the case, it won’t have really learned the underlying data-&gt;decision function, and thus won’t generalize well to new data.</p>
<p>To check this, we can evaluate the <em>cross-validated</em> accuracy.</p>
<p>Execute to see schematic</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute to see schematic</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="s2">"http://kordinglab.com/images/others/justCV-01.png"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="cross-validating-using-scikit-learn-helper-functions">
<h3>Cross-validating using <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> helper functions<a class="headerlink" href="#cross-validating-using-scikit-learn-helper-functions" title="Permalink to this headline">¶</a></h3>
<p>Yesterday, we asked you to write your own functions for implementing cross-validation. In practice, this won’t be necessary, because <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> offers a number of <a class="reference external" href="https://scikit-learn.org/stable/model_selection.html">helpful functions</a> that will do this for you. For example, you can cross-validate a classifier using <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> takes a <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> model like <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code>, as well as your <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> data. It then retrains your model on test/train splits of <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>, and returns the test accuracy on each of the test sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracies</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">'none'</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>  <span class="c1"># k=8 cross validation</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id1">
<h4><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Run to plot out these <code class="docutils literal notranslate"><span class="pre">k=8</span></code> accuracy scores.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>
<span class="c1">#@markdown Run to plot out these `k=8` accuracy scores.</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">vert</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">widths</span><span class="o">=</span><span class="mf">.7</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">accuracies</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span>
  <span class="n">xlabel</span><span class="o">=</span><span class="s2">"Accuracy"</span><span class="p">,</span>
  <span class="n">yticks</span><span class="o">=</span><span class="p">[],</span>
  <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s2">"Average test accuracy: </span><span class="si">{</span><span class="n">accuracies</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">"</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">"left"</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The lower cross-validated accuracy compared to the training accuracy (100%) suggests that the model is being <em>overfit</em>. Is this surprising? Think about the shape of the <span class="math notranslate nohighlight">\(X\)</span> matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
</div>
<p>The model has almost three times as many features as samples. This is a situation where overfitting is very likely (almost guaranteed).</p>
<p><strong>Link to neuroscience</strong>: Neuro data commonly has more features than samples. Having more neurons than independent trials is one example. In fMRI data, there are commonly more measured voxels than independent trials.</p>
</div>
</div>
<div class="section" id="why-more-features-than-samples-leads-to-overfitting">
<h3>Why more features than samples leads to overfitting<a class="headerlink" href="#why-more-features-than-samples-leads-to-overfitting" title="Permalink to this headline">¶</a></h3>
<p>In brief, the variance of model estimation increases when there are more features than samples. That is, you would get a very different model every time you get new data and run <code class="docutils literal notranslate"><span class="pre">.fit()</span></code>. This is very related to the <em>bias/variance tradeoff</em> you learned about on day 1.</p>
<p>Why does this happen? Here’s a tiny example to get your intuition going. Imagine trying to find a best-fit line in 2D when you only have 1 datapoint. There are simply an infinite number of lines that pass through that point. This is the situation we find ourselves in with more features than samples.</p>
</div>
<div class="section" id="what-we-can-do-about-it">
<h3>What we can do about it<a class="headerlink" href="#what-we-can-do-about-it" title="Permalink to this headline">¶</a></h3>
<p>As you learned on day 1, you can decrease model variance if you don’t mind increasing its bias. Here, we will increase bias by assuming that the correct parameters are all small. In our 2D example, this is like preferring the horizontal line to all others. This is one example of <em>regularization</em>.</p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-regularization">
<h1>Section 3: Regularization<a class="headerlink" href="#section-3-regularization" title="Permalink to this headline">¶</a></h1>
<p><em>Estimated timing to here from start of tutorial: 50 min</em></p>
<div class="section" id="video-2-regularization">
<h2>Video 2: Regularization<a class="headerlink" href="#video-2-regularization" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<details>
<summary> <font color="blue">Click here for text recap of video </font></summary>
<p>Regularization forces a model to learn a set solutions you <em>a priori</em> believe to be more correct, which reduces overfitting because it doesn’t have as much flexibility to fit idiosyncracies in the training data. This adds model bias, but it’s a good bias because you know (maybe) that parameters should be small or mostly 0.</p>
<p>In a GLM, a common form of regularization is to <em>shrink</em> the classifier weights. In a linear model, you can see its effect by plotting the weights. We’ve defined a helper function, <code class="docutils literal notranslate"><span class="pre">plot_weights</span></code>, that we’ll use extensively in this section.</p>
</details>
<p>Here is what the weights look like for a Logistic Regression model with no regularization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">"none"</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot_weights</span><span class="p">({</span><span class="s2">"No regularization"</span><span class="p">:</span> <span class="n">log_reg</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<p>It’s important to understand this plot. Each dot visualizes a value in our parameter vector <span class="math notranslate nohighlight">\(\theta\)</span>. (It’s the same style of plot as the one showing <span class="math notranslate nohighlight">\(\theta\)</span> in the video). Since each feature is the time-averaged response of a neuron, each dot shows how the model uses each neuron to estimate a decision.</p>
<p>Note the scale of the y-axis. Some neurons have values of about <span class="math notranslate nohighlight">\(20\)</span>, whereas others scale to <span class="math notranslate nohighlight">\(-20\)</span>.</p>
</div>
<div class="section" id="section-3-1-l-2-regularization">
<h2>Section 3.1: <span class="math notranslate nohighlight">\(L_2\)</span> regularization<a class="headerlink" href="#section-3-1-l-2-regularization" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 53 min</em></p>
<p>Regularization comes in different flavors. A very common one uses an <span class="math notranslate nohighlight">\(L_2\)</span> or “ridge” penalty. This changes the objective function to</p>
<div class="amsmath math notranslate nohighlight" id="equation-728a4fe3-80eb-48b0-b61f-fa288677d086">
<span class="eqno">(192)<a class="headerlink" href="#equation-728a4fe3-80eb-48b0-b61f-fa288677d086" title="Permalink to this equation">¶</a></span>\[\begin{equation}
-\log\mathcal{L}'(\theta | X, y)= -\log\mathcal{L}(\theta | X, y) +\frac\beta2\sum_i\theta_i^2,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is a <em>hyperparameter</em> that sets the <em>strength</em> of the regularization.</p>
<p>You can use regularization in <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> by changing the <code class="docutils literal notranslate"><span class="pre">penalty</span></code>, and you can set the strength of the regularization with the <code class="docutils literal notranslate"><span class="pre">C</span></code> hyperparameter (<span class="math notranslate nohighlight">\(C = \frac{1}{\beta}\)</span>, so this sets the <em>inverse</em> regularization).</p>
<p>Let’s compare the unregularized classifier weights with the classifier weights when we use the default <code class="docutils literal notranslate"><span class="pre">C</span> <span class="pre">=</span> <span class="pre">1</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_reg_l2</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">"l2"</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># now show the two models</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">"No regularization"</span><span class="p">:</span> <span class="n">log_reg</span><span class="p">,</span>
  <span class="s2">"$L_2$ (C = 1)"</span><span class="p">:</span> <span class="n">log_reg_l2</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Using the same scale for the two y axes, it’s almost impossible to see the <span class="math notranslate nohighlight">\(L_2\)</span> weights. Let’s allow the y axis scales to adjust to each set of weights:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now you can see that the weights have the same basic pattern, but the regularized weights are an order-of-magnitude smaller.</p>
<div class="section" id="interactive-demo-3-1-the-effect-of-varying-c-on-parameter-size">
<h3>Interactive Demo 3.1: The effect of varying C on parameter size<a class="headerlink" href="#interactive-demo-3-1-the-effect-of-varying-c-on-parameter-size" title="Permalink to this headline">¶</a></h3>
<p>We can use this same approach to see how the weights depend on the <em>strength</em> of the regularization:</p>
<p>Execute this cell to enable the widget!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to enable the widget!</span>

<span class="c1"># Precompute the models so the widget is responsive</span>
<span class="n">log_C_steps</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">penalized_models</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">log_C</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">*</span><span class="n">log_C_steps</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">):</span>
  <span class="n">m</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="s2">"l2"</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">10</span> <span class="o">**</span> <span class="n">log_C</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
  <span class="n">penalized_models</span><span class="p">[</span><span class="n">log_C</span><span class="p">]</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_observed</span><span class="p">(</span><span class="n">log_C</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)):</span>
  <span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"No regularization"</span><span class="p">:</span> <span class="n">log_reg</span><span class="p">,</span>
    <span class="sa">f</span><span class="s2">"$L_2$ (C = $10^</span><span class="si">{</span><span class="n">log_C</span><span class="si">}</span><span class="s2">$)"</span><span class="p">:</span> <span class="n">penalized_models</span><span class="p">[</span><span class="n">log_C</span><span class="p">]</span>
  <span class="p">}</span>
  <span class="n">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Recall from above that <span class="math notranslate nohighlight">\(C=\frac1\beta\)</span> so larger <code class="docutils literal notranslate"><span class="pre">C</span></code> is less regularization. The top panel corresponds to <span class="math notranslate nohighlight">\(C=\infty\)</span>.</p>
</div>
</div>
<div class="section" id="section-3-2-l-1-regularization">
<h2>Section 3.2: <span class="math notranslate nohighlight">\(L_1\)</span> regularization<a class="headerlink" href="#section-3-2-l-1-regularization" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 1 hr, 3 min</em></p>
<p><span class="math notranslate nohighlight">\(L_2\)</span> is not the only option for regularization. There is also the <span class="math notranslate nohighlight">\(L_1\)</span>, or “Lasso” penalty. This changes the objective function to</p>
<div class="amsmath math notranslate nohighlight" id="equation-f7f2e921-d396-4516-aec4-f5a8402cd3bb">
<span class="eqno">(193)<a class="headerlink" href="#equation-f7f2e921-d396-4516-aec4-f5a8402cd3bb" title="Permalink to this equation">¶</a></span>\[\begin{equation}
-\log\mathcal{L}'(\theta | X, y) = -\log\mathcal{L}(\theta | X, y) +\frac\beta2\sum_i|\theta_i|
\end{equation}\]</div>
<p>In practice, using the summed absolute values of the weights causes <em>sparsity</em>: instead of just getting smaller, some of the weights will get forced to <span class="math notranslate nohighlight">\(0\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">log_reg_l1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">"l1"</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">"saga"</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">log_reg_l1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s2">"$L_2$ (C = 1)"</span><span class="p">:</span> <span class="n">log_reg_l2</span><span class="p">,</span>
  <span class="s2">"$L_1$ (C = 1)"</span><span class="p">:</span> <span class="n">log_reg_l1</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">plot_weights</span><span class="p">(</span><span class="n">models</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Note: You’ll notice that we added two additional parameters: <code class="docutils literal notranslate"><span class="pre">solver="saga"</span></code> and <code class="docutils literal notranslate"><span class="pre">max_iter=5000</span></code>. The <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> class can use several different optimization algorithms (“solvers”), and not all of them support the <span class="math notranslate nohighlight">\(L_1\)</span> penalty. At a certain point, the solver will give up if it hasn’t found a minimum value. The <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> parameter tells it to make more attempts; otherwise, we’d see an ugly warning about “convergence”.</p>
</div>
<div class="section" id="section-3-3-the-key-difference-between-l-1-and-l-2-regularization-sparsity">
<h2>Section 3.3: The key difference between <span class="math notranslate nohighlight">\(L_1\)</span> and <span class="math notranslate nohighlight">\(L_2\)</span> regularization: sparsity<a class="headerlink" href="#section-3-3-the-key-difference-between-l-1-and-l-2-regularization-sparsity" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 1 hr, 10 min</em></p>
<p>When should you use <span class="math notranslate nohighlight">\(L_1\)</span> vs. <span class="math notranslate nohighlight">\(L_2\)</span> regularization? Both penalties shrink parameters, and both will help reduce overfitting. However, the models they lead to are different.</p>
<p>In particular, the <span class="math notranslate nohighlight">\(L_1\)</span> penalty encourages <em>sparse</em> solutions in which most parameters are 0. Let’s unpack the notion of sparsity.</p>
<p>A “dense” vector has mostly nonzero elements:
<span class="math notranslate nohighlight">\(\begin{bmatrix}
  0.1 \\ -0.6\\-9.1\\0.07 
\end{bmatrix}\)</span>.
A “sparse” vector has mostly zero elements:
<span class="math notranslate nohighlight">\(\begin{bmatrix}
  0 \\ -0.7\\ 0\\0
\end{bmatrix}\)</span>.</p>
<p>The same is true of matrices:</p>
<p>Execute to plot a dense and a sparse matrix</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute to plot a dense and a sparse matrix</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
<span class="n">M_sparse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">M_sparse</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"A dense matrix"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"A sparse matrix"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">text_kws</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">ha</span><span class="o">=</span><span class="s2">"center"</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">"center"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">iter_parts</span> <span class="o">=</span> <span class="n">axs</span><span class="p">,</span> <span class="p">[</span><span class="n">M</span><span class="p">,</span> <span class="n">M_sparse</span><span class="p">],</span> <span class="p">[</span><span class="s2">"</span><span class="si">{:.1f}</span><span class="s2">"</span><span class="p">,</span> <span class="s2">"</span><span class="si">{:d}</span><span class="s2">"</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">mat</span><span class="p">,</span> <span class="n">fmt</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">iter_parts</span><span class="p">):</span>
      <span class="n">val</span> <span class="o">=</span> <span class="n">mat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
      <span class="n">color</span> <span class="o">=</span> <span class="s2">".1"</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="mf">.7</span> <span class="k">else</span> <span class="s2">"w"</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">fmt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">val</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="o">**</span><span class="n">text_kws</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="coding-exercise-3-3-the-effect-of-l-1-regularization-on-parameter-sparsity">
<h3>Coding Exercise 3.3: The effect of <span class="math notranslate nohighlight">\(L_1\)</span> regularization on parameter sparsity<a class="headerlink" href="#coding-exercise-3-3-the-effect-of-l-1-regularization-on-parameter-sparsity" title="Permalink to this headline">¶</a></h3>
<p>Please complete the following function to fit a regularized <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model and return <strong>the number of coefficients in the parameter vector that are equal to 0</strong>.</p>
<p>Don’t forget to check out the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">scikit-learn documentation</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">count_non_zero_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Fit models with different L1 penalty values and count non-zero coefficients.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    C_values (1D array): List of hyperparameter values</span>

<span class="sd">  Returns:</span>
<span class="sd">    non_zero_coefs (list): number of coefficients in each model that are nonzero</span>

<span class="sd">  """</span>
  <span class="c1">#############################################################################</span>
  <span class="c1"># TODO Complete the function and remove the error</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Implement the count_non_zero_coefs function"</span><span class="p">)</span>
  <span class="c1">#############################################################################</span>

  <span class="n">non_zero_coefs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>

    <span class="c1"># Initialize and fit the model</span>
    <span class="c1"># (Hint, you may need to set max_iter)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
    <span class="o">...</span>

    <span class="c1"># Get the coefs of the fit model (in sklearn, we can do this using model.coef_)</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Count the number of non-zero elements in coefs</span>
    <span class="n">non_zero</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">non_zero_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">non_zero</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">non_zero_coefs</span>


<span class="c1"># Use log-spaced values for C</span>
<span class="n">C_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Count non zero coefficients</span>
<span class="n">non_zero_l1</span> <span class="o">=</span> <span class="n">count_non_zero_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">plot_non_zero_coefs</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">non_zero_l1</span><span class="p">,</span> <span class="n">n_voxels</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">def</span> <span class="nf">count_non_zero_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Fit models with different L1 penalty values and count non-zero coefficients.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    C_values (1D array): List of hyperparameter values</span>

<span class="sd">  Returns:</span>
<span class="sd">    non_zero_coefs (list): number of coefficients in each model that are nonzero</span>

<span class="sd">  """</span>
  <span class="n">non_zero_coefs</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>

    <span class="c1"># Initialize and fit the model</span>
    <span class="c1"># (Hint, you may need to set max_iter)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">"l1"</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">"saga"</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Get the coefs of the fit model (in sklearn, we can do this using model.coef_)</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>

    <span class="c1"># Count the number of non-zero elements in coefs</span>
    <span class="n">non_zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">coefs</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">non_zero_coefs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">non_zero</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">non_zero_coefs</span>


<span class="c1"># Use log-spaced values for C</span>
<span class="n">C_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># Count non zero coefficients</span>
<span class="n">non_zero_l1</span> <span class="o">=</span> <span class="n">count_non_zero_coefs</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_non_zero_coefs</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">non_zero_l1</span><span class="p">,</span> <span class="n">n_voxels</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Smaller <code class="docutils literal notranslate"><span class="pre">C</span></code> (bigger <span class="math notranslate nohighlight">\(\beta\)</span>) leads to sparser solutions.</p>
<p><strong>Link to neuroscience</strong>: When is it OK to assume that the parameter vector is sparse? Whenever it is true that most features don’t affect the outcome. One use-case might be decoding low-level visual features from whole-brain fMRI: we may expect only voxels in V1 and thalamus should be used in the prediction.</p>
<p><strong>WARNING</strong>: be careful when interpreting <span class="math notranslate nohighlight">\(\theta\)</span>. Never interpret the nonzero coefficients as <em>evidence</em> that only those voxels/neurons/features carry information about the outcome. This is a product of our regularization scheme, and thus <em>our prior assumption that the solution is sparse</em>. Other regularization types or models may find very distributed relationships across the brain. Never use a model as evidence for a phenomena when that phenomena is encoded in the assumptions of the model.</p>
</div>
</div>
<div class="section" id="section-3-4-choosing-the-regularization-penalty">
<h2>Section 3.4: Choosing the regularization penalty<a class="headerlink" href="#section-3-4-choosing-the-regularization-penalty" title="Permalink to this headline">¶</a></h2>
<p><em>Estimated timing to here from start of tutorial: 1 hr, 25 min</em></p>
<p>In the examples above, we just picked arbitrary numbers for the strength of regularization. How do you know what value of the hyperparameter to use?</p>
<p>The answer is the same as when you want to know whether you have learned good parameter values: use cross-validation. The best hyperparameter will be the one that allows the model to generalize best to unseen data.</p>
<div class="section" id="coding-exercise-3-4-model-selection">
<h3>Coding Exercise 3.4: Model selection<a class="headerlink" href="#coding-exercise-3-4-model-selection" title="Permalink to this headline">¶</a></h3>
<p>In the final exercise, we will use cross-validation to evaluate a set of models, each with a different <span class="math notranslate nohighlight">\(L_2\)</span> penalty. Your <code class="docutils literal notranslate"><span class="pre">model_selection</span></code> function should have a for-loop that gets the mean cross-validated accuracy for each penalty value (use the <code class="docutils literal notranslate"><span class="pre">cross_val_score</span></code> function that we introduced above, with 8 folds).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Compute CV accuracy for each C value.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    C_values (1D array): Array of hyperparameter values</span>

<span class="sd">  Returns:</span>
<span class="sd">    accuracies (1D array): CV accuracy with each value of C</span>

<span class="sd">  """</span>
  <span class="c1">#############################################################################</span>
  <span class="c1"># TODO Complete the function and remove the error</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Implement the model_selection function"</span><span class="p">)</span>
  <span class="c1">#############################################################################</span>

  <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>

    <span class="c1"># Initialize and fit the model</span>
    <span class="c1"># (Hint, you may need to set max_iter)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Get the accuracy for each test split using cross-validation</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="o">...</span>

    <span class="c1"># Store the average test accuracy for this value of C</span>
    <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">accuracies</span>


<span class="c1"># Use log-spaced values for C</span>
<span class="n">C_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>

<span class="c1"># Compute accuracies</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="n">model_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">plot_model_selection</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">def</span> <span class="nf">model_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Compute CV accuracy for each C value.</span>

<span class="sd">  Args:</span>
<span class="sd">    X (2D array): Data matrix</span>
<span class="sd">    y (1D array): Label vector</span>
<span class="sd">    C_values (1D array): Array of hyperparameter values.</span>

<span class="sd">  Returns:</span>
<span class="sd">    accuracies (1D array): CV accuracy with each value of C.</span>

<span class="sd">  """</span>
  <span class="n">accuracies</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">C</span> <span class="ow">in</span> <span class="n">C_values</span><span class="p">:</span>

    <span class="c1"># Initialize and fit the model</span>
    <span class="c1"># (Hint, you may need to set max_iter)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s2">"l2"</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>

    <span class="c1"># Get the accuracy for each test split using cross-validation</span>
    <span class="n">accs</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

    <span class="c1"># Store the average test accuracy for this value of C</span>
    <span class="n">accuracies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accs</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

  <span class="k">return</span> <span class="n">accuracies</span>


<span class="c1"># Use log-spaced values for C</span>
<span class="n">C_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>

<span class="c1"># Compute accuracies</span>
<span class="n">accuracies</span> <span class="o">=</span> <span class="n">model_selection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C_values</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">plot_model_selection</span><span class="p">(</span><span class="n">C_values</span><span class="p">,</span> <span class="n">accuracies</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This plot suggests that the right value of <span class="math notranslate nohighlight">\(C\)</span> does matter — up to a point. Remember that C is the <em>inverse</em> regularization. The plot shows that models where the regularization was too strong (small C values) performed very poorly. For <span class="math notranslate nohighlight">\(C &gt; 10^{-2}\)</span>, the differences are marginal, but the best performance was obtained with an intermediate value (<span class="math notranslate nohighlight">\(C \approx 10^1\)</span>).</p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p><em>Estimated timing of tutorial: 1 hour, 35 minutes</em></p>
<p>In this notebook, we learned about Logistic Regression, a fundamental algorithm for <em>classification</em>. We applied the algorithm to a <em>neural decoding</em> problem: we tried to predict an animal’s behavioural choice from its neural activity. We saw again how important it is to use <em>cross-validation</em> to evaluate complex models that are at risk for <em>overfitting</em>, and we learned how <em>regularization</em> can be used to fit models that generalize better. Finally, we learned about some of the different options for regularization, and we saw how cross-validation can be useful for <em>model selection</em>.</p>
</div>
<hr class="docutils"/>
<div class="section" id="notation">
<h1>Notation<a class="headerlink" href="#notation" title="Permalink to this headline">¶</a></h1>
<div class="amsmath math notranslate nohighlight" id="equation-aa2bda26-b9bd-4698-9595-ceb9e725a93c">
<span class="eqno">(194)<a class="headerlink" href="#equation-aa2bda26-b9bd-4698-9595-ceb9e725a93c" title="Permalink to this equation">¶</a></span>\[\begin{align}
x &amp;\quad \text{input}\\
y &amp;\quad \text{measurement, response}\\
\theta &amp;\quad \text{parameter}\\
\sigma(z) &amp;\quad \text{logistic function}\\
C &amp;\quad \text{inverse regularization strength parameter}\\
\beta &amp;\quad \text{regularization strength parameter}\\
\hat{y} &amp;\quad \text{estimated output}\\
\mathcal{L}(\theta| y_i, x_i) &amp;\quad \text{likelihood of that parameter } \theta \text{ producing response } y_i \text{ from input } x_i\\
L_1 &amp;\quad \text{Lasso regularization}\\
L_2 &amp;\quad \text{ridge regularization}\\
\end{align}\]</div>
</div>
<hr class="docutils"/>
<div class="section" id="bonus">
<h1>Bonus<a class="headerlink" href="#bonus" title="Permalink to this headline">¶</a></h1>
<hr class="docutils"/>
<div class="section" id="bonus-section-1-the-logistic-regression-model-in-full">
<h2>Bonus Section 1: The Logistic Regression model in full<a class="headerlink" href="#bonus-section-1-the-logistic-regression-model-in-full" title="Permalink to this headline">¶</a></h2>
<p>The fundamental input/output equation of logistic regression is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2b7c7b53-ea51-4b1d-b9fc-caf09f01f78d">
<span class="eqno">(195)<a class="headerlink" href="#equation-2b7c7b53-ea51-4b1d-b9fc-caf09f01f78d" title="Permalink to this equation">¶</a></span>\[\begin{equation}
p(y_i = 1 |x_i, \theta) = \sigma(\theta^\top x_i)
\end{equation}\]</div>
<p><strong>The logistic link function</strong></p>
<p>You’ve seen <span class="math notranslate nohighlight">\(\theta^T x_i\)</span> before, but the <span class="math notranslate nohighlight">\(\sigma\)</span> is new. It’s the <em>sigmoidal</em> or <em>logistic</em> link function that “squashes” <span class="math notranslate nohighlight">\(\theta^T x_i\)</span> to keep it between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-d3260bd5-1037-40d4-82d5-45a0916502aa">
<span class="eqno">(196)<a class="headerlink" href="#equation-d3260bd5-1037-40d4-82d5-45a0916502aa" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\sigma(z) = \frac{1}{1 + \textrm{exp}(-z)}
\end{equation}\]</div>
<p><strong>The Bernoulli likelihood</strong></p>
<p>You might have noticed that the output of the sigmoid, <span class="math notranslate nohighlight">\(\hat{y}\)</span> is not a binary value (0 or 1), even though the true data <span class="math notranslate nohighlight">\(y\)</span> is! Instead, we interpret the value of <span class="math notranslate nohighlight">\(\hat{y}\)</span> as the <em>probability that y = 1</em>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0808e91f-facb-47af-b1ad-ab224e522d1c">
<span class="eqno">(197)<a class="headerlink" href="#equation-0808e91f-facb-47af-b1ad-ab224e522d1c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\hat{y_i} \equiv p(y_i=1|x_i,\theta) = \frac{1}{{1 + \textrm{exp}(-\theta^\top x_i)}}
\end{equation}\]</div>
<p>To get the likelihood of the parameters, we need to define <em>the probability of seeing <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(\hat{y}\)</span></em>. In logistic regression, we do this using the Bernoulli distribution:</p>
<div class="amsmath math notranslate nohighlight" id="equation-044a2f39-8bf6-4578-af04-d32e0591e2f7">
<span class="eqno">(198)<a class="headerlink" href="#equation-044a2f39-8bf6-4578-af04-d32e0591e2f7" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(y_i\ |\ \hat{y}_i) = \hat{y}_i^{y_i}(1 - \hat{y}_i)^{(1 - y_i)}
\end{equation}\]</div>
<p>So plugging in the regression model:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c88a5462-9aca-48dc-97bd-88f0aead8f3c">
<span class="eqno">(199)<a class="headerlink" href="#equation-c88a5462-9aca-48dc-97bd-88f0aead8f3c" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(y_i\ |\ \theta, x_i) = \sigma(\theta^\top x_i)^{y_i}\left(1 - \sigma(\theta^\top x_i)\right)^{(1 - y_i)}.
\end{equation}\]</div>
<p>This expression effectively measures how good our parameters <span class="math notranslate nohighlight">\(\theta\)</span> are. We can also write it as the likelihood of the parameters given the data:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3dba905a-d7e6-4741-81be-edb0645dd9a5">
<span class="eqno">(200)<a class="headerlink" href="#equation-3dba905a-d7e6-4741-81be-edb0645dd9a5" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\mathcal{L}(\theta\ |\ y_i, x_i) = P(y_i\ |\ \theta, x_i),
\end{equation}\]</div>
<p>and then use this as a target of optimization, considering all of the trials independently:</p>
<div class="amsmath math notranslate nohighlight" id="equation-180534b7-1ad7-4d54-b411-8b18a79aeb7d">
<span class="eqno">(201)<a class="headerlink" href="#equation-180534b7-1ad7-4d54-b411-8b18a79aeb7d" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\log\mathcal{L}(\theta | X, y) = \sum_{i=1}^Ny_i\log\left(\sigma(\theta^\top x_i)\right)\ +\ (1-y_i)\log\left(1 - \sigma(\theta^\top x_i)\right).
\end{equation}\]</div>
</div>
<hr class="docutils"/>
<div class="section" id="bonus-section-2-more-detail-about-model-selection">
<h2>Bonus Section 2: More detail about model selection<a class="headerlink" href="#bonus-section-2-more-detail-about-model-selection" title="Permalink to this headline">¶</a></h2>
<p>In the final exercise, we used all of the data to choose the hyperparameters. That means we don’t have any fresh data left over to evaluate the performance of the selected model. In practice, you would want to have two <em>nested</em> layers of cross-validation, where the final evaluation is performed on data that played no role in selecting or training the model.</p>
<p>Indeed, the proper method for splitting your data to choose hyperparameters can get confusing. Here’s a guide that the authors of this notebook developed while writing a tutorial on using machine learning for neural decoding <a class="reference external" href="https://arxiv.org/abs/1708.00909">arxiv:1708.00909</a>.</p>
<p>Execute to see schematic</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute to see schematic</span>
<span class="kn">import</span> <span class="nn">IPython</span>
<span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="s2">"http://kordinglab.com/images/others/CV-01.png"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W1D3_GeneralizedLinearModels/instructor"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="W1D3_Tutorial1.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Tutorial 1: GLMs for Encoding</p>
</div>
</a>
<a class="right-next" href="W1D3_Outro.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Outro</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By Neuromatch<br/>
<div class="extra_footer">
<div>
<a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png"/></a>
<a href="https://opensource.org/licenses/BSD-3-Clause"><img src="https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667"/></a>
The contents of this repository are shared under the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Software elements are additionally licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD (3-Clause) License</a>.
</div>
</div>
</p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>