
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 2: Statistical Inference — Neuromatch Academy: Computational Neuroscience (instructor's version)</title>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/custom.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-logo-square-4xp.png" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="W0D5_Outro.html" rel="next" title="Outro"/>
<link href="W0D5_Tutorial1.html" rel="prev" title="Tutorial 1: Probability Distributions"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../../../_static/nma-logo-square-4xp.png"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Computational Neuroscience (instructor's version)</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</input></li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using discord
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../TechnicalHelp/Links_Policy.html">
   Quick links and policies
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../prereqs/ComputationalNeuroscience.html">
   Prerequisites and preparatory materials for NMA Computational Neuroscience
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../tatraining/TA_Training_CN.html">
   TA Training - Computational Neuroscience
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Pre-reqs Refresher
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/chapter_title.html">
   Neuro Video Series (W0D0)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial1.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial2.html">
     Human Psychophysics
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial3.html">
     Behavioral Readout
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial4.html">
     Live in Lab
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial5.html">
     Brain Signals: Spiking Activity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial6.html">
     Brain Signals: LFP
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial7.html">
     Brain Signals: EEG &amp; MEG
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial8.html">
     Brain Signals: fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial9.html">
     Brain Signals: Calcium Imaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial10.html">
     Stimulus Representation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial11.html">
     Neurotransmitters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial12.html">
     Neurons to Consciousness
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/chapter_title.html">
   Python Workshop 1 (W0D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/instructor/W0D1_Tutorial1.html">
     Tutorial: LIF Neuron Part I
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/chapter_title.html">
   Python Workshop 2 (W0D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/instructor/W0D2_Tutorial1.html">
     Tutorial 1: LIF Neuron Part II
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D3_LinearAlgebra/chapter_title.html">
   Linear Algebra (W0D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial1.html">
     Tutorial 1: Vectors
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial2.html">
     Tutorial 2: Matrices
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial3.html">
     Bonus Tutorial: Discrete Dynamical Systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D4_Calculus/chapter_title.html">
   Calculus (W0D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial1.html">
     Tutorial 1: Differentiation and Integration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial2.html">
     Tutorial 2: Differential Equations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial3.html">
     Tutorial 3: Numerical Methods
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Statistics (W0D5)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="W0D5_Tutorial1.html">
     Tutorial 1: Probability Distributions
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 2: Statistical Inference
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W0D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W0D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Intro to Modeling
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_ModelTypes/chapter_title.html">
   Model Types (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial1.html">
     Tutorial 1: “What” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial2.html">
     Tutorial 2: “How” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial3.html">
     Tutorial 3: “Why” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial4.html">
     Tutorial 4: Model Discussions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ModelingPractice/chapter_title.html">
   Modeling Practice (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Tutorial1.html">
     Tutorial 1: Framing the Question
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_ModelFitting/chapter_title.html">
   Model Fitting (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial1.html">
     Tutorial 1: Linear regression with MSE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial2.html">
     Tutorial 2: Linear regression with MLE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial3.html">
     Tutorial 3: Confidence intervals and bootstrapping
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial4.html">
     Tutorial 4: Multiple linear regression and polynomial regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial5.html">
     Tutorial 5: Model Selection: Bias-variance trade-off
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial6.html">
     Tutorial 6: Model Selection: Cross-validation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/chapter_title.html">
   Generalized Linear Models (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial1.html">
     Tutorial 1: GLMs for Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial2.html">
     Tutorial 2: Classifiers and regularizers
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/chapter_title.html">
   Dimensionality Reduction (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial1.html">
     Tutorial 1: Geometric view of data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial2.html">
     Tutorial 2: Principal Component Analysis
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial3.html">
     Tutorial 3: Dimensionality Reduction &amp; Reconstruction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial4.html">
     Tutorial 4:  Nonlinear Dimensionality Reduction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_DeepLearning/chapter_title.html">
   Deep Learning (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial1.html">
     Tutorial 1: Decoding Neural Responses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial2.html">
     Tutorial 2: Convolutional Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial3.html">
     Tutorial 3: Building and Evaluating Normative Encoding Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial4.html">
     Bonus Tutorial: Diving Deeper into Decoding &amp; Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Bonus_Autoencoders/chapter_title.html">
   Autoencoders (Bonus)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial1.html">
     Tutorial 1: Intro to Autoencoders
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial2.html">
     Tutorial 2: Autoencoder extensions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial3.html">
     Tutorial 3: Autoencoders applications
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/MachineLearning.html">
   Machine Learning Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Dynamical Systems
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_LinearSystems/chapter_title.html">
   Linear Systems (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial1.html">
     Tutorial 1: Linear dynamical systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial2.html">
     Tutorial 2: Markov Processes
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial3.html">
     Tutorial 3: Combining determinism and stochasticity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial4.html">
     Tutorial 4: Autoregressive models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/chapter_title.html">
   Biological Neuron Models (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial1.html">
     Tutorial 1: The Leaky Integrate-and-Fire (LIF) Neuron Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial2.html">
     Tutorial 2: Effects of Input Correlation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial3.html">
     Tutorial 3: Synaptic transmission - Models of static and dynamic synapses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial4.html">
     Bonus Tutorial: Spike-timing dependent plasticity (STDP)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_DynamicNetworks/chapter_title.html">
   Dynamic Networks (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial1.html">
     Tutorial 1: Neural Rate Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial2.html">
     Tutorial 2: Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial3.html">
     Bonus Tutorial: Extending the Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/DynamicalSystems.html">
   Dynamical Systems Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Stochastic Processes
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_BayesianDecisions/chapter_title.html">
   Bayesian Decisions (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial1.html">
     Tutorial 1: Bayes with a binary hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial2.html">
     Tutorial 2: Bayesian inference and decisions with continuous hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial3.html">
     Bonus Tutorial : Fitting to data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_HiddenDynamics/chapter_title.html">
   Hidden Dynamics (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial1.html">
     Tutorial 1: Sequential Probability Ratio Test
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial2.html">
     Tutorial 2: Hidden Markov Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial3.html">
     Tutorial 3: The Kalman Filter
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial4.html">
     Bonus Tutorial 4: The Kalman Filter, part 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial5.html">
     Bonus Tutorial 5: Expectation Maximization for spiking neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_OptimalControl/chapter_title.html">
   Optimal Control (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Tutorial1.html">
     Tutorial 1: Optimal Control for Discrete States
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Tutorial2.html">
     Tutorial 2: Optimal Control for Continuous State
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/chapter_title.html">
   Reinforcement Learning (W3D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial1.html">
     Tutorial 1: Learning to Predict
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial2.html">
     Tutorial 2: Learning to Act: Multi-Armed Bandits
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial3.html">
     Tutorial 3: Learning to Act: Q-Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Tutorial4.html">
     Tutorial 4: Model-Based Reinforcement Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D4_ReinforcementLearning/instructor/W3D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D5_NetworkCausality/chapter_title.html">
   Network Causality (W3D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
<label for="toctree-checkbox-24">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial1.html">
     Tutorial 1: Interventions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial2.html">
     Tutorial 2: Correlations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial3.html">
     Tutorial 3: Simultaneous fitting/regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial4.html">
     Tutorial 4: Instrumental Variables
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/StochasticProcesses.html">
   Stochastic Processes Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
<label for="toctree-checkbox-25">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through4.html">
     Modeling Steps 1 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through10.html">
     Modeling Steps 5 - 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModel.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProject.html">
     Example Data Project: the Train Illusion
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/datasets_overview.html">
   Datasets
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
<label for="toctree-checkbox-26">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/neurons.html">
     Neurons
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
<label for="toctree-checkbox-27">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/neurons_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/fMRI.html">
     fMRI
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
<label for="toctree-checkbox-28">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/fMRI_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/ECoG.html">
     ECoG
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
<label for="toctree-checkbox-29">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/ECoG_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/behavior.html">
     Behavior
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
<label for="toctree-checkbox-30">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/behavior_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/theory.html">
     Theory
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
<label for="toctree-checkbox-31">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/theory/README.html">
       Guide
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_templates.html">
   Project Templates
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/project_2020_highlights.html">
   Projects 2020
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/>
<label for="toctree-checkbox-32">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/neurons.html">
     Neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/theory.html">
     Theory
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/behavior.html">
     Behavior
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/fMRI.html">
     fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/eeg.html">
     EEG
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
<button aria-label="Launch interactive content" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-rocket"></i>
</button>
<div class="menu-dropdown__content">
<ul>
</ul>
</div>
</div>
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="menu-dropdown menu-dropdown-repository-buttons">
<button aria-label="Source repositories" class="headerbtn menu-dropdown__trigger">
<i class="fab fa-github"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/instructor-course-content" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="headerbtn__text-container">repository</span>
</a>
</li>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/instructor-course-content/issues/new?title=Issue%20on%20page%20%2Ftutorials/W0D5_Statistics/instructor/W0D5_Tutorial2.html&amp;body=Your%20issue%20content%20here." title="Open an issue">
<span class="headerbtn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="headerbtn__text-container">open issue</span>
</a>
</li>
</ul>
</div>
</div>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../../../_sources/tutorials/W0D5_Statistics/instructor/W0D5_Tutorial2.ipynb" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 2: Statistical Inference
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-helper-functions">
     Plotting &amp; Helper functions
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-basic-probability">
   Section 1: Basic probability
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-basic-probability-theory">
     Section 1.1: Basic probability theory
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-basic-probability">
       Video 1: Basic Probability
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#math-exercise-1-1-probability-example">
       Math Exercise 1.1: Probability example
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#a-product">
       A) Product
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#b-joint-probability-generally">
       B) Joint probability generally
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#c-conditional-probability">
       C) Conditional probability
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#d-marginal-probability">
       D) Marginal probability
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-markov-chains">
     Section 1.2: Markov chains
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-markov-chains">
       Video 2: Markov Chains
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-2-markov-chains">
       Coding exercise 1.2 Markov chains
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-statistical-inference-and-likelihood">
   Section 2: Statistical inference and likelihood
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-likelihoods">
     Section 2.1: Likelihoods
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-statistical-inference-and-likelihood">
       Video 3: Statistical inference and likelihood
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-computing-likelihood">
       Coding Exercise 2.1: Computing likelihood
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-maximum-likelihood">
     Section 2.2: Maximum likelihood
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-maximum-likelihood">
       Video 4: Maximum likelihood
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-1-searching-for-best-parameters">
       Section 2.2.1: Searching for best parameters
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-2-2-maximum-likelihood-inference">
         Interactive Demo 2.2: Maximum likelihood inference
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-2-optimization-to-find-parameters">
       Section 2.2.2: Optimization to find parameters
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-2-maximum-likelihood-estimation">
         Coding Exercise 2.2: Maximum Likelihood Estimation
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-3-analytical-solution">
       Section 2.2.3: Analytical solution
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-bayesian-inference">
   Section 3: Bayesian Inference
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-bayes">
     Section 3.1: Bayes
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-bayesian-inference-with-gaussian-distribution">
       Video 5: Bayesian inference with Gaussian distribution
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-3-1-bayesian-inference-with-gaussian-distribution">
       Think! 3.1: Bayesian inference with Gaussian distribution
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-conjugate-priors">
     Section 3.2: Conjugate priors
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-6-conjugate-priors">
       Video 6: Conjugate priors
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-3-2-conjugate-priors">
       Interactive Demo 3.2: Conjugate priors
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-3-2-bayesian-brains">
       Think! 3.2: Bayesian Brains
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-7-summary">
     Video 7: Summary
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-coding-exercise-1-finding-the-posterior-computationally">
     Bonus Coding Exercise 1: Finding the posterior computationally
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-coding-exercise-2-bayes-net">
     Bonus Coding Exercise 2: Bayes Net
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-think-causality-in-the-brain">
     Bonus Think!: Causality in the Brain
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>Tutorial 2: Statistical Inference</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 2: Statistical Inference
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-helper-functions">
     Plotting &amp; Helper functions
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-basic-probability">
   Section 1: Basic probability
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-1-basic-probability-theory">
     Section 1.1: Basic probability theory
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-basic-probability">
       Video 1: Basic Probability
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#math-exercise-1-1-probability-example">
       Math Exercise 1.1: Probability example
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#a-product">
       A) Product
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#b-joint-probability-generally">
       B) Joint probability generally
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#c-conditional-probability">
       C) Conditional probability
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#d-marginal-probability">
       D) Marginal probability
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-2-markov-chains">
     Section 1.2: Markov chains
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-2-markov-chains">
       Video 2: Markov Chains
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-2-markov-chains">
       Coding exercise 1.2 Markov chains
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-statistical-inference-and-likelihood">
   Section 2: Statistical inference and likelihood
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-1-likelihoods">
     Section 2.1: Likelihoods
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-3-statistical-inference-and-likelihood">
       Video 3: Statistical inference and likelihood
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-1-computing-likelihood">
       Coding Exercise 2.1: Computing likelihood
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-maximum-likelihood">
     Section 2.2: Maximum likelihood
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-4-maximum-likelihood">
       Video 4: Maximum likelihood
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-1-searching-for-best-parameters">
       Section 2.2.1: Searching for best parameters
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-2-2-maximum-likelihood-inference">
         Interactive Demo 2.2: Maximum likelihood inference
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-2-optimization-to-find-parameters">
       Section 2.2.2: Optimization to find parameters
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-2-2-maximum-likelihood-estimation">
         Coding Exercise 2.2: Maximum Likelihood Estimation
        </a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-2-3-analytical-solution">
       Section 2.2.3: Analytical solution
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-bayesian-inference">
   Section 3: Bayesian Inference
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-1-bayes">
     Section 3.1: Bayes
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-5-bayesian-inference-with-gaussian-distribution">
       Video 5: Bayesian inference with Gaussian distribution
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-3-1-bayesian-inference-with-gaussian-distribution">
       Think! 3.1: Bayesian inference with Gaussian distribution
      </a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-2-conjugate-priors">
     Section 3.2: Conjugate priors
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-6-conjugate-priors">
       Video 6: Conjugate priors
      </a>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-3-2-conjugate-priors">
       Interactive Demo 3.2: Conjugate priors
      </a>
<ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-3-2-bayesian-brains">
       Think! 3.2: Bayesian Brains
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-7-summary">
     Video 7: Summary
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-coding-exercise-1-finding-the-posterior-computationally">
     Bonus Coding Exercise 1: Finding the posterior computationally
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-coding-exercise-2-bayes-net">
     Bonus Coding Exercise 2: Bayes Net
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-think-causality-in-the-brain">
     Bonus Think!: Causality in the Brain
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/precourse/blob/main/tutorials/W0D5_Statistics/instructor/W0D5_Tutorial2.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/precourse/main/tutorials/W0D5_Statistics/instructor/W0D5_Tutorial2.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<div class="section" id="tutorial-2-statistical-inference">
<h1>Tutorial 2: Statistical Inference<a class="headerlink" href="#tutorial-2-statistical-inference" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 0, Day 5: Probability &amp; Statistics</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Ulrik Beierholm</p>
<p><strong>Content reviewers:</strong> Natalie Schaworonkow, Keith van Antwerp, Anoop Kulkarni, Pooya Pakarian, Hyosub Kim</p>
<p><strong>Production editors:</strong> Ethan Cheng, Ella Batty</p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p><hr class="docutils"/>
<p>#Tutorial Objectives</p>
<p>This tutorial builds on Tutorial 1 by explaining how to do inference through inverting the generative process.</p>
<p>By completing the exercises in this tutorial, you should:</p>
<ul class="simple">
<li><p>understand what the likelihood function is, and have some intuition of why it is important</p></li>
<li><p>know how to summarise the Gaussian distribution using mean and variance</p></li>
<li><p>know how to maximise a likelihood function</p></li>
<li><p>be able to do simple inference in both classical and Bayesian ways</p></li>
<li><p>(Optional) understand how Bayes Net can be used to model causal relationships</p></li>
</ul>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">default_rng</span>  <span class="c1"># a default random number generator</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>  <span class="c1"># interactive display</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">HBox</span><span class="p">,</span> <span class="n">Layout</span><span class="p">,</span> <span class="n">VBox</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">Label</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-helper-functions">
<h2>Plotting &amp; Helper functions<a class="headerlink" href="#plotting-helper-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plotting &amp; Helper functions</span>

<span class="k">def</span> <span class="nf">plot_hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">xlabel</span><span class="p">,</span> <span class="n">figtitle</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">num_bins</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Plot the given data as a histogram.</span>

<span class="sd">    Args:</span>
<span class="sd">      data (ndarray): array with data to plot as histogram</span>
<span class="sd">      xlabel (str): label of x-axis</span>
<span class="sd">      figtitle (str): title of histogram plot (default is no title)</span>
<span class="sd">      num_bins (int): number of bins for histogram (default is 10)</span>

<span class="sd">    Returns:</span>
<span class="sd">      count (ndarray): number of samples in each histogram bin</span>
<span class="sd">      bins (ndarray): center of each histogram bin</span>
<span class="sd">  """</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Count'</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">num_bins</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">count</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="n">num_bins</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">count</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>  <span class="c1"># 10 bins default</span>
  <span class="k">if</span> <span class="n">figtitle</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">figtitle</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">count</span><span class="p">,</span> <span class="n">bins</span>

<span class="k">def</span> <span class="nf">plot_gaussian_samples_true</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">xspace</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">xlabel</span><span class="p">,</span> <span class="n">ylabel</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Plot a histogram of the data samples on the same plot as the gaussian</span>
<span class="sd">  distribution specified by the give mu and sigma values.</span>

<span class="sd">    Args:</span>
<span class="sd">      samples (ndarray): data samples for gaussian distribution</span>
<span class="sd">      xspace (ndarray): x values to sample from normal distribution</span>
<span class="sd">      mu (scalar): mean parameter of normal distribution</span>
<span class="sd">      sigma (scalar): variance parameter of normal distribution</span>
<span class="sd">      xlabel (str): the label of the x-axis of the histogram</span>
<span class="sd">      ylabel (str): the label of the y-axis of the histogram</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing.</span>
<span class="sd">  """</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
  <span class="c1"># num_samples = samples.shape[0]</span>

  <span class="n">count</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># probability density function</span>

  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="s1">'r-'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_likelihoods</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">,</span> <span class="n">mean_vals</span><span class="p">,</span> <span class="n">variance_vals</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Plot the likelihood values on a heatmap plot where the x and y axes match</span>
<span class="sd">  the mean and variance parameter values the likelihoods were computed for.</span>

<span class="sd">    Args:</span>
<span class="sd">      likelihoods (ndarray): array of computed likelihood values</span>
<span class="sd">      mean_vals (ndarray): array of mean parameter values for which the</span>
<span class="sd">                            likelihood was computed</span>
<span class="sd">      variance_vals (ndarray): array of variance parameter values for which the</span>
<span class="sd">                            likelihood was computed</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing.</span>
<span class="sd">  """</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">)</span>

  <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">figure</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
  <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'log likelihood'</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=-</span><span class="mi">90</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">"bottom"</span><span class="p">)</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mean_vals</span><span class="p">)))</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">variance_vals</span><span class="p">)))</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">mean_vals</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">variance_vals</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Mean'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Variance'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">posterior_plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prior</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                   <span class="n">posterior_pointwise</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  Plots normalized Gaussian distributions and posterior.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (numpy array of floats):         points at which the likelihood has been evaluated</span>
<span class="sd">        auditory (numpy array of floats):  normalized probabilities for auditory likelihood evaluated at each `x`</span>
<span class="sd">        visual (numpy array of floats):    normalized probabilities for visual likelihood evaluated at each `x`</span>
<span class="sd">        posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`</span>
<span class="sd">        ax: Axis in which to plot. If None, create new axis.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Nothing.</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">likelihood</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">prior</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">posterior_pointwise</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">posterior_pointwise</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="s1">'-C1'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Auditory'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="s1">'-C0'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Visual'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">posterior_pointwise</span><span class="p">,</span> <span class="s1">'-C2'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Posterior'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Probability'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Orientation (Degrees)'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">ax</span>


<span class="k">def</span> <span class="nf">plot_classical_vs_bayesian_normal</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">mu_classic</span><span class="p">,</span> <span class="n">var_classic</span><span class="p">,</span>
                                      <span class="n">mu_bayes</span><span class="p">,</span> <span class="n">var_bayes</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Helper function to plot optimal normal distribution parameters for varying</span>
<span class="sd">  observed sample sizes using both classic and Bayesian inference methods.</span>

<span class="sd">    Args:</span>
<span class="sd">      num_points (int): max observed sample size to perform inference with</span>
<span class="sd">      mu_classic (ndarray): estimated mean parameter for each observed sample size</span>
<span class="sd">                                using classic inference method</span>
<span class="sd">      var_classic (ndarray): estimated variance parameter for each observed sample size</span>
<span class="sd">                                using classic inference method</span>
<span class="sd">      mu_bayes (ndarray): estimated mean parameter for each observed sample size</span>
<span class="sd">                                using Bayesian inference method</span>
<span class="sd">      var_bayes (ndarray): estimated variance parameter for each observed sample size</span>
<span class="sd">                                using Bayesian inference method</span>

<span class="sd">    Returns:</span>
<span class="sd">      Nothing.</span>
<span class="sd">  """</span>
  <span class="n">xspace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'n data points'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'mu'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">mu_classic</span><span class="p">,</span><span class="s1">'r-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Classical"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">mu_bayes</span><span class="p">,</span><span class="s1">'b-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Bayes"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'n data points'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'sigma^2'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">var_classic</span><span class="p">,</span><span class="s1">'r-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Classical"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xspace</span><span class="p">,</span> <span class="n">var_bayes</span><span class="p">,</span><span class="s1">'b-'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Bayes"</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-basic-probability">
<h1>Section 1: Basic probability<a class="headerlink" href="#section-1-basic-probability" title="Permalink to this headline">¶</a></h1>
<div class="section" id="section-1-1-basic-probability-theory">
<h2>Section 1.1: Basic probability theory<a class="headerlink" href="#section-1-1-basic-probability-theory" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-1-basic-probability">
<h3>Video 1: Basic Probability<a class="headerlink" href="#video-1-basic-probability" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
<p>This video covers basic probability theory, including complementary probability, conditional probability, joint probability, and marginalisation.</p>
<details>
<summary> <font color="blue">Click here for text recap of video </font></summary>
<p>Previously we were only looking at sampling or properties of a single variables, but as we will now move on to statistical inference, it is useful to go over basic probability theory.</p>
<p>As a reminder, probability has to be in the range 0 to 1
<span class="math notranslate nohighlight">\(P(A) \in  [0,1] \)</span></p>
<p>and the complementary can always be defined as</p>
<p><span class="math notranslate nohighlight">\(P(\neg A) = 1-P(A)\)</span></p>
<p>When we have two variables, the <em>conditional probability</em> of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span> is</p>
<p><span class="math notranslate nohighlight">\(P (A|B) = P (A \cap B)/P (B)=P (A, B)/P (B)\)</span></p>
<p>while the <em>joint probability</em> of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is</p>
<p><span class="math notranslate nohighlight">\(P(A \cap B)=P(A,B) = P(B|A)P(A) = P(A|B)P(B) \)</span></p>
<p>We can then also define the process of <em>marginalisation</em> (for discrete variables) as</p>
<p><span class="math notranslate nohighlight">\(P(A)=\sum P(A,B)=\sum P(A|B)P(B)\)</span></p>
<p>where the summation is over the possible values of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>As an example if <span class="math notranslate nohighlight">\(B\)</span> is a binary variable that can take values <span class="math notranslate nohighlight">\(B+\)</span> or <span class="math notranslate nohighlight">\(B0\)</span> then
<span class="math notranslate nohighlight">\(P(A)=\sum P(A,B)=P(A|B+)P(B+)+ P(A|B0)P(B0) \)</span>.</p>
<p>For continuous variables marginalization is given as
<span class="math notranslate nohighlight">\(P(A)=\int P(A,B) dB=\int P(A|B)P(B) dB\)</span></p>
</details></div>
<div class="section" id="math-exercise-1-1-probability-example">
<h3>Math Exercise 1.1: Probability example<a class="headerlink" href="#math-exercise-1-1-probability-example" title="Permalink to this headline">¶</a></h3>
<p>To remind ourselves of how to use basic probability theory we will do a short exercise (no coding needed!), based on measurement of binary probabilistic neural responses.
As shown by Hubel and Wiesel in 1959 there are neurons in primary visual cortex that respond to different orientations of visual stimuli, with different neurons being sensitive to different orientations. The numbers in the following are however purely fictional.</p>
<p>Imagine that your collaborator tells you that they have recorded the activity of visual neurons while presenting either a horizontal or vertical grid as a visual stimulus. The activity of the neurons is measured as binary: they are either active or inactive in response to the stimulus.</p>
<p>After recording from a large number of neurons they find that when presenting a horizontal grid, on average 40% of neurons are active, while 30% respond to vertical grids.</p>
<p>We will use the following notation to indicate the probability that a randomly chosen neuron responds to horizontal grids</p>
<div class="amsmath math notranslate nohighlight" id="equation-bf39c29a-47d4-45ae-8ab1-db49bc9fdd23">
<span class="eqno">(105)<a class="headerlink" href="#equation-bf39c29a-47d4-45ae-8ab1-db49bc9fdd23" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(h_+)=0.4
\end{equation}\]</div>
<p>and this to show the probability it responds to vertical:</p>
<div class="amsmath math notranslate nohighlight" id="equation-41bd4161-086d-4774-b0df-2f569fdaad92">
<span class="eqno">(106)<a class="headerlink" href="#equation-41bd4161-086d-4774-b0df-2f569fdaad92" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(v_+)=0.3
\end{equation}\]</div>
<p>We can find the complementary event, that the neuron does not respond to the horizontal grid, using the fact that these events must add up to 1. We see that the probability the neuron does not respond to the horizontal grid (<span class="math notranslate nohighlight">\(h_0\)</span>) is</p>
<div class="amsmath math notranslate nohighlight" id="equation-90c91149-c339-49a6-b20b-633e4a2f2b33">
<span class="eqno">(107)<a class="headerlink" href="#equation-90c91149-c339-49a6-b20b-633e4a2f2b33" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(h_0)=1-P(h_+)=0.6
\end{equation}\]</div>
<p>and that the probability to not respond to vertical is</p>
<div class="amsmath math notranslate nohighlight" id="equation-c1573d6e-27d5-402e-8922-f9d4460d5351">
<span class="eqno">(108)<a class="headerlink" href="#equation-c1573d6e-27d5-402e-8922-f9d4460d5351" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(v_0)=1-P(v_+)=0.7
\end{equation}\]</div>
<p>We will practice computing various probabilities in this framework.</p>
</div>
<div class="section" id="a-product">
<h3>A) Product<a class="headerlink" href="#a-product" title="Permalink to this headline">¶</a></h3>
<p>Assuming that the horizontal and vertical orientation selectivity are independent, what is the probability that a randomly chosen neuron is sensitive to both horizontal and vertical orientations?</p>
<p>Hint: Two events are independent if the outcome of one does not affect the outcome of the other.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">Independent here means that P(h+,v+) = P(h+)𝑃(v+)</span>

<span class="sd">P(h+,v+) = P(h+) P(v+) = 0.4*0.3 = 0.12</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="b-joint-probability-generally">
<h3>B) Joint probability generally<a class="headerlink" href="#b-joint-probability-generally" title="Permalink to this headline">¶</a></h3>
<p>A collaborator informs you that actually these are not independent. Of those neurons that respond to vertical, only 10 percent also respond to horizontal, i.e. the probability of responding to horizonal <em>given</em> that it responds to vertical is <span class="math notranslate nohighlight">\(P(h+|v+)=0.1\)</span></p>
<p>Given this new information, what is now the probability that a randomly chosen neuron is sensitive to both horizontal and vertical orientations?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">Remember that joint probability can generally be expressed as  𝑃(𝑎,𝑏)=𝑃(𝑎|𝑏)𝑃(𝑏)</span>

<span class="sd">P(h+,v+) = P(h+|v+)P(v+) = 0.1 ∗ 0.3 = 0.03</span>

<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="c-conditional-probability">
<h3>C) Conditional probability<a class="headerlink" href="#c-conditional-probability" title="Permalink to this headline">¶</a></h3>
<p>You start measuring from a neuron and find that it responds to horizontal orientations. What is now the probability that it also responds to vertical, i.e., <span class="math notranslate nohighlight">\(P(v_+|h_+)\)</span>)?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">The conditional probability is given by P(a|b) = P(a,b)/P(b)</span>

<span class="sd">P(v+|h+) = P(v+,h+)/P(h+) = P(h+|v+)P(v+)/P(h+) = 0.1∗0.3/0.4 = 0.075</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="d-marginal-probability">
<h3>D) Marginal probability<a class="headerlink" href="#d-marginal-probability" title="Permalink to this headline">¶</a></h3>
<p>Lastly, let’s check that everything has been done correctly. Given our knowledge about the conditional probabilities, we should be able to use marginalisation to recover the marginal probability of a random neuron responding to vertical orientations, i.e.,<span class="math notranslate nohighlight">\(P(v_+)\)</span>. We know from above that this should equal 0.3.</p>
<p>Calculate <span class="math notranslate nohighlight">\(P(v_+)\)</span> based on the conditional probabilities for <span class="math notranslate nohighlight">\(P(v_+|h_+)\)</span> and <span class="math notranslate nohighlight">\(P(v_+|h_0)\)</span> (the latter which you will need to calculate).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">The first step is to calculute:</span>
<span class="sd">P(v+|h0) = P(h0|v+)P(v+)/P(h0) = (1 − 0.1)∗0.3/(1 − 0.4) = 0.45</span>

<span class="sd">Then use the property of marginalisation (discrete version)</span>

<span class="sd">P(a) = Sum(i*P(a|b=i)P(b=i))</span>

<span class="sd">P(v+) = P(v+|h+)P(h+) + P(v+|h0)P(h0) = 0.075∗0.4 + 0.45∗(1 − 0.4) = 0.3</span>

<span class="sd">Phew, we recovered the correct value!</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-1-2-markov-chains">
<h2>Section 1.2: Markov chains<a class="headerlink" href="#section-1-2-markov-chains" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-2-markov-chains">
<h3>Video 2: Markov Chains<a class="headerlink" href="#video-2-markov-chains" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="coding-exercise-1-2-markov-chains">
<h3>Coding exercise 1.2 Markov chains<a class="headerlink" href="#coding-exercise-1-2-markov-chains" title="Permalink to this headline">¶</a></h3>
<p>We will practice more probability theory by looking at <strong>Markov chains</strong>. The Markov property specifies that you can fully encapsulate the important properties of a system based on its <em>current</em> state at the current time, any previous history does not matter. It is memoryless.</p>
<p>As an example imagine that a rat is able to move freely between 3 areas: a dark rest area
(<span class="math notranslate nohighlight">\(state=1\)</span>), a nesting area (<span class="math notranslate nohighlight">\(state=2\)</span>) and a bright area for collecting food (<span class="math notranslate nohighlight">\(state=3\)</span>). Every 5 minutes (timepoint <span class="math notranslate nohighlight">\(i\)</span>) we record the rat’s location. We can use a <strong>categorical distribution</strong> to look at the probability that the rat moves to one state from another.</p>
<p>The table below shows the probability of the rat transitioning from one area to another between timepoints (<span class="math notranslate nohighlight">\(state_i\)</span> to <span class="math notranslate nohighlight">\(state_{i+1}\)</span>).</p>
<div class="amsmath math notranslate nohighlight" id="equation-39487a41-aa39-4665-9a30-9bc6d6aa0370">
<span class="eqno">(109)<a class="headerlink" href="#equation-39487a41-aa39-4665-9a30-9bc6d6aa0370" title="Permalink to this equation">¶</a></span>\[\begin{matrix}
\hline
state_{i} &amp;P(state_{i+1}=1|state_i=*) &amp;P(state_{i+1}=2|state_i=*) &amp; P(state_{i+1}=3|state=_i*) \\ \hline
state_{i}=1 &amp; 0.2 &amp; 0.6 &amp; 0.2 \\
state_{i}=2 &amp; 0.6 &amp; 0.3 &amp; 0.1 \\
state_{i}=3 &amp; 0.8 &amp; 0.2 &amp; 0.0 \\
\hline
\end{matrix}\]</div>
<p>We are modeling this as a Markov chain, so the animal is only in one of the states at a time and can transition between the states.</p>
<p>We want to get the probability of each state at time <span class="math notranslate nohighlight">\(i+1\)</span>. We know from Section 1.1 that we can use marginalisation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-edf5f626-3c7c-48b0-91cd-cd3ed4f69c51">
<span class="eqno">(110)<a class="headerlink" href="#equation-edf5f626-3c7c-48b0-91cd-cd3ed4f69c51" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(state_{i+1} = 1) = P(state_{i+1}=1|state_i=1)P(state_i = 1) +  P(state_{i+1}=1|state_i=2)P(state_i = 2) +  P(state_{i+1}=1|state_i=3)P(state_i = 3)
\end{equation}\]</div>
<p>Let’s say we had a row vector (a vector defined as a row, not a column so matrix multiplication will work out) of the probabilities of the current state:</p>
<div class="amsmath math notranslate nohighlight" id="equation-de03ceed-06d4-4dbd-850f-31c1a53bd22b">
<span class="eqno">(111)<a class="headerlink" href="#equation-de03ceed-06d4-4dbd-850f-31c1a53bd22b" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P_i = [P(state_i = 1), P(state_i = 2), P(state_i = 3) ]
\end{equation}\]</div>
<p>If we actually know where the rat is at the current time point, this would be deterministic (e.g., <span class="math notranslate nohighlight">\(P_i = [0, 1, 0]\)</span> if the rat is in state 2). Otherwise, this could be probabilistic (e.g. <span class="math notranslate nohighlight">\(P_i = [0.1, 0.7, 0.2]\)</span>).</p>
<p>To compute the vector of probabilities of the state at the time <span class="math notranslate nohighlight">\(i+1\)</span>, we can use linear algebra and multiply our vector of the probabilities of the current state with the transition matrix.  Recall your matrix multiplication skills from W0D3 to check this!</p>
<div class="amsmath math notranslate nohighlight" id="equation-340b1377-e843-4486-9981-0c2943d2d41a">
<span class="eqno">(112)<a class="headerlink" href="#equation-340b1377-e843-4486-9981-0c2943d2d41a" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P_{i+1} = P_{i} T
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is our transition matrix.</p>
<p>This is the same formula for every step, which allows us to get the probabilities for a time more than 1 step in advance easily. If we started at <span class="math notranslate nohighlight">\(i=0\)</span> and wanted to look at the probabilities at step <span class="math notranslate nohighlight">\(i=2\)</span>, we could do:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b612c048-90f2-4238-baea-9ae41f47ab2b">
<span class="eqno">(113)<a class="headerlink" href="#equation-b612c048-90f2-4238-baea-9ae41f47ab2b" title="Permalink to this equation">¶</a></span>\[\begin{align}
P_{1} &amp;= P_{0}T\\
P_{2} &amp;= P_{1}T = P_{0}TT = P_{0}T^2\\
\end{align}\]</div>
<p>So, every time we take a further step we can just multiply with the transition matrix again. So, the probability vector of states at j timepoints after the current state at timepoint i is equal to the probability vector at timepoint i times the transition matrix raised to the jth power.</p>
<div class="amsmath math notranslate nohighlight" id="equation-17e0e65d-3e1d-422e-8b79-117c7ad61310">
<span class="eqno">(114)<a class="headerlink" href="#equation-17e0e65d-3e1d-422e-8b79-117c7ad61310" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P_{i + j} = P_{i}T^j
\end{equation}\]</div>
<p>If the animal starts in area 2, what is the probability the animal will again be in area 2 when we check on it 20 minutes (4 transitions) later?</p>
<p>Fill in the transition matrix in the code below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">###################################################################</span>
<span class="c1">## TODO for student</span>
<span class="c1">## Fill out the following then remove</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: compute state probabilities after 4 transitions"</span><span class="p">)</span>
<span class="c1">###################################################################</span>

<span class="c1"># Transition matrix</span>
<span class="n">transition_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span> <span class="mf">.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Initial state, p0</span>
<span class="n">p0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Compute the probabilities 4 transitions later (use np.linalg.matrix_power to raise a matrix a power)</span>
<span class="n">p4</span> <span class="o">=</span> <span class="o">...</span>

<span class="c1"># The second area is indexed as 1 (Python starts indexing at 0)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The probability the rat will be in area 2 after 4 transitions is: </span><span class="si">{</span><span class="n">p4</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="c1"># Transition matrix</span>
<span class="n">transition_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span> <span class="mf">.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Initial state, p0</span>
<span class="n">p0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Compute the probabilities 4 transitions later (use np.linalg.matrix_power to raise a matrix a power)</span>
<span class="n">p4</span> <span class="o">=</span> <span class="n">p0</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_power</span><span class="p">(</span><span class="n">transition_matrix</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># The second area is indexed as 1 (Python starts indexing at 0)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The probability the rat will be in area 2 after 4 transitions is: </span><span class="si">{</span><span class="n">p4</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You should get a probability of 0.4311, i.e., there is a 43.11% chance that you will find the rat in area 2 in 20 minutes.</p>
<p>What is the average amount of time spent by the rat in each of the states?</p>
<p>Implicit in the question is the idea that we can start off with a random initial state and then measure how much relative time is spent in each area. If we make a few assumptions (e.g. ergodic or ‘randomly mixing’ system), we can instead start with an initial random distribution and see how the final probabilities of each state after many time steps (100) to estimate the time spent in each state.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize random initial distribution</span>
<span class="n">p_random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">/</span><span class="mi">3</span>

<span class="c1">###################################################################</span>
<span class="c1">## TODO for student: Fill compute the state matrix after 100 transitions</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: need to complete computation below"</span><span class="p">)</span>
<span class="c1">###################################################################</span>

<span class="c1"># Fill in the missing line to get the state matrix after 100 transitions, like above</span>
<span class="n">p_average_time_spent</span> <span class="o">=</span> <span class="o">...</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The proportion of time spend by the rat in each of the three states is: </span><span class="si">{</span><span class="n">p_average_time_spent</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="c1"># Initialize random initial distribution</span>
<span class="n">p_random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">/</span><span class="mi">3</span>

<span class="c1"># Fill in the missing line to get the state matrix after 100 transitions, like above</span>
<span class="n">p_average_time_spent</span> <span class="o">=</span> <span class="n">p_random</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_power</span><span class="p">(</span><span class="n">transition_matrix</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The proportion of time spend by the rat in each of the three states is: </span><span class="si">{</span><span class="n">p_average_time_spent</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The proportion of time spend in each of the three areas are 0.4473, 0.4211, and 0.1316, respectively.</p>
<p>Imagine now that if the animal is satiated and tired the transitions change to:</p>
<div class="amsmath math notranslate nohighlight" id="equation-1507f6f2-8ccc-4821-af73-34766cda0b99">
<span class="eqno">(115)<a class="headerlink" href="#equation-1507f6f2-8ccc-4821-af73-34766cda0b99" title="Permalink to this equation">¶</a></span>\[\begin{matrix}
\hline
state_{i} &amp; P(state_{i+1}=1|state_i=*) &amp; P(state_{i+1}=2|state_i=*) &amp; P(state_{i+1}=3|state_i=*) \\
\hline
state_{i}=1 &amp; 0.2 &amp; 0.7 &amp; 0.1\\
state_{i}=2 &amp; 0.3 &amp; 0.7 &amp; 0.0\\
state_{i}=3 &amp; 0.8 &amp; 0.2 &amp; 0.0\\
\hline
\end{matrix}\]</div>
<p>Try repeating the questions above for this table of transitions by changing the transition matrix. Based on the probability values, what would you predict? Check how much time the rat spends on average in each area and see if it matches your predictions.</p>
<p><strong>Main course preview:</strong> The Markov property is extremely important for many models, particularly Hidden Markov Models, discussed on day W3D2, and for methods such as Markov Chain Monte Carlo sampling.</p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-statistical-inference-and-likelihood">
<h1>Section 2: Statistical inference and likelihood<a class="headerlink" href="#section-2-statistical-inference-and-likelihood" title="Permalink to this headline">¶</a></h1>
<div class="section" id="section-2-1-likelihoods">
<h2>Section 2.1: Likelihoods<a class="headerlink" href="#section-2-1-likelihoods" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-3-statistical-inference-and-likelihood">
<h3>Video 3: Statistical inference and likelihood<a class="headerlink" href="#video-3-statistical-inference-and-likelihood" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
<p><strong>Correction to video</strong>: The variance estimate that maximizes the likelihood is <span class="math notranslate nohighlight">\(\bar{\sigma}^2=\frac{1}{n} \sum_i (x_i-\bar{x})^2 \)</span>. This is a biased estimate. Shown in the video is the sample variance, which is an unbiased estimate for variance: <span class="math notranslate nohighlight">\(\bar{\sigma}^2=\frac{1}{n-1} \sum_i (x_i-\bar{x})^2 \)</span>. See section 2.2.3 for more details.</p>
<details>
<summary> <font color="blue">Click here for text recap of video </font></summary>
<p>A generative model (such as the Gaussian distribution from the previous tutorial) allows us to make predictions about outcomes.</p>
<p>However, after we observe <span class="math notranslate nohighlight">\(n\)</span> data points, we can also evaluate our model (and any of its associated parameters) by calculating the <strong>likelihood</strong> of our model having generated each of those data points <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-10b94e45-a48c-44ae-afb0-1ea6010fdbb9">
<span class="eqno">(116)<a class="headerlink" href="#equation-10b94e45-a48c-44ae-afb0-1ea6010fdbb9" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(x_i|\mu,\sigma)=\mathcal{N}(x_i,\mu,\sigma)
\end{equation}\]</div>
<p>For all data points <span class="math notranslate nohighlight">\(\mathbf{x}=(x_1, x_2, x_3, ...x_n) \)</span> we can then calculate the likelihood for the whole dataset by computing the product of the likelihood for each single data point.</p>
<div class="amsmath math notranslate nohighlight" id="equation-e4b7d097-b4db-4c5c-863a-b8227c1cace8">
<span class="eqno">(117)<a class="headerlink" href="#equation-e4b7d097-b4db-4c5c-863a-b8227c1cace8" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(\mathbf{x}|\mu,\sigma)=\prod_{i=1}^n \mathcal{N}(x_i,\mu,\sigma)
\end{equation}\]</div>
</details>
<p>While the likelihood may be written as a conditional probability (<span class="math notranslate nohighlight">\(P(x|\mu,\sigma)\)</span>), we refer to it as the <strong>likelihood function</strong>, <span class="math notranslate nohighlight">\(L(\mu,\sigma)\)</span>.  This slight switch in notation is to emphasize our focus: we use likelihood functions when the data points <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> are fixed and we are focused on the parameters.</p>
<p>Our new notation makes clear that the likelihood <span class="math notranslate nohighlight">\(L(\mu,\sigma)\)</span> is a function of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>, not of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>In the last tutorial we reviewed how the data was generated given the selected parameters of the generative process. If we do not know the parameters <span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\sigma\)</span> that generated the data, we can try to <strong>infer</strong> which parameter values (given our model) gives the best (highest) likelihood. This is what we call statistical inference: trying to infer what parameters make our observed data the most likely or probable?</p>
</div>
<div class="section" id="coding-exercise-2-1-computing-likelihood">
<h3>Coding Exercise 2.1: Computing likelihood<a class="headerlink" href="#coding-exercise-2-1-computing-likelihood" title="Permalink to this headline">¶</a></h3>
<p>Let’s start with computing the likelihood of some set of data points being drawn from a Gaussian distribution with a mean and variance we choose.</p>
<p>As multiplying small probabilities together can lead to very small numbers, it is often convenient to report the <em>logarithm</em> of the likelihood. This is just a convenient transformation and as logarithm is a monotonically increasing function this does not change what parameters maximise the function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_likelihood_normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean_val</span><span class="p">,</span> <span class="n">standard_dev_val</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Computes the log-likelihood values given a observed data sample x, and</span>
<span class="sd">  potential mean and variance values for a normal distribution</span>

<span class="sd">    Args:</span>
<span class="sd">      x (ndarray): 1-D array with all the observed data</span>
<span class="sd">      mean_val (scalar): value of mean for which to compute likelihood</span>
<span class="sd">      standard_dev_val (scalar): value of variance for which to compute likelihood</span>

<span class="sd">    Returns:</span>
<span class="sd">      likelihood (scalar): value of likelihood for this combination of means/variances</span>
<span class="sd">  """</span>

  <span class="c1">###################################################################</span>
  <span class="c1">## TODO for student</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: compute likelihood"</span><span class="p">)</span>
  <span class="c1">###################################################################</span>

  <span class="c1"># Get probability of each data point (use norm.pdf from scipy stats)</span>
  <span class="n">p_data</span> <span class="o">=</span> <span class="o">...</span>

  <span class="c1"># Compute likelihood (sum over the log of the probabilities)</span>
  <span class="n">likelihood</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">likelihood</span>

<span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Generate data</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_standard_dev</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mean</span><span class="p">,</span> <span class="n">true_standard_dev</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,))</span>

<span class="c1"># Compute likelihood for a guessed mean/standard dev</span>
<span class="n">guess_mean</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">guess_standard_dev</span> <span class="o">=</span> <span class="mf">.1</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">compute_likelihood_normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">guess_mean</span><span class="p">,</span> <span class="n">guess_standard_dev</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">likelihood</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">def</span> <span class="nf">compute_likelihood_normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean_val</span><span class="p">,</span> <span class="n">standard_dev_val</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Computes the log-likelihood values given a observed data sample x, and</span>
<span class="sd">  potential mean and variance values for a normal distribution</span>

<span class="sd">    Args:</span>
<span class="sd">      x (ndarray): 1-D array with all the observed data</span>
<span class="sd">      mean_val (scalar): value of mean for which to compute likelihood</span>
<span class="sd">      standard_dev_val (scalar): value of variance for which to compute likelihood</span>

<span class="sd">    Returns:</span>
<span class="sd">      likelihood (scalar): value of likelihood for this combination of means/variances</span>
<span class="sd">  """</span>

  <span class="c1"># Get probability of each data point (use norm.pdf from scipy stats)</span>
  <span class="n">p_data</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean_val</span><span class="p">,</span> <span class="n">standard_dev_val</span><span class="p">)</span>

  <span class="c1"># Compute likelihood (sum over the log of the probabilities)</span>
  <span class="n">likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_data</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">likelihood</span>

<span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Generate data</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_standard_dev</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mean</span><span class="p">,</span> <span class="n">true_standard_dev</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,))</span>

<span class="c1"># Compute likelihood for a guessed mean/standard dev</span>
<span class="n">guess_mean</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">guess_standard_dev</span> <span class="o">=</span> <span class="mf">.1</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">compute_likelihood_normal</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">guess_mean</span><span class="p">,</span> <span class="n">guess_standard_dev</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">likelihood</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You should get a likelihood of -92904.81. This is somewhat meaningless to us! For it to be useful, we need to compare it to the likelihoods computing using other guesses of the mean or standard deviation. The visualization below shows us the likelihood for various values of the mean and the standard deviation. Essentially, we are performing a rough grid-search over means and standard deviations.  What would you guess as the true mean and standard deviation based on this visualization?</p>
<p>Execute to visualize likelihoods</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute to visualize likelihoods</span>

<span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Generate data</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_standard_dev</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mean</span><span class="p">,</span> <span class="n">true_standard_dev</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,))</span>


<span class="c1"># Compute likelihood for different mean/variance values</span>
<span class="n">mean_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># potential mean values to ry</span>
<span class="n">standard_dev_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span> <span class="c1"># potential variance values to try</span>

<span class="c1"># Initialise likelihood collection array</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">mean_vals</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">standard_dev_vals</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="c1"># Compute the likelihood for observing the gvien data x assuming</span>
<span class="c1"># each combination of mean and variance values</span>
<span class="k">for</span> <span class="n">idxMean</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mean_vals</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
  <span class="k">for</span> <span class="n">idxVar</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">standard_dev_vals</span> <span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">likelihood</span><span class="p">[</span><span class="n">idxVar</span><span class="p">,</span><span class="n">idxMean</span><span class="p">]</span><span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean_vals</span><span class="p">[</span><span class="n">idxMean</span><span class="p">],</span>
                                              <span class="n">standard_dev_vals</span><span class="p">[</span><span class="n">idxVar</span><span class="p">])))</span>

<span class="c1"># Uncomment once you've generated the samples and compute likelihoods</span>
<span class="n">xspace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plot_likelihoods</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">mean_vals</span><span class="p">,</span> <span class="n">standard_dev_vals</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="section-2-2-maximum-likelihood">
<h2>Section 2.2: Maximum likelihood<a class="headerlink" href="#section-2-2-maximum-likelihood" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-4-maximum-likelihood">
<h3>Video 4: Maximum likelihood<a class="headerlink" href="#video-4-maximum-likelihood" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
<p>Implicitly, by looking for the parameters that give the highest likelihood in the last section, we have been searching for the <strong>maximum likelihood</strong> estimate.</p>
<div class="amsmath math notranslate nohighlight" id="equation-56a31009-c313-4d42-b0fc-4289b01c60b6">
<span class="eqno">(118)<a class="headerlink" href="#equation-56a31009-c313-4d42-b0fc-4289b01c60b6" title="Permalink to this equation">¶</a></span>\[\begin{equation}
(\hat{\mu},\hat{\sigma}) = \underset{\mu,\sigma}{\operatorname{argmax}}L(\mu,\sigma) = \underset{\mu,\sigma}{\operatorname{argmax}} \prod_{i=1}^n \mathcal{N}(x_i,\mu,\sigma).
\end{equation}\]</div>
<p>In next sections, we will look at other ways of inferring such parameter variables.</p>
</div>
<div class="section" id="section-2-2-1-searching-for-best-parameters">
<h3>Section 2.2.1: Searching for best parameters<a class="headerlink" href="#section-2-2-1-searching-for-best-parameters" title="Permalink to this headline">¶</a></h3>
<p>We want to do inference on this data set, i.e. we want to infer the parameters that most likely gave rise to the data given our model. Intuitively that means that we want as good as possible a fit between the observed data and the probability distribution function with the best inferred parameters. We can search for the best parameters manually by trying out a bunch of possible values of the parameters, computing the likelihoods, and picking the parameters that resulted in the highest likelihood.</p>
<div class="section" id="interactive-demo-2-2-maximum-likelihood-inference">
<h4>Interactive Demo 2.2: Maximum likelihood inference<a class="headerlink" href="#interactive-demo-2-2-maximum-likelihood-inference" title="Permalink to this headline">¶</a></h4>
<p>Try to see how well you can fit the probability distribution to the data by using the demo sliders to control the mean and standard deviation parameters of the distribution. We will visualize the histogram of data points (in blue) and the Gaussian density curve with that mean and standard deviation (in red). Below, we print the log-likelihood.</p>
<ul class="simple">
<li><p>What (approximate) values of mu and sigma result in the best fit?</p></li>
<li><p>How does the value below the plot (the log-likelihood) change with the quality of fit?</p></li>
</ul>
<p>Make sure you execute this cell to enable the widget and fit by hand!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to enable the widget and fit by hand!</span>
<span class="c1"># Generate data</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_standard_dev</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mean</span><span class="p">,</span> <span class="n">true_standard_dev</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,))</span>

<span class="k">def</span> <span class="nf">plotFnc</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">):</span>
  <span class="n">loglikelihood</span><span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">)))</span>
  <span class="c1">#calculate histogram</span>

  <span class="c1">#prepare to plot</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'x'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'probability'</span><span class="p">)</span>

  <span class="c1">#plot histogram</span>
  <span class="n">count</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">ignored</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">vals</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

  <span class="c1">#plot pdf</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mu</span><span class="p">,</span><span class="n">sigma</span><span class="p">),</span><span class="s1">'r-'</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"The log-likelihood for the selected parameters is: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">loglikelihood</span><span class="p">))</span>

<span class="c1">#interact(plotFnc, mu=5.0, sigma=2.1);</span>
<span class="c1">#interact(plotFnc, mu=widgets.IntSlider(min=0.0, max=10.0, step=1, value=4.0),sigma=widgets.IntSlider(min=0.1, max=10.0, step=1, value=4.0));</span>
<span class="n">interact</span><span class="p">(</span><span class="n">plotFnc</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span><span class="n">sigma</span><span class="o">=</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">));</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">- The log-likelihood should be greatest when mu=5 and sigma=1.</span>
<span class="sd">- The summed log-liklihood increases (becomes less negative) as the fit improves</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<p>Doing this was similar to the grid searched image from Section 2.1. Really, we want to see if we can do inference on observed data in a bit more principled way.</p>
</div>
</div>
<div class="section" id="section-2-2-2-optimization-to-find-parameters">
<h3>Section 2.2.2: Optimization to find parameters<a class="headerlink" href="#section-2-2-2-optimization-to-find-parameters" title="Permalink to this headline">¶</a></h3>
<p>Let’s again assume that we have a data set, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, assumed to be generated by a normal distribution (we actually generate it ourselves in line 1, so we know how it was generated!).
We want to maximise the likelihood of the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>. We can do so using a couple of tricks:</p>
<ul class="simple">
<li><p>Using a log transform will not change the maximum of the function, but will allow us to work with very small numbers that could lead to problems with machine precision.</p></li>
<li><p>Maximising a function is the same as minimising the negative of a function, allowing us to use the minimize optimisation provided by scipy.</p></li>
</ul>
<p>The optimisation will be done using <code class="docutils literal notranslate"><span class="pre">sp.optimize.minimize</span></code>, which does a version of gradient descent (there are hundreds of ways to do numerical optimisation, we will not cover these here!).</p>
<div class="section" id="coding-exercise-2-2-maximum-likelihood-estimation">
<h4>Coding Exercise 2.2: Maximum Likelihood Estimation<a class="headerlink" href="#coding-exercise-2-2-maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h4>
<p>In the code below, insert the missing line (see the <code class="docutils literal notranslate"><span class="pre">compute_likelihood_normal</span></code> function from previous exercise), with the mean as <code class="docutils literal notranslate"><span class="pre">theta[0]</span></code> and standard deviation as <code class="docutils literal notranslate"><span class="pre">theta[1]</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># We define the function to optimise, the negative log likelihood</span>
<span class="k">def</span> <span class="nf">negLogLike</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Function for computing the negative log-likelihood given the observed data</span>
<span class="sd">      and given parameter values stored in theta.</span>

<span class="sd">      Args:</span>
<span class="sd">        theta (ndarray): normal distribution parameters</span>
<span class="sd">                        (mean is theta[0], standard deviation is theta[1])</span>
<span class="sd">        x (ndarray): array with observed data points</span>

<span class="sd">      Returns:</span>
<span class="sd">        Calculated negative Log Likelihood value!</span>
<span class="sd">  """</span>
  <span class="c1">###################################################################</span>
  <span class="c1">## TODO for students: Compute the negative log-likelihood value for the</span>
  <span class="c1">## given observed data values and parameters (theta)</span>
  <span class="c1"># Fill out the following then remove</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: need to compute the negative </span><span class="se">\</span>
<span class="s2">                                log-likelihood value"</span><span class="p">)</span>
  <span class="c1">###################################################################</span>
  <span class="k">return</span> <span class="o">...</span>

<span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Generate data</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_standard_dev</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mean</span><span class="p">,</span> <span class="n">true_standard_dev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># Define bounds, var has to be positive</span>
<span class="n">bnds</span> <span class="o">=</span> <span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

<span class="c1"># Optimize with scipy!</span>
<span class="n">optimal_parameters</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">negLogLike</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">args</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bnds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The optimal mean estimate is: </span><span class="si">{</span><span class="n">optimal_parameters</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The optimal standard deviation estimate is: </span><span class="si">{</span><span class="n">optimal_parameters</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># optimal_parameters contains a lot of information about the optimization,</span>
<span class="c1"># but we mostly want the mean and standard deviation</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="c1"># We define the function to optimise, the negative log likelihood</span>
<span class="k">def</span> <span class="nf">negLogLike</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Function for computing the negative log-likelihood given the observed data</span>
<span class="sd">      and given parameter values stored in theta.</span>

<span class="sd">      Args:</span>
<span class="sd">        theta (ndarray): normal distribution parameters</span>
<span class="sd">                        (mean is theta[0], standard deviation is theta[1])</span>
<span class="sd">        x (ndarray): array with observed data points</span>

<span class="sd">      Returns:</span>
<span class="sd">        Calculated negative Log Likelihood value!</span>
<span class="sd">  """</span>
  <span class="k">return</span> <span class="o">-</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">])))</span>

<span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Generate data</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_standard_dev</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mean</span><span class="p">,</span> <span class="n">true_standard_dev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># Define bounds, var has to be positive</span>
<span class="n">bnds</span> <span class="o">=</span> <span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>

<span class="c1"># Optimize with scipy!</span>
<span class="n">optimal_parameters</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">negLogLike</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">args</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bnds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The optimal mean estimate is: </span><span class="si">{</span><span class="n">optimal_parameters</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The optimal standard deviation estimate is: </span><span class="si">{</span><span class="n">optimal_parameters</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># optimal_parameters contains a lot of information about the optimization,</span>
<span class="c1"># but we mostly want the mean and standard deviation</span>
</pre></div>
</div>
</div>
</div>
<p>These are the approximations of the parameters that maximise the likelihood (<span class="math notranslate nohighlight">\(\mu\)</span> ~ 5.280 and <span class="math notranslate nohighlight">\(\sigma\)</span> ~ 1.148).</p>
</div>
</div>
<div class="section" id="section-2-2-3-analytical-solution">
<h3>Section 2.2.3: Analytical solution<a class="headerlink" href="#section-2-2-3-analytical-solution" title="Permalink to this headline">¶</a></h3>
<p>Sometimes, things work out well and we can come up with formulas for the maximum likelihood estimates of parameters. We won’t get into this further but basically we could set the derivative of the likelihood to 0 (to find a maximum) and solve for the parameters. This won’t always work but for the Gaussian distribution, it does.</p>
<p>Specifically , the special thing about the Gaussian is that mean and standard deviation of the random sample can effectively approximate the two parameters of a Gaussian, <span class="math notranslate nohighlight">\(\mu, \sigma\)</span>.</p>
<p>Hence using the  mean, <span class="math notranslate nohighlight">\(\bar{x}=\frac{1}{n}\sum_i x_i\)</span>, and variance, <span class="math notranslate nohighlight">\(\bar{\sigma}^2=\frac{1}{n} \sum_i (x_i-\bar{x})^2 \)</span> of the sample should give us the best/maximum likelihood, <span class="math notranslate nohighlight">\(L(\bar{x},\bar{\sigma}^2)\)</span>.</p>
<p>Let’s compare these values to those we’ve been finding using manual search and optimization, and the true values (which we only know because we generated the numbers!).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Generate data</span>
<span class="n">true_mean</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_standard_dev</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">true_mean</span><span class="p">,</span> <span class="n">true_standard_dev</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="p">))</span>

<span class="c1"># Compute and print sample means and standard deviations</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"This is the sample mean as estimated by numpy: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"This is the sample standard deviation as estimated by numpy: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">""" You should notice that the parameters estimated by maximum likelihood</span>
<span class="sd">estimation/inference are very close to the true parameters (mu = 5, sigma = 1),</span>
<span class="sd">as well as the parameters visualized to be best after Coding Exercise 2.1,</span>
<span class="sd"> where all likelihood values were calculated explicitly.</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<p>If you try out different values of the mean and standard deviation in all the previous exercises, you should see that changing the mean and
sigma parameter values (and generating new data from a distribution with theseparameters) makes no difference as MLE methods can still recover these parameters.</p>
<p>There is a slight problem: it turns out that the maximum likelihood estimate for the variance is actually a biased one! This means that the estimators expected value (mean value) and the true value of the parameter are different.  An unbiased estimator for the variance is <span class="math notranslate nohighlight">\(\bar{\sigma}^2=\frac{1}{n-1} \sum_i (x_i-\bar{x})^2 \)</span>, this is called the sample variance. For more details, see <a class="reference external" href="https://en.wikipedia.org/wiki/Bias_of_an_estimator">the wiki page on bias of estimators</a>.</p>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-bayesian-inference">
<h1>Section 3: Bayesian Inference<a class="headerlink" href="#section-3-bayesian-inference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="section-3-1-bayes">
<h2>Section 3.1: Bayes<a class="headerlink" href="#section-3-1-bayes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-5-bayesian-inference-with-gaussian-distribution">
<h3>Video 5: Bayesian inference with Gaussian distribution<a class="headerlink" href="#video-5-bayesian-inference-with-gaussian-distribution" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
<p>We will start to introduce Bayesian inference here to contrast with our maximum likelihood methods, but you will also revisit Bayesian inference in great detail on W3D1 of the course so we won’t dive into all details.</p>
<p>For Bayesian inference we do not focus on the likelihood function <span class="math notranslate nohighlight">\(L(y)=P(x|y)\)</span>, but instead focus on the posterior distribution:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c8473a92-2e74-4f68-9a4c-0b60e3d928df">
<span class="eqno">(119)<a class="headerlink" href="#equation-c8473a92-2e74-4f68-9a4c-0b60e3d928df" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(y|x)=\frac{P(x|y)P(y)}{P(x)}
\end{equation}\]</div>
<p>which is composed of the <strong>likelihood</strong> function <span class="math notranslate nohighlight">\(P(x|y)\)</span>, the <strong>prior</strong> <span class="math notranslate nohighlight">\(P(y)\)</span> and a normalising term <span class="math notranslate nohighlight">\(P(x)\)</span> (which we will ignore for now).</p>
<p>While there are other advantages to using Bayesian inference (such as the ability to derive Bayesian Nets, see optional bonus task below), we will start by focusing on the role of the prior in inference. Does including prior information allow us to infer parameters in a better way?</p>
</div>
<div class="section" id="think-3-1-bayesian-inference-with-gaussian-distribution">
<h3>Think! 3.1: Bayesian inference with Gaussian distribution<a class="headerlink" href="#think-3-1-bayesian-inference-with-gaussian-distribution" title="Permalink to this headline">¶</a></h3>
<p>In the above sections we performed inference using maximum likelihood, i.e. finding the parameters that maximised the likelihood of a set of parameters, given the model and data.</p>
<p>We will now repeat the inference process, but with an added Bayesian prior, and compare it to the “classical” inference (maximum likelihood) process we did before (Section 2). When using conjugate priors (more on this below) we can just update the parameter values of the distributions (here Gaussian distributions).</p>
<p>For the prior we start by guessing a mean of 5 (mean of previously observed data points 4 and 6) and variance of 1 (variance of 4 and 6). We use a trick (not detailed here) that is a simplified way of applying a prior, that allows us to just add these 2 values (pseudo-data) to the real data.</p>
<p>See the visualization below that shows the mean and standard deviation inferred by our classical maximum likelihood approach and the Bayesian approach for different numbers of data points.</p>
<p>Remembering that our true values are <span class="math notranslate nohighlight">\(\mu = 5\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>, how do the Bayesian inference and classical inference compare?</p>
<p>Execute to visualize inference</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute to visualize inference</span>

<span class="k">def</span> <span class="nf">classic_vs_bayesian_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">num_points</span><span class="p">,</span> <span class="n">prior</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Compute both classical and Bayesian inference processes over the range of</span>
<span class="sd">  data sample sizes (num_points) for a normal distribution with parameters</span>
<span class="sd">  mu,sigma for comparison.</span>

<span class="sd">  Args:</span>
<span class="sd">    mu (scalar): the mean parameter of the normal distribution</span>
<span class="sd">    sigma (scalar): the standard deviation parameter of the normal distribution</span>
<span class="sd">    num_points (int): max number of points to use for inference</span>
<span class="sd">    prior (ndarray): prior data points for Bayesian inference</span>

<span class="sd">  Returns:</span>
<span class="sd">    mean_classic (ndarray): estimate mean parameter via classic inference</span>
<span class="sd">    var_classic (ndarray): estimate variance parameter via classic inference</span>
<span class="sd">    mean_bayes (ndarray): estimate mean parameter via Bayesian inference</span>
<span class="sd">    var_bayes (ndarray): estimate variance parameter via Bayesian inference</span>
<span class="sd">  """</span>

  <span class="c1"># Initialize the classical and Bayesian inference arrays that will estimate</span>
  <span class="c1"># the normal parameters given a certain number of randomly sampled data points</span>
  <span class="n">mean_classic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">)</span>
  <span class="n">var_classic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">)</span>

  <span class="n">mean_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">)</span>
  <span class="n">var_bayes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">nData</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_points</span><span class="p">):</span>

    <span class="n">random_num_generator</span> <span class="o">=</span> <span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">random_num_generator</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">nData</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Compute the mean of those points and set the corresponding array entry to this value</span>
    <span class="n">mean_classic</span><span class="p">[</span><span class="n">nData</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Compute the variance of those points and set the corresponding array entry to this value</span>
    <span class="n">var_classic</span><span class="p">[</span><span class="n">nData</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Bayesian inference with the given prior is performed below for you</span>
    <span class="n">xsupp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">))</span>
    <span class="n">mean_bayes</span><span class="p">[</span><span class="n">nData</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">xsupp</span><span class="p">)</span>
    <span class="n">var_bayes</span><span class="p">[</span><span class="n">nData</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">xsupp</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">mean_classic</span><span class="p">,</span> <span class="n">var_classic</span><span class="p">,</span> <span class="n">mean_bayes</span><span class="p">,</span> <span class="n">var_bayes</span>

<span class="c1"># Set random seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Set normal distribution parameters, mu and sigma</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Set the prior to be two new data points, 4 and 6, and print the mean and variance</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"The mean of the data comprising the prior is: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">prior</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"The variance of the data comprising the prior is: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">prior</span><span class="p">)))</span>

<span class="n">mean_classic</span><span class="p">,</span> <span class="n">var_classic</span><span class="p">,</span> <span class="n">mean_bayes</span><span class="p">,</span> <span class="n">var_bayes</span> <span class="o">=</span> <span class="n">classic_vs_bayesian_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>
<span class="n">plot_classical_vs_bayesian_normal</span><span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="n">mean_classic</span><span class="p">,</span> <span class="n">var_classic</span><span class="p">,</span> <span class="n">mean_bayes</span><span class="p">,</span> <span class="n">var_bayes</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">Hopefully you can see that the blue line stays a little closer to the true values (mu=5, sigma^2=1).</span>

<span class="sd"> Having a simple prior in the Bayesian inference process (blue) helps to regularise</span>
<span class="sd"> the inference of the mean and variance parameters when you have very little data,</span>
<span class="sd"> but has little effect with large data sets. You can see that as the number of data points</span>
<span class="sd"> (x-axis) increases, both inference processes (blue and red lines) get closer and closer</span>
<span class="sd"> together, i.e., their estimates for the true parameters converge as sample size increases.</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<p>Note that the prior is only beneficial when it is close to the true value, i.e., ‘a good guess’ (or at least not ‘a bad guess’). As we will see in the next exercise, if you have a prior/bias that is very wrong, your inference will start off very wrong!</p>
</div>
</div>
<div class="section" id="section-3-2-conjugate-priors">
<h2>Section 3.2: Conjugate priors<a class="headerlink" href="#section-3-2-conjugate-priors" title="Permalink to this headline">¶</a></h2>
<div class="section" id="video-6-conjugate-priors">
<h3>Video 6: Conjugate priors<a class="headerlink" href="#video-6-conjugate-priors" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_remove-input docutils container">
</div>
</div>
<div class="section" id="interactive-demo-3-2-conjugate-priors">
<h3>Interactive Demo 3.2: Conjugate priors<a class="headerlink" href="#interactive-demo-3-2-conjugate-priors" title="Permalink to this headline">¶</a></h3>
<p>Let’s return to our example from Tutorial 1 using the binomial distribution - rat in a T-maze.</p>
<p>Bayesian inference can be used for any likelihood distribution, but it is a lot more convenient to work with <strong>conjugate</strong> priors, where multiplying the prior with the likelihood just provides another instance of the prior distribution with updated values.</p>
<p>For the binomial likelihood it is convenient to use the <strong>beta</strong> distribution as a prior</p>
<div class="amsmath math notranslate nohighlight" id="equation-5d340b5f-059b-4760-bb91-999ced65e638">
<span class="eqno">(120)<a class="headerlink" href="#equation-5d340b5f-059b-4760-bb91-999ced65e638" title="Permalink to this equation">¶</a></span>\[\begin{equation}
f(p;\alpha ,\beta )={\frac {1}{\mathrm {B} (\alpha ,\beta )}}p^{\alpha -1}(1-p)^{\beta -1}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(B\)</span> is the beta function, <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are parameters, and <span class="math notranslate nohighlight">\(p\)</span> is the probability of the rat turning left or right. The beta distribution is thus a distribution over a probability.</p>
<p>Given a series of Left and Right moves of the rat, we can now estimate the probability that the animal will turn left. Using Bayesian Inference, we use a beta distribution <em>prior</em>, which is then multiplied with the <em>likelihood</em> to create a <em>posterior</em> that is also a beta distribution, but with updated parameters (we will not cover the math here).</p>
<p>Activate the widget below to explore the variables, and follow the instructions below.</p>
<div class="section" id="id1">
<h4><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Make sure you execute this cell to enable the widget</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>

<span class="c1">#@markdown Make sure you execute this cell to enable the widget</span>

<span class="c1">#beta distribution</span>
<span class="c1">#and binomial</span>
<span class="k">def</span> <span class="nf">plotFnc</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">priorL</span><span class="p">,</span><span class="n">priorR</span><span class="p">):</span>
  <span class="c1"># Set random seed</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="c1">#sample from binomial</span>
  <span class="n">numL</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">numR</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">numL</span>
  <span class="n">stepSize</span><span class="o">=</span><span class="mf">0.001</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">stepSize</span><span class="p">)</span>
  <span class="n">betaPdf</span><span class="o">=</span><span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">numL</span><span class="o">+</span><span class="n">priorL</span><span class="p">,</span><span class="n">numR</span><span class="o">+</span><span class="n">priorR</span><span class="p">)</span>
  <span class="n">betaPrior</span><span class="o">=</span><span class="n">sp</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">priorL</span><span class="p">,</span><span class="n">priorR</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"number of left "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">numL</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"number of right "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">numR</span><span class="p">))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">" "</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"max likelihood "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">numL</span><span class="o">/</span><span class="p">(</span><span class="n">numL</span><span class="o">+</span><span class="n">numR</span><span class="p">)))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">" "</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"max posterior "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">betaPdf</span><span class="p">)]))</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"mean posterior "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">betaPdf</span><span class="o">*</span><span class="n">x</span><span class="p">)))</span>


  <span class="nb">print</span><span class="p">(</span><span class="s2">" "</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
    <span class="c1">#rng.beta()</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">'font.size'</span><span class="p">:</span> <span class="mi">22</span><span class="p">})</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'p'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'probability density'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">betaPdf</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">"Posterior"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">betaPrior</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">"Prior"</span><span class="p">)</span>
    <span class="c1">#print(int(len(betaPdf)/2))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>





<span class="n">interact</span><span class="p">(</span><span class="n">plotFnc</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span><span class="n">n</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">priorL</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span><span class="n">priorR</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">));</span>
</pre></div>
</div>
</div>
</div>
<p>The plot above shows you the prior distribution (i.e. before any data) and the posterior distribution (after data), with a summary of the data (number of left and right moves) and the maximum likelihood, maximum posterior and mean of the posterior. Dependent on the purpose either the mean or the max of the posterior can be useful as a ‘single-number’ summary of the posterior.
Once you are familiar with the sliders and what they represent, go through these instructions.</p>
<p><strong>For <span class="math notranslate nohighlight">\(p=0.5\)</span></strong></p>
<ul class="simple">
<li><p>Set <span class="math notranslate nohighlight">\(p=0.5\)</span> and start off with a “flat” prior (<code class="docutils literal notranslate"><span class="pre">priorL=0</span></code>, <code class="docutils literal notranslate"><span class="pre">priorR=0</span></code>). Note that the prior distribution (orange) is flat, also known as uniformative. In this case the maximum likelihood and maximum posterior will get you almost identical results as you vary the number of datapoints (<span class="math notranslate nohighlight">\(n\)</span>) and the probability of the rat going left. However the posterior is a full distribution and not just a single point estimate.</p></li>
<li><p>As <span class="math notranslate nohighlight">\(n\)</span> gets large you will also notice that the estimate (max likelihood or max posterior) changes less for each change in <span class="math notranslate nohighlight">\(n\)</span>, i.e. the estimation stabilises.</p></li>
<li><p>How many data points do you need think is needed for the probability estimate to stabilise? Note that this depends on how large fluctuations you are willing to accept.</p></li>
<li><p>Try increasing the strength of the prior, <code class="docutils literal notranslate"><span class="pre">priorL=10</span></code> and <code class="docutils literal notranslate"><span class="pre">priorR=10</span></code>. You will see that the prior distribution becomes more ‘peaky’. In short this prior means that small or large values of <span class="math notranslate nohighlight">\(p\)</span> are conidered very unlikely. Try playing with the number of data points <span class="math notranslate nohighlight">\(n\)</span>, you should find that the prior stabilises/regularises the maximum posterior estimate so that it does not move as much.</p></li>
</ul>
<p><strong>For <span class="math notranslate nohighlight">\(p=0.2\)</span></strong></p>
<p>Try the same as you just did, now with <span class="math notranslate nohighlight">\(p=0.2\)</span>,
do you notice any differences? Note that the prior (assumeing equal chance Left and Right) is now badly matched to the data. Do the maximum likelihood and maximum posterior still give similar results, for a weak prior? For a strong prior? Does the prior still have a stabilising effect on the estimate?</p>
<p><strong>Take-away message:</strong>
Bayesian inference gives you a full distribution over the variables that you are inferring, can help regularise inference when you have limited data, and allows you to build more complex models that better reflects true causality (see bonus below).</p>
</div>
</div>
<div class="section" id="think-3-2-bayesian-brains">
<h3>Think! 3.2: Bayesian Brains<a class="headerlink" href="#think-3-2-bayesian-brains" title="Permalink to this headline">¶</a></h3>
<p>Bayesian inference can help you when doing data analysis, especially when you only have little data. But consider whether the brain might be able to benefit from this too. If the brain needs to make inferences about the world, would it be useful to do regularisation on the input? Maybe there are times where having a full probability distribution could be useful?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">""" You will learn more about "Bayesian brains" and the theory surrounding</span>
<span class="sd">these ideas once the course begins. Here is a brief explanation: it may</span>
<span class="sd">be ideal for human brains to implement Bayesian inference by integrating "prior"</span>
<span class="sd">information the brain has about the world (memories, prior knowledge, etc.) with</span>
<span class="sd">new evidence that updates its "beliefs"/prior. This process seems to parallel</span>
<span class="sd">the brain's method of learning about its environment, making it a compelling</span>
<span class="sd">theory for many neuroscience researchers. One of Bonus exercises below examines a possible</span>
<span class="sd">real world model for Bayesian inference: sound localization.</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-7-summary">
<h2>Video 7: Summary<a class="headerlink" href="#video-7-summary" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p>Having done the different exercises you should now:</p>
<ul class="simple">
<li><p>understand what the likelihood function is, and have some intuition of why it is important</p></li>
<li><p>know how to summarise the Gaussian distribution using mean and variance</p></li>
<li><p>know how to maximise a likelihood function</p></li>
<li><p>be able to do simple inference in both classical and Bayesian ways</p></li>
</ul>
<p>For more resources see <a class="reference external" href="https://github.com/NeuromatchAcademy/precourse/blob/master/resources.md">here</a>.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="bonus">
<h1>Bonus<a class="headerlink" href="#bonus" title="Permalink to this headline">¶</a></h1>
<div class="section" id="bonus-coding-exercise-1-finding-the-posterior-computationally">
<h2>Bonus Coding Exercise 1: Finding the posterior computationally<a class="headerlink" href="#bonus-coding-exercise-1-finding-the-posterior-computationally" title="Permalink to this headline">¶</a></h2>
<p>Imagine an experiment where participants estimate the location of a noise-emitting object. To estimate its position, the participants can use two sources of information:</p>
<ol class="simple">
<li><p>new noisy auditory information (the likelihood)</p></li>
<li><p>prior visual expectations of where the stimulus is likely to come from (visual prior).</p></li>
</ol>
<p>The auditory and visual information are both noisy, so participants will combine these sources of information to better estimate the position of the object.</p>
<p>We will use Gaussian distributions to represent the auditory likelihood (in red), and a Gaussian visual prior (expectations - in blue). Using Bayes rule, you will combine them into a posterior distribution that summarizes the probability that the object is in each possible location.</p>
<p>We have provided you with a ready-to-use plotting function, and a code skeleton.</p>
<ul class="simple">
<li><p>You can use <code class="docutils literal notranslate"><span class="pre">my_gaussian</span></code> from Tutorial 1 (also included below), to generate an auditory likelihood with parameters <span class="math notranslate nohighlight">\(\mu = 3\)</span> and <span class="math notranslate nohighlight">\(\sigma = 1.5\)</span></p></li>
<li><p>Generate a visual prior with parameters <span class="math notranslate nohighlight">\(\mu = -1\)</span> and <span class="math notranslate nohighlight">\(\sigma = 1.5\)</span></p></li>
<li><p>Calculate the posterior using pointwise multiplication of the likelihood and prior. Don’t forget to normalize so the posterior adds up to 1</p></li>
<li><p>Plot the likelihood, prior and posterior using the predefined function <code class="docutils literal notranslate"><span class="pre">posterior_plot</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">my_gaussian</span><span class="p">(</span><span class="n">x_points</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Returns normalized Gaussian estimated at points `x_points`, with parameters:</span>
<span class="sd">  mean `mu` and standard deviation `sigma`</span>

<span class="sd">  Args:</span>
<span class="sd">      x_points (ndarray of floats): points at which the gaussian is evaluated</span>
<span class="sd">      mu (scalar): mean of the Gaussian</span>
<span class="sd">      sigma (scalar): standard deviation of the gaussian</span>

<span class="sd">  Returns:</span>
<span class="sd">      (numpy array of floats) : normalized Gaussian evaluated at `x`</span>
<span class="sd">  """</span>
  <span class="n">px</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_points</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

  <span class="c1"># as we are doing numerical integration we may have to remember to normalise</span>
  <span class="c1"># taking into account the stepsize (0.1)</span>
  <span class="n">px</span> <span class="o">=</span> <span class="n">px</span><span class="o">/</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="nb">sum</span><span class="p">(</span><span class="n">px</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">px</span>


<span class="k">def</span> <span class="nf">compute_posterior_pointwise</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Compute the posterior probability distribution point-by-point using Bayes</span>
<span class="sd">  Rule.</span>

<span class="sd">    Args:</span>
<span class="sd">      prior (ndarray): probability distribution of prior</span>
<span class="sd">      likelihood (ndarray): probability distribution of likelihood</span>

<span class="sd">    Returns:</span>
<span class="sd">      posterior (ndarray): probability distribution of posterior</span>
<span class="sd">  """</span>
  <span class="c1">##############################################################################</span>
  <span class="c1"># TODO for students: Write code to compute the posterior from the prior and</span>
  <span class="c1"># likelihood via pointwise multiplication. (You may assume both are defined</span>
  <span class="c1"># over the same x-axis)</span>
  <span class="c1">#</span>
  <span class="c1"># Comment out the line below to test your solution</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Finish the simulation code first"</span><span class="p">)</span>
  <span class="c1">##############################################################################</span>

  <span class="n">posterior</span> <span class="o">=</span> <span class="o">...</span>

  <span class="k">return</span> <span class="n">posterior</span>


<span class="k">def</span> <span class="nf">localization_simulation</span><span class="p">(</span><span class="n">mu_auditory</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">sigma_auditory</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
                            <span class="n">mu_visual</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_visual</span><span class="o">=</span><span class="mf">1.5</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Perform a sound localization simulation with an auditory prior.</span>

<span class="sd">    Args:</span>
<span class="sd">      mu_auditory (float): mean parameter value for auditory prior</span>
<span class="sd">      sigma_auditory (float): standard deviation parameter value for auditory</span>
<span class="sd">                                prior</span>
<span class="sd">      mu_visual (float): mean parameter value for visual likelihood distribution</span>
<span class="sd">      sigma_visual (float): standard deviation parameter value for visual</span>
<span class="sd">                                likelihood distribution</span>

<span class="sd">    Returns:</span>
<span class="sd">      x (ndarray): range of values for which to compute probabilities</span>
<span class="sd">      auditory (ndarray): probability distribution of the auditory prior</span>
<span class="sd">      visual (ndarray): probability distribution of the visual likelihood</span>
<span class="sd">      posterior_pointwise (ndarray): posterior probability distribution</span>
<span class="sd">  """</span>
  <span class="c1">##############################################################################</span>
  <span class="c1">## Using the x variable below,</span>
  <span class="c1">##      create a gaussian called 'auditory' with mean 3, and std 1.5</span>
  <span class="c1">##      create a gaussian called 'visual' with mean -1, and std 1.5</span>
  <span class="c1">#</span>
  <span class="c1">#</span>
  <span class="c1">## Comment out the line below to test your solution</span>
  <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Finish the simulation code first"</span><span class="p">)</span>
  <span class="c1">###############################################################################</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

  <span class="n">auditory</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">visual</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">posterior</span> <span class="o">=</span> <span class="n">compute_posterior_pointwise</span><span class="p">(</span><span class="n">auditory</span><span class="p">,</span> <span class="n">visual</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">auditory</span><span class="p">,</span> <span class="n">visual</span><span class="p">,</span> <span class="n">posterior</span>


<span class="n">x</span><span class="p">,</span> <span class="n">auditory</span><span class="p">,</span> <span class="n">visual</span><span class="p">,</span> <span class="n">posterior_pointwise</span><span class="o">=</span><span class="n">localization_simulation</span><span class="p">()</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">posterior_plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">auditory</span><span class="p">,</span> <span class="n">visual</span><span class="p">,</span> <span class="n">posterior_pointwise</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="k">def</span> <span class="nf">my_gaussian</span><span class="p">(</span><span class="n">x_points</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Returns normalized Gaussian estimated at points `x_points`, with parameters:</span>
<span class="sd">  mean `mu` and standard deviation `sigma`</span>

<span class="sd">  Args:</span>
<span class="sd">      x_points (ndarray of floats): points at which the gaussian is evaluated</span>
<span class="sd">      mu (scalar): mean of the Gaussian</span>
<span class="sd">      sigma (scalar): standard deviation of the gaussian</span>

<span class="sd">  Returns:</span>
<span class="sd">      (numpy array of floats) : normalized Gaussian evaluated at `x`</span>
<span class="sd">  """</span>
  <span class="n">px</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span> <span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x_points</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

  <span class="c1"># as we are doing numerical integration we may have to remember to normalise</span>
  <span class="c1"># taking into account the stepsize (0.1)</span>
  <span class="n">px</span> <span class="o">=</span> <span class="n">px</span><span class="o">/</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="nb">sum</span><span class="p">(</span><span class="n">px</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">px</span>


<span class="k">def</span> <span class="nf">compute_posterior_pointwise</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Compute the posterior probability distribution point-by-point using Bayes</span>
<span class="sd">  Rule.</span>

<span class="sd">    Args:</span>
<span class="sd">      prior (ndarray): probability distribution of prior</span>
<span class="sd">      likelihood (ndarray): probability distribution of likelihood</span>

<span class="sd">    Returns:</span>
<span class="sd">      posterior (ndarray): probability distribution of posterior</span>
<span class="sd">  """</span>

  <span class="n">posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
  <span class="n">posterior</span> <span class="o">=</span><span class="n">posterior</span><span class="o">/</span> <span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="n">posterior</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>

  <span class="k">return</span> <span class="n">posterior</span>


<span class="k">def</span> <span class="nf">localization_simulation</span><span class="p">(</span><span class="n">mu_auditory</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">sigma_auditory</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span>
                            <span class="n">mu_visual</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_visual</span><span class="o">=</span><span class="mf">1.5</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Perform a sound localization simulation with an auditory prior.</span>

<span class="sd">    Args:</span>
<span class="sd">      mu_auditory (float): mean parameter value for auditory prior</span>
<span class="sd">      sigma_auditory (float): standard deviation parameter value for auditory</span>
<span class="sd">                                prior</span>
<span class="sd">      mu_visual (float): mean parameter value for visual likelihood distribution</span>
<span class="sd">      sigma_visual (float): standard deviation parameter value for visual</span>
<span class="sd">                                likelihood distribution</span>

<span class="sd">    Returns:</span>
<span class="sd">      x (ndarray): range of values for which to compute probabilities</span>
<span class="sd">      auditory (ndarray): probability distribution of the auditory prior</span>
<span class="sd">      visual (ndarray): probability distribution of the visual likelihood</span>
<span class="sd">      posterior_pointwise (ndarray): posterior probability distribution</span>
<span class="sd">  """</span>

  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

  <span class="n">auditory</span> <span class="o">=</span> <span class="n">my_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu_auditory</span><span class="p">,</span> <span class="n">sigma_auditory</span><span class="p">)</span>
  <span class="n">visual</span> <span class="o">=</span> <span class="n">my_gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu_visual</span><span class="p">,</span> <span class="n">sigma_visual</span><span class="p">)</span>
  <span class="n">posterior</span> <span class="o">=</span> <span class="n">compute_posterior_pointwise</span><span class="p">(</span><span class="n">auditory</span><span class="p">,</span> <span class="n">visual</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">auditory</span><span class="p">,</span> <span class="n">visual</span><span class="p">,</span> <span class="n">posterior</span>

<span class="c1"># Uncomment the lines below to plot the results</span>
<span class="n">x</span><span class="p">,</span> <span class="n">auditory</span><span class="p">,</span> <span class="n">visual</span><span class="p">,</span> <span class="n">posterior_pointwise</span><span class="o">=</span><span class="n">localization_simulation</span><span class="p">()</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">_</span> <span class="o">=</span> <span class="n">posterior_plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">auditory</span><span class="p">,</span> <span class="n">visual</span><span class="p">,</span> <span class="n">posterior_pointwise</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Combining the the visual and auditory information could help the brain get a better estimate of the location of an audio-visual object, with lower variance.</p>
<p><strong>Main course preview:</strong> On Week 3 Day 1 (W3D1) there will be a whole day devoted to examining whether the brain uses Bayesian inference. Is the brain Bayesian?!</p>
</div>
<div class="section" id="bonus-coding-exercise-2-bayes-net">
<h2>Bonus Coding Exercise 2: Bayes Net<a class="headerlink" href="#bonus-coding-exercise-2-bayes-net" title="Permalink to this headline">¶</a></h2>
<p>If you have the time, here is another extra exercise.</p>
<p>Bayes Net, or Bayesian Belief Networks, provide a way to make inferences about multiple levels of information, which would be very difficult to do in a classical frequentist paradigm.</p>
<p>We can encapsulate our knowledge about causal relationships and use this to make inferences about hidden properties.</p>
<p>We will try a simple example of a Bayesian Net (aka belief network). Imagine that you have a house with an unreliable sprinkler system installed for watering the grass. This is set to water the grass independently of whether it has rained that day. We have three variables, rain (<span class="math notranslate nohighlight">\(r\)</span>), sprinklers (<span class="math notranslate nohighlight">\(s\)</span>) and wet grass (<span class="math notranslate nohighlight">\(w\)</span>). Each of these can be true (1) or false (0). See the graphical model representing the relationship between the variables.</p>
<p align="center"><img src="https://raw.githubusercontent.com/NeuromatchAcademy/precourse/main/tutorials/W0D5_Statistics/static/nodes_.png"/></p><p>There is a table below describing all the relationships between <span class="math notranslate nohighlight">\(w, r\)</span>, and s$.</p>
<p>Obviously the grass is more likely to be wet if either the sprinklers were on or it was raining. On any given day the sprinklers have probability 0.25 of being on, <span class="math notranslate nohighlight">\(P(s = 1) = 0.25\)</span>, while there is a probability 0.1 of rain, <span class="math notranslate nohighlight">\(P (r = 1) = 0.1\)</span>. The table then lists the conditional probabilities for the given being wet, given a rain and sprinkler condition for that day.</p>
<br/>
<div class="amsmath math notranslate nohighlight" id="equation-570a2f4e-68db-4702-9bc1-2937190a5638">
<span class="eqno">(121)<a class="headerlink" href="#equation-570a2f4e-68db-4702-9bc1-2937190a5638" title="Permalink to this equation">¶</a></span>\[\begin{matrix}
\hline
r &amp; s &amp; P(w=0|r,s) &amp; P(w=1|r,s)\\
\hline
0 &amp; 0  &amp; 0.999 &amp; 0.001 \\
0 &amp; 1 &amp; 0.1 &amp; 0.9 \\
1 &amp; 0 &amp; 0.01 &amp; 0.99 \\
1 &amp; 1 &amp; 0.001 &amp; 0.999 \\
\hline
\end{matrix}\]</div>
<br/>
<p>You come home and find that the the grass is wet, what is the probability the sprinklers were on today (you do not know if it was raining)?</p>
<p>We can start by writing out the joint probability:
<span class="math notranslate nohighlight">\(P(r,w,s)=P(w|r,s)P(r)P(s)\)</span></p>
<p>The conditional probability is then:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ccf1f774-54d1-47f0-afc7-770048d35a78">
<span class="eqno">(122)<a class="headerlink" href="#equation-ccf1f774-54d1-47f0-afc7-770048d35a78" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(s|w)=\frac{\sum_{r} P(w|s,r)P(s)  P(r)}{P(w)}=\frac{P(s) \sum_{r} P(w|s,r) P(r)}{P(w)}
\end{equation}\]</div>
<p>Note that we are summing over all possible conditions for <span class="math notranslate nohighlight">\(r\)</span> as we do not know if it was raining. Specifically, we want to know the probability of sprinklers having been on given the wet grass, <span class="math notranslate nohighlight">\(P(s=1|w=1)\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-64337f03-c0a8-44ed-9189-337ac0157efc">
<span class="eqno">(123)<a class="headerlink" href="#equation-64337f03-c0a8-44ed-9189-337ac0157efc" title="Permalink to this equation">¶</a></span>\[\begin{equation}
P(s=1|w=1) = \frac{P(s = 1)( P(w = 1|s = 1, r = 1) P(r = 1)+ P(w = 1|s = 1,r = 0)  P(r = 0))}{P(w = 1)} 
\end{equation}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight" id="equation-055f4065-23bd-49ad-8388-1cf6484bd620">
<span class="eqno">(124)<a class="headerlink" href="#equation-055f4065-23bd-49ad-8388-1cf6484bd620" title="Permalink to this equation">¶</a></span>\[\begin{eqnarray}
P(w=1)=P(s=1)( P(w=1|s=1,r=1 ) P(r=1) &amp;+ P(w=1|s=1,r=0)  P(r=0))\\
+P(s=0)( P(w=1|s=0,r=1 )  P(r=1) &amp;+ P(w=1|s=0,r=0)  P(r=0))\\
\end{eqnarray}\]</div>
<p>This code has been written out below, you just need to insert the right numbers from the table.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">##############################################################################</span>
<span class="c1"># TODO for student: Write code to insert the correct conditional probabilities</span>
<span class="c1"># from the table; see the comments to match variable with table entry.</span>
<span class="c1"># Comment out the line below to test your solution</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Finish the simulation code first"</span><span class="p">)</span>
<span class="c1">##############################################################################</span>

<span class="n">Pw1r1s1</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># the probability of wet grass given rain and sprinklers on</span>
<span class="n">Pw1r1s0</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># the probability of wet grass given rain and sprinklers off</span>
<span class="n">Pw1r0s1</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># the probability of wet grass given no rain and sprinklers on</span>
<span class="n">Pw1r0s0</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># the probability of wet grass given no rain and sprinklers off</span>
<span class="n">Ps</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># the probability of the sprinkler being on</span>
<span class="n">Pr</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># the probability of rain that day</span>


<span class="c1"># Calculate A and B</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">Ps</span> <span class="o">*</span> <span class="p">(</span><span class="n">Pw1r1s1</span> <span class="o">*</span> <span class="n">Pr</span> <span class="o">+</span> <span class="p">(</span><span class="n">Pw1r0s1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Pr</span><span class="p">))</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Ps</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Pw1r1s0</span> <span class="o">*</span><span class="n">Pr</span> <span class="o">+</span> <span class="p">(</span><span class="n">Pw1r0s0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Pr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Given that the grass is wet, the probability the sprinkler was on is: </span><span class="si">{</span><span class="n">A</span><span class="o">/</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">B</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>

<span class="n">Pw1r1s1</span> <span class="o">=</span> <span class="mf">0.999</span>  <span class="c1"># the probability of wet grass given rain and sprinklers on</span>
<span class="n">Pw1r1s0</span> <span class="o">=</span> <span class="mf">0.99</span>   <span class="c1"># the probability of wet grass given rain and sprinklers off</span>
<span class="n">Pw1r0s1</span> <span class="o">=</span> <span class="mf">0.9</span>    <span class="c1"># the probability of wet grass given no rain and sprinklers on</span>
<span class="n">Pw1r0s0</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># the probability of wet grass given no rain and sprinklers off</span>
<span class="n">Ps</span> <span class="o">=</span> <span class="mf">0.25</span>  <span class="c1"># the probability of the sprinkler being on</span>
<span class="n">Pr</span> <span class="o">=</span> <span class="mf">0.1</span>   <span class="c1"># the probability of rain that day</span>

<span class="c1"># Calculate A and B</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">Ps</span> <span class="o">*</span> <span class="p">(</span><span class="n">Pw1r1s1</span> <span class="o">*</span> <span class="n">Pr</span> <span class="o">+</span> <span class="p">(</span><span class="n">Pw1r0s1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Pr</span><span class="p">))</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Ps</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">Pw1r1s0</span> <span class="o">*</span><span class="n">Pr</span> <span class="o">+</span> <span class="p">(</span><span class="n">Pw1r0s0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Pr</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Given that the grass is wet, the probability the sprinkler was on is: </span><span class="si">{</span><span class="n">A</span><span class="o">/</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">B</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The probability you should get is about <span class="math notranslate nohighlight">\(0.7522\)</span>.</p>
<p>Your neighbour now tells you that it was indeed
raining today, <span class="math notranslate nohighlight">\(P(r=1) = 1\)</span>, so what is now the probability the sprinklers were on? Try changing the numbers above.</p>
</div>
<div class="section" id="bonus-think-causality-in-the-brain">
<h2>Bonus Think!: Causality in the Brain<a class="headerlink" href="#bonus-think-causality-in-the-brain" title="Permalink to this headline">¶</a></h2>
<p>In a causal stucture this is the correct way to calculate the probabilities. Do you think this is how the brain solves such problems? Would it be different for task involving novel stimuli (e.g., for someone with no previous exposure to sprinklers), as opposed to common stimuli?</p>
<p><strong>Main course preview:</strong> On W3D5 we will discuss causality further!</p>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W0D5_Statistics/instructor"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="W0D5_Tutorial1.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Tutorial 1: Probability Distributions</p>
</div>
</a>
<a class="right-next" href="W0D5_Outro.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Outro</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By Neuromatch<br>
<div class="extra_footer">
<div>
<a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png"/></a>
<a href="https://opensource.org/licenses/BSD-3-Clause"><img src="https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667"/></a>
The contents of this repository are shared under the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Software elements are additionally licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD (3-Clause) License</a>.
</div>
</div>
</br></p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>