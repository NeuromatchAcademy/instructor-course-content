
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Tutorial 1: Learning to Predict — Neuromatch Academy: Computational Neuroscience (instructor's version)</title>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/custom.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../_static/nma-logo-square-4xp.png" rel="shortcut icon">
<link href="../../../genindex.html" rel="index" title="Index"/>
<link href="../../../search.html" rel="search" title="Search"/>
<link href="W3D4_Tutorial2.html" rel="next" title="Tutorial 2: Learning to Act: Multi-Armed Bandits"/>
<link href="W3D4_Intro.html" rel="prev" title="Intro"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
<img alt="logo" class="logo" src="../../../_static/nma-logo-square-4xp.png"/>
<h1 class="site-logo" id="site-title">Neuromatch Academy: Computational Neuroscience (instructor's version)</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Schedule/schedule_intro.html">
   Schedule
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/daily_schedules.html">
     General schedule
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/shared_calendars.html">
     Shared calendars
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Schedule/timezone_widget.html">
     Timezone widget
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../TechnicalHelp/tech_intro.html">
   Technical Help
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../TechnicalHelp/Jupyterbook.html">
     Using jupyterbook
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_colab.html">
       Using Google Colab
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../TechnicalHelp/Tutorial_kaggle.html">
       Using Kaggle
      </a>
</li>
</ul>
</input></li>
<li class="toctree-l2">
<a class="reference internal" href="../../TechnicalHelp/Discord.html">
     Using discord
    </a>
</li>
</ul>
</input></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../TechnicalHelp/Links_Policy.html">
   Quick links and policies
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../prereqs/ComputationalNeuroscience.html">
   Prerequisites and preparatory materials for NMA Computational Neuroscience
  </a>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../tatraining/TA_Training_CN.html">
   TA Training - Computational Neuroscience
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Pre-reqs Refresher
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/chapter_title.html">
   Neuro Video Series (W0D0)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial1.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial2.html">
     Human Psychophysics
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial3.html">
     Behavioral Readout
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial4.html">
     Live in Lab
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial5.html">
     Brain Signals: Spiking Activity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial6.html">
     Brain Signals: LFP
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial7.html">
     Brain Signals: EEG &amp; MEG
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial8.html">
     Brain Signals: fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial9.html">
     Brain Signals: Calcium Imaging
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial10.html">
     Stimulus Representation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial11.html">
     Neurotransmitters
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D0_NeuroVideoSeries/instructor/W0D0_Tutorial12.html">
     Neurons to Consciousness
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/chapter_title.html">
   Python Workshop 1 (W0D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D1_PythonWorkshop1/instructor/W0D1_Tutorial1.html">
     Tutorial: LIF Neuron Part I
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/chapter_title.html">
   Python Workshop 2 (W0D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
<label for="toctree-checkbox-6">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D2_PythonWorkshop2/instructor/W0D2_Tutorial1.html">
     Tutorial 1: LIF Neuron Part II
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D3_LinearAlgebra/chapter_title.html">
   Linear Algebra (W0D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
<label for="toctree-checkbox-7">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial1.html">
     Tutorial 1: Vectors
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial2.html">
     Tutorial 2: Matrices
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Tutorial3.html">
     Bonus Tutorial: Discrete Dynamical Systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D3_LinearAlgebra/instructor/W0D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D4_Calculus/chapter_title.html">
   Calculus (W0D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
<label for="toctree-checkbox-8">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial1.html">
     Tutorial 1: Differentiation and Integration
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial2.html">
     Tutorial 2: Differential Equations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_Tutorial3.html">
     Tutorial 3: Numerical Methods
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D4_Calculus/instructor/W0D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W0D5_Statistics/chapter_title.html">
   Statistics (W0D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
<label for="toctree-checkbox-9">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_Tutorial1.html">
     Tutorial 1: Probability Distributions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_Tutorial2.html">
     Tutorial 2: Statistical Inference
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W0D5_Statistics/instructor/W0D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Intro to Modeling
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D1_ModelTypes/chapter_title.html">
   Model Types (W1D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
<label for="toctree-checkbox-10">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial1.html">
     Tutorial 1: “What” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial2.html">
     Tutorial 2: “How” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial3.html">
     Tutorial 3: “Why” models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Tutorial4.html">
     Tutorial 4: Model Discussions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D1_ModelTypes/instructor/W1D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D1_ModelingPractice/chapter_title.html">
   Modeling Practice (W2D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
<label for="toctree-checkbox-11">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Tutorial1.html">
     Tutorial 1: Framing the Question
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D1_ModelingPractice/instructor/W2D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D2_ModelFitting/chapter_title.html">
   Model Fitting (W1D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
<label for="toctree-checkbox-12">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial1.html">
     Tutorial 1: Linear regression with MSE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial2.html">
     Tutorial 2: Linear regression with MLE
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial3.html">
     Tutorial 3: Confidence intervals and bootstrapping
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial4.html">
     Tutorial 4: Multiple linear regression and polynomial regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial5.html">
     Tutorial 5: Model Selection: Bias-variance trade-off
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Tutorial6.html">
     Tutorial 6: Model Selection: Cross-validation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D2_ModelFitting/instructor/W1D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/chapter_title.html">
   Generalized Linear Models (W1D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
<label for="toctree-checkbox-13">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial1.html">
     Tutorial 1: GLMs for Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Tutorial2.html">
     Tutorial 2: Classifiers and regularizers
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D3_GeneralizedLinearModels/instructor/W1D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/chapter_title.html">
   Dimensionality Reduction (W1D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
<label for="toctree-checkbox-14">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial1.html">
     Tutorial 1: Geometric view of data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial2.html">
     Tutorial 2: Principal Component Analysis
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial3.html">
     Tutorial 3: Dimensionality Reduction &amp; Reconstruction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Tutorial4.html">
     Tutorial 4:  Nonlinear Dimensionality Reduction
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D4_DimensionalityReduction/instructor/W1D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W1D5_DeepLearning/chapter_title.html">
   Deep Learning (W1D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
<label for="toctree-checkbox-15">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial1.html">
     Tutorial 1: Decoding Neural Responses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial2.html">
     Tutorial 2: Convolutional Neural Networks
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial3.html">
     Tutorial 3: Building and Evaluating Normative Encoding Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Tutorial4.html">
     Bonus Tutorial: Diving Deeper into Decoding &amp; Encoding
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W1D5_DeepLearning/instructor/W1D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../Bonus_Autoencoders/chapter_title.html">
   Autoencoders (Bonus)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
<label for="toctree-checkbox-16">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial1.html">
     Tutorial 1: Intro to Autoencoders
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial2.html">
     Tutorial 2: Autoencoder extensions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Tutorial3.html">
     Tutorial 3: Autoencoders applications
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../Bonus_Autoencoders/instructor/Bonus_Outro.html">
     Outro
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/MachineLearning.html">
   Machine Learning Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Dynamical Systems
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D2_LinearSystems/chapter_title.html">
   Linear Systems (W2D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
<label for="toctree-checkbox-17">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial1.html">
     Tutorial 1: Linear dynamical systems
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial2.html">
     Tutorial 2: Markov Processes
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial3.html">
     Tutorial 3: Combining determinism and stochasticity
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Tutorial4.html">
     Tutorial 4: Autoregressive models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D2_LinearSystems/instructor/W2D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/chapter_title.html">
   Biological Neuron Models (W2D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
<label for="toctree-checkbox-18">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial1.html">
     Tutorial 1: The Leaky Integrate-and-Fire (LIF) Neuron Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial2.html">
     Tutorial 2: Effects of Input Correlation
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial3.html">
     Tutorial 3: Synaptic transmission - Models of static and dynamic synapses
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Tutorial4.html">
     Bonus Tutorial: Spike-timing dependent plasticity (STDP)
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D3_BiologicalNeuronModels/instructor/W2D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W2D4_DynamicNetworks/chapter_title.html">
   Dynamic Networks (W2D4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
<label for="toctree-checkbox-19">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial1.html">
     Tutorial 1: Neural Rate Models
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial2.html">
     Tutorial 2: Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Tutorial3.html">
     Bonus Tutorial: Extending the Wilson-Cowan Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W2D4_DynamicNetworks/instructor/W2D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/DynamicalSystems.html">
   Dynamical Systems Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Stochastic Processes
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D1_BayesianDecisions/chapter_title.html">
   Bayesian Decisions (W3D1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
<label for="toctree-checkbox-20">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial1.html">
     Tutorial 1: Bayes with a binary hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial2.html">
     Tutorial 2: Bayesian inference and decisions with continuous hidden state
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Tutorial3.html">
     Bonus Tutorial : Fitting to data
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D1_BayesianDecisions/instructor/W3D1_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D2_HiddenDynamics/chapter_title.html">
   Hidden Dynamics (W3D2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
<label for="toctree-checkbox-21">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial1.html">
     Tutorial 1: Sequential Probability Ratio Test
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial2.html">
     Tutorial 2: Hidden Markov Model
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial3.html">
     Tutorial 3: The Kalman Filter
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial4.html">
     Bonus Tutorial 4: The Kalman Filter, part 2
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Tutorial5.html">
     Bonus Tutorial 5: Expectation Maximization for spiking neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D2_HiddenDynamics/instructor/W3D2_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D3_OptimalControl/chapter_title.html">
   Optimal Control (W3D3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
<label for="toctree-checkbox-22">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Tutorial1.html">
     Tutorial 1: Optimal Control for Discrete States
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Tutorial2.html">
     Tutorial 2: Optimal Control for Continuous State
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D3_OptimalControl/instructor/W3D3_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="../chapter_title.html">
   Reinforcement Learning (W3D4)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
<label for="toctree-checkbox-23">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="W3D4_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
     Tutorial 1: Learning to Predict
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W3D4_Tutorial2.html">
     Tutorial 2: Learning to Act: Multi-Armed Bandits
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W3D4_Tutorial3.html">
     Tutorial 3: Learning to Act: Q-Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W3D4_Tutorial4.html">
     Tutorial 4: Model-Based Reinforcement Learning
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W3D4_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="W3D4_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../W3D5_NetworkCausality/chapter_title.html">
   Network Causality (W3D5)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
<label for="toctree-checkbox-24">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Intro.html">
     Intro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial1.html">
     Tutorial 1: Interventions
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial2.html">
     Tutorial 2: Correlations
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial3.html">
     Tutorial 3: Simultaneous fitting/regression
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Tutorial4.html">
     Tutorial 4: Instrumental Variables
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_Outro.html">
     Outro
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/further_reading.html">
     Suggested further readings
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../W3D5_NetworkCausality/instructor/W3D5_DaySummary.html">
     Day Summary
    </a>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../Module_WrapUps/StochasticProcesses.html">
   Stochastic Processes Wrap-Up
  </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Project Booklet
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/README.html">
   Introduction
  </a>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_guidance.html">
   Daily guide for projects
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/modelingsteps/intro.html">
   Modeling Step-by-Step Guide
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
<label for="toctree-checkbox-25">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_1through4.html">
     Modeling Steps 1 - 4
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/ModelingSteps_5through10.html">
     Modeling Steps 5 - 10
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionModel.html">
     Example Model Project: the Train Illusion
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/modelingsteps/TrainIllusionDataProject.html">
     Example Data Project: the Train Illusion
    </a>
</li>
</ul>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/datasets_overview.html">
   Datasets
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
<label for="toctree-checkbox-26">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/neurons.html">
     Neurons
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
<label for="toctree-checkbox-27">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/neurons/neurons_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/fMRI.html">
     fMRI
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
<label for="toctree-checkbox-28">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/fMRI/fMRI_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/ECoG.html">
     ECoG
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
<label for="toctree-checkbox-29">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/ECoG/ECoG_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/behavior.html">
     Behavior
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
<label for="toctree-checkbox-30">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/README.html">
       Guide
      </a>
</li>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/behavior/behavior_videos.html">
       Overview videos
      </a>
</li>
</ul>
</li>
<li class="toctree-l2 has-children">
<a class="reference internal" href="../../../projects/docs/theory.html">
     Theory
    </a>
<input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
<label for="toctree-checkbox-31">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l3">
<a class="reference internal" href="../../../projects/theory/README.html">
       Guide
      </a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1">
<a class="reference internal" href="../../../projects/docs/project_templates.html">
   Project Templates
  </a>
</li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../../projects/docs/project_2020_highlights.html">
   Projects 2020
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/>
<label for="toctree-checkbox-32">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/neurons.html">
     Neurons
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/theory.html">
     Theory
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/behavior.html">
     Behavior
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/fMRI.html">
     fMRI
    </a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../../projects/docs/projects_2020/eeg.html">
     EEG
    </a>
</li>
</ul>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
<button aria-label="Launch interactive content" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-rocket"></i>
</button>
<div class="menu-dropdown__content">
<ul>
</ul>
</div>
</div>
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="menu-dropdown menu-dropdown-repository-buttons">
<button aria-label="Source repositories" class="headerbtn menu-dropdown__trigger">
<i class="fab fa-github"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/instructor-course-content" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="headerbtn__text-container">repository</span>
</a>
</li>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/NeuromatchAcademy/instructor-course-content/issues/new?title=Issue%20on%20page%20%2Ftutorials/W3D4_ReinforcementLearning/instructor/W3D4_Tutorial1.html&amp;body=Your%20issue%20content%20here." title="Open an issue">
<span class="headerbtn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="headerbtn__text-container">open issue</span>
</a>
</li>
</ul>
</div>
</div>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../../../_sources/tutorials/W3D4_ReinforcementLearning/instructor/W3D4_Tutorial1.ipynb" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Learning to Predict
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial objectives
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure Settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions-and-classes">
     Helper Functions and Classes
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-temporal-difference-learning">
   Section 1: Temporal difference learning
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-introduction">
     Video 1: Introduction
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-td-learning-with-guaranteed-rewards">
     Coding Exercise 1: TD-learning with guaranteed rewards
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-1-1-us-to-cs-transfer">
     Interactive Demo 1.1: US to CS Transfer
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-1-2-learning-rates-and-discount-factors">
     Interactive Demo 1.2: Learning Rates and Discount Factors
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
</a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-td-learning-with-varying-reward-magnitudes">
   Section 2: TD-learning with varying reward magnitudes
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-2-match-the-value-functions">
     Interactive Demo 2: Match the Value Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-2-examining-the-td-error">
     Think! 2: Examining the TD Error
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-td-learning-with-probabilistic-rewards">
   Section 3: TD-learning with probabilistic rewards
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-3-probabilistic-rewards">
     Think! 3: Probabilistic rewards
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-think-1-removing-the-cs">
     Bonus Think! 1: Removing the CS
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>Tutorial 1: Learning to Predict</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
   Tutorial 1: Learning to Predict
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#tutorial-objectives">
   Tutorial objectives
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#figure-settings">
     Figure Settings
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#plotting-functions">
     Plotting Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#helper-functions-and-classes">
     Helper Functions and Classes
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-1-temporal-difference-learning">
   Section 1: Temporal difference learning
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#video-1-introduction">
     Video 1: Introduction
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#coding-exercise-1-td-learning-with-guaranteed-rewards">
     Coding Exercise 1: TD-learning with guaranteed rewards
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-1-1-us-to-cs-transfer">
     Interactive Demo 1.1: US to CS Transfer
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id1">
</a>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-1-2-learning-rates-and-discount-factors">
     Interactive Demo 1.2: Learning Rates and Discount Factors
    </a>
<ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry">
<a class="reference internal nav-link" href="#id2">
</a>
</li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-2-td-learning-with-varying-reward-magnitudes">
   Section 2: TD-learning with varying reward magnitudes
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#interactive-demo-2-match-the-value-functions">
     Interactive Demo 2: Match the Value Functions
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-2-examining-the-td-error">
     Think! 2: Examining the TD Error
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#section-3-td-learning-with-probabilistic-rewards">
   Section 3: TD-learning with probabilistic rewards
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#think-3-probabilistic-rewards">
     Think! 3: Probabilistic rewards
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#summary">
   Summary
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus">
   Bonus
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#bonus-think-1-removing-the-cs">
     Bonus Think! 1: Removing the CS
    </a>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<p><a href="https://colab.research.google.com/github/NeuromatchAcademy/course-content/blob/main/tutorials/W3D4_ReinforcementLearning/instructor/W3D4_Tutorial1.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/tutorials/W3D4_ReinforcementLearning/instructor/W3D4_Tutorial1.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<div class="section" id="tutorial-1-learning-to-predict">
<h1>Tutorial 1: Learning to Predict<a class="headerlink" href="#tutorial-1-learning-to-predict" title="Permalink to this headline">¶</a></h1>
<p><strong>Week 3, Day 4: Reinforcement Learning</strong></p>
<p><strong>By Neuromatch Academy</strong></p>
<p><strong>Content creators:</strong> Marcelo G Mattar, Eric DeWitt, Matt Krause, Matthew Sargent, Anoop Kulkarni, Sowmya Parthiban, Feryal Behbahani, Jane Wang</p>
<p><strong>Content reviewers:</strong> Ella Batty, Byron Galbraith, Michael Waskom, Ezekiel Williams, Mehul Rastogi, Lily Cheng, Roberto Guidotti, Arush Tagade, Kelson Shilling-Scrivo</p>
<p><strong>Production editors:</strong> Gagana B, Spiros Chavlis</p>
<p align="center"><img src="https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True"/></p></div>
<hr class="docutils"/>
<div class="section" id="tutorial-objectives">
<h1>Tutorial objectives<a class="headerlink" href="#tutorial-objectives" title="Permalink to this headline">¶</a></h1>
<p><em>Estimated timing of tutorial: 50 min</em></p>
<p>Reinforcement Learning (RL) is a framework for defining and solving a problem where an agent learns to take actions that maximize reward. The problem setting is as follows: an agent, biological or artificial, observes the current state of the world and selects an action based on that state. Upon executing an action, the agent receives a reward and uses this information to improve its future actions. Reinforcement learning provides formal, optimal descriptions of learning. These descriptions were first derived from studies of animal behavior and then validated when the formal quantities used in the model were observed in the brain in humans and animals.</p>
<p>Reinforcement learning is a broad framework and it has deep connections to many topics covered in NMA. For instance, most of reinforcement learning defines the world as a Markov Decision Problem, which is built on Hidden Dynamics and Optimal Control. More broadly, reinforcement learning can be seen as a framework that allows us to bring in many ideas and formalisms from other areas like economics, psychology, computer science, artificial intelligence, etc. to define algorithms or models that can solve large, complex problems with only a simple reward signal.</p>
<p>In this tutorial, we will model the agent as an observer that learns to predict future rewards. This agent takes no actions and thus cannot influence how much reward it receives. By predicting how much reward follows from each state, the agent can learn to identify the best states of the world – i.e. the ones that tend to be followed by the most reward.</p>
<p>More specifically, we will learn how to estimate state-value functions in a classical conditioning paradigm using Temporal Difference (TD) learning and examine TD-errors at the presentation of the conditioned and unconditioned stimulus (CS and US) under different CS-US contingencies. These exercises will provide you with an understanding of both how reward prediction errors (RPEs) behave in classical conditioning and what we should expect to see if Dopamine represents a “canonical” model-free RPE.</p>
<p>At the end of this tutorial:</p>
<ul class="simple">
<li><p>You will learn to use the standard tapped delay line conditioning model</p></li>
<li><p>You will understand how RPEs move to CS</p></li>
<li><p>You will understand how variability in reward size effects RPEs</p></li>
<li><p>You will understand how differences in US-CS timing effect RPEs</p></li>
</ul>
<p>Tutorials 1-4 should take a 3+ hours to complete, though it could take longer if you’re completely new to this material and/or if you are interested in studying all of the extra material.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Tutorial slides</span>
<span class="c1"># @markdown These are the slides for all videos in this tutorial.</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">IFrame</span>
<span class="n">link_id</span> <span class="o">=</span> <span class="s2">"2jzdu"</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"If you want to download the slides: https://osf.io/download/</span><span class="si">{</span><span class="n">link_id</span><span class="si">}</span><span class="s2">/"</span><span class="p">)</span>
<span class="n">IFrame</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="sa">f</span><span class="s2">"https://mfr.ca-1.osf.io/render?url=https://osf.io/</span><span class="si">{</span><span class="n">link_id</span><span class="si">}</span><span class="s2">/?direct%26mode=render%26action=download%26mode=render"</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">854</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">480</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="figure-settings">
<h2>Figure Settings<a class="headerlink" href="#figure-settings" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Figure Settings</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>  <span class="c1"># interactive display</span>
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/main/nma.mplstyle"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="plotting-functions">
<h2>Plotting Functions<a class="headerlink" href="#plotting-functions" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Plotting Functions</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">ticker</span>

<span class="k">def</span> <span class="nf">plot_value_function</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Plot V(s), the value function"""</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

  <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'State'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Value function: $V(s)$"</span><span class="p">)</span>

  <span class="k">if</span> <span class="n">show</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_tde_trace</span><span class="p">(</span><span class="n">TDE</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">skip</span><span class="o">=</span><span class="mi">400</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Plot the TD Error across trials"""</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="n">ax</span><span class="p">:</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>

  <span class="n">indx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">TDE</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">skip</span><span class="p">)</span>
  <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">TDE</span><span class="p">[:,</span><span class="n">indx</span><span class="p">])</span>
  <span class="n">positions</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_xticks</span><span class="p">()</span>
  <span class="c1"># Avoid warning when setting string tick labels</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">ticker</span><span class="o">.</span><span class="n">FixedLocator</span><span class="p">(</span><span class="n">positions</span><span class="p">))</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="n">skip</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">positions</span><span class="p">])</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'TD-error over learning'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'State'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Iterations'</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">figure</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">show</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">learning_summary_plot</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">TDE</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Summary plot for Ex1"""</span>
  <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">gridspec_kw</span><span class="o">=</span><span class="p">{</span><span class="s1">'height_ratios'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]})</span>

  <span class="n">plot_value_function</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="n">plot_tde_trace</span><span class="p">(</span><span class="n">TDE</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="helper-functions-and-classes">
<h2>Helper Functions and Classes<a class="headerlink" href="#helper-functions-and-classes" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Helper Functions and Classes</span>

<span class="k">def</span> <span class="nf">reward_guesser_title_hint</span><span class="p">(</span><span class="n">r1</span><span class="p">,</span> <span class="n">r2</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""""Provide a mildly obfuscated hint for a demo."""</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">r1</span><span class="o">==</span><span class="mi">14</span> <span class="ow">and</span> <span class="n">r2</span><span class="o">==</span><span class="mi">6</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">r1</span><span class="o">==</span><span class="mi">6</span> <span class="ow">and</span> <span class="n">r2</span><span class="o">==</span><span class="mi">14</span><span class="p">):</span>
    <span class="k">return</span> <span class="s2">"Technically correct...(the best kind of correct)"</span>

  <span class="k">if</span>  <span class="o">~</span><span class="p">(</span><span class="o">~</span><span class="p">(</span><span class="n">r1</span><span class="o">+</span><span class="n">r2</span><span class="p">)</span> <span class="o">^</span> <span class="mi">11</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">==</span> <span class="p">(</span><span class="mi">6</span> <span class="o">|</span> <span class="mi">24</span><span class="p">):</span> <span class="c1"># Don't spoil the fun :-)</span>
    <span class="k">return</span> <span class="s2">"Congratulations! You solved it!"</span>

  <span class="k">return</span> <span class="s2">"Keep trying...."</span>


<span class="k">class</span> <span class="nc">ClassicalConditioning</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">reward_magnitude</span><span class="p">,</span> <span class="n">reward_time</span><span class="p">):</span>

        <span class="c1"># Task variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">=</span> <span class="n">n_steps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cs_time</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_steps</span><span class="o">/</span><span class="mi">4</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="c1"># Reward variables</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_state</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_magnitude</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_probability</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_time</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Time step at which the conditioned stimulus is presented</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_reward</span><span class="p">(</span><span class="n">reward_magnitude</span><span class="p">,</span> <span class="n">reward_time</span><span class="p">)</span>

        <span class="c1"># Create a state dictionary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_create_state_dictionary</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">set_reward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward_magnitude</span><span class="p">,</span> <span class="n">reward_time</span><span class="p">):</span>

<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Determine reward state and magnitude of reward</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">reward_time</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">cs_time</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_magnitude</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_magnitude</span> <span class="o">=</span> <span class="n">reward_magnitude</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reward_state</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">reward_time</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">get_outcome</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_state</span><span class="p">):</span>

<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Determine next state and reward</span>
<span class="sd">        """</span>
        <span class="c1"># Update state</span>
        <span class="k">if</span> <span class="n">current_state</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="n">current_state</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Check for reward</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_state</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span><span class="n">current_state</span><span class="p">]:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_magnitude</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span>

    <span class="k">def</span> <span class="nf">_create_state_dictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        This dictionary maps number of time steps/ state identities</span>
<span class="sd">        in each episode to some useful state attributes:</span>

<span class="sd">        state      - 0 1 2 3 4 5 (cs) 6 7 8 9 10 11 12 ...</span>
<span class="sd">        is_delay   - 0 0 0 0 0 0 (cs) 1 1 1 1  1  1  1 ...</span>
<span class="sd">        t_in_delay - 0 0 0 0 0 0 (cs) 1 2 3 4  5  6  7 ...</span>
<span class="sd">        """</span>
        <span class="n">d</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_steps</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cs_time</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">d</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># Time in delay</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">d</span><span class="p">]</span>


<span class="k">class</span> <span class="nc">MultiRewardCC</span><span class="p">(</span><span class="n">ClassicalConditioning</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Classical conditioning paradigm, except that one randomly selected reward,</span>
<span class="sd">    magnitude, from a list, is delivered of a single fixed reward."""</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">reward_magnitudes</span><span class="p">,</span> <span class="n">reward_time</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">""""Build a multi-reward classical conditioning environment</span>
<span class="sd">      Args:</span>
<span class="sd">        - nsteps: Maximum number of steps</span>
<span class="sd">        - reward_magnitudes: LIST of possible reward magnitudes.</span>
<span class="sd">        - reward_time: Single fixed reward time</span>
<span class="sd">      Uses numpy global random state.</span>
<span class="sd">      """</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">reward_time</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reward_magnitudes</span> <span class="o">=</span> <span class="n">reward_magnitudes</span>

  <span class="k">def</span> <span class="nf">get_outcome</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_state</span><span class="p">):</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_outcome</span><span class="p">(</span><span class="n">current_state</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reward</span><span class="p">:</span>
      <span class="n">reward</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward_magnitudes</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span>


<span class="k">class</span> <span class="nc">ProbabilisticCC</span><span class="p">(</span><span class="n">ClassicalConditioning</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""Classical conditioning paradigm, except that rewards are stochastically omitted."""</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_steps</span><span class="p">,</span> <span class="n">reward_magnitude</span><span class="p">,</span> <span class="n">reward_time</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">p_reward</span><span class="o">=</span><span class="mf">0.75</span><span class="p">):</span>
<span class="w">    </span><span class="sd">""""Build a multi-reward classical conditioning environment</span>
<span class="sd">      Args:</span>
<span class="sd">        - nsteps: Maximum number of steps</span>
<span class="sd">        - reward_magnitudes: Reward magnitudes.</span>
<span class="sd">        - reward_time: Single fixed reward time.</span>
<span class="sd">        - p_reward: probability that reward is actually delivered in rewarding state</span>
<span class="sd">      Uses numpy global random state.</span>
<span class="sd">      """</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">reward_magnitude</span><span class="p">,</span> <span class="n">reward_time</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">p_reward</span> <span class="o">=</span> <span class="n">p_reward</span>

  <span class="k">def</span> <span class="nf">get_outcome</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_state</span><span class="p">):</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_outcome</span><span class="p">(</span><span class="n">current_state</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reward</span><span class="p">:</span>
      <span class="n">reward</span><span class="o">*=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">p_reward</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-1-temporal-difference-learning">
<h1>Section 1: Temporal difference learning<a class="headerlink" href="#section-1-temporal-difference-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="video-1-introduction">
<h2>Video 1: Introduction<a class="headerlink" href="#video-1-introduction" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_remove-input docutils container">
</div>
<p><strong>Environment:</strong></p>
<ul class="simple">
<li><p>The agent experiences the environment in episodes (also known as trials).</p></li>
<li><p>Episodes terminate by transitioning to the inter-trial-interval (ITI) state and they are initiated from the ITI state as well. We clamp the value of the terminal/ITI states to zero.</p></li>
<li><p>The classical conditioning environment is composed of a sequence of states that the agent deterministically transitions through. Starting at State 0, the agent moves to State 1 in the first step, from State 1 to State 2 in the second, and so on.  These states represent time in what is called “tapped delay line” representation</p></li>
<li><p>Within each episode, the agent is presented with a CS (cue) and US (reward).</p></li>
<li><p>The CS (cue) is presented at the end of the first quarter the trial. The US (reward) is delivered shortly afterwards. The interval between the CS and US is specified by <code class="docutils literal notranslate"><span class="pre">reward_time</span></code>.</p></li>
<li><p>The agent’s goal is to learn to predict expected rewards from each state in the trial.</p></li>
</ul>
<br/>
<p><strong>General concepts</strong></p>
<p>Reference: McClelland, J. L., Rumelhart, D. E. (1989). Explorations in parallel distributed processing: A handbook of models, programs, and exercises. Chapter 9, MIT press. url: <a class="reference external" href="https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html">web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html</a></p>
<br/>
<ul class="simple">
<li><p>Return <span class="math notranslate nohighlight">\(G_{t}\)</span>: future cumulative reward at time <span class="math notranslate nohighlight">\(t\)</span>:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-70036c5b-b64c-4f5c-b96b-717b87dd4b09">
<span class="eqno">(429)<a class="headerlink" href="#equation-70036c5b-b64c-4f5c-b96b-717b87dd4b09" title="Permalink to this equation">¶</a></span>\[\begin{equation}
G_{t} = \sum \limits_{k = 0}^{\infty} \gamma^{k} r_{t+k+1}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(r_{t}\)</span> is the amount of reward received at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(\gamma \in [0, 1]\)</span> is a discount factor that specifies the relevance in the present of future rewards.
Note that the return <span class="math notranslate nohighlight">\(G_{t}\)</span> can be written in a recursive form:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e7c98ede-8972-4003-9dbd-975112626058">
<span class="eqno">(430)<a class="headerlink" href="#equation-e7c98ede-8972-4003-9dbd-975112626058" title="Permalink to this equation">¶</a></span>\[\begin{equation}
G_{t} = r_{t+1} + \gamma G_{t+1}
\end{equation}\]</div>
<ul class="simple">
<li><p>State <span class="math notranslate nohighlight">\(s\)</span> describes the current state or situation, typically obtained from observations that the agent receives from the environment.</p></li>
<li><p>Policy <span class="math notranslate nohighlight">\(\pi\)</span> is a specification of how the agent acts. <span class="math notranslate nohighlight">\(\pi(a|s)\)</span> gives the probability of taking action <span class="math notranslate nohighlight">\(a\)</span> when in state <span class="math notranslate nohighlight">\(s\)</span>.</p></li>
<li><p>The value function <span class="math notranslate nohighlight">\(V_{\pi}(s_t=s)\)</span> is defined as the expected return starting with state <span class="math notranslate nohighlight">\(s\)</span> and successively following policy <span class="math notranslate nohighlight">\(\pi\)</span>. Roughly speaking, the value function estimates “how good” it is to be in state <span class="math notranslate nohighlight">\(s\)</span> when following policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-a693962d-73ab-4589-bab5-79a70bf3b77e">
<span class="eqno">(431)<a class="headerlink" href="#equation-a693962d-73ab-4589-bab5-79a70bf3b77e" title="Permalink to this equation">¶</a></span>\[\begin{align}
V_{\pi}(s_t=s) &amp;= \mathbb{E} [ G_{t}\; | \; s_t=s, a_{t:\infty}\sim\pi] \\
&amp; = \mathbb{E} [ r_{t+1} + \gamma G_{t+1}\; | \; s_t=s, a_{t:\infty}\sim\pi]
\end{align}\]</div>
<ul class="simple">
<li><p>Combining the above, we have:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-134dba40-f39f-4470-a13e-1780e1d10ac4">
<span class="eqno">(432)<a class="headerlink" href="#equation-134dba40-f39f-4470-a13e-1780e1d10ac4" title="Permalink to this equation">¶</a></span>\[\begin{align}
V_{\pi}(s_t=s) &amp;= \mathbb{E} [ r_{t+1} + \gamma V_{\pi}(s_{t+1})\; | \; s_t=s, a_{t:\infty}\sim\pi] \\
&amp;= \sum_a \pi(a|s) \sum_{r, s'}p(s', r)(r + V_{\pi}(s_{t+1}=s'))
\end{align}\]</div>
<br/>
<p><strong>Temporal difference (TD) learning</strong></p>
<ul class="simple">
<li><p>With a <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_property">Markovian assumption</a>, we can use <span class="math notranslate nohighlight">\(V(s_{t+1})\)</span> as a proxy for the true value of the return <span class="math notranslate nohighlight">\(G_{t+1}\)</span>. Thus, we obtain a generalized equation to calculate the TD-error:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-c6b6040f-39ee-4f28-9c22-21f62fbb61bf">
<span class="eqno">(433)<a class="headerlink" href="#equation-c6b6040f-39ee-4f28-9c22-21f62fbb61bf" title="Permalink to this equation">¶</a></span>\[\begin{align}
\delta_{t} = r_{t+1} + \gamma V(s_{t+1}) - V(s_{t})
\end{align}\]</div>
<ul class="simple">
<li><p>The TD-error measures the discrepancy between the values at time <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(t+1\)</span>. Once the TD-error is calculated, we can perform a “value update” to to reduce the value discrepancy:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-f20877c7-d475-475d-8e98-13d6de4631a6">
<span class="eqno">(434)<a class="headerlink" href="#equation-f20877c7-d475-475d-8e98-13d6de4631a6" title="Permalink to this equation">¶</a></span>\[\begin{align}
V(s_{t}) \leftarrow V(s_{t}) + \alpha \delta_{t}
\end{align}\]</div>
<ul class="simple">
<li><p>The speed by which the discrepancy is reduced is specified by a constant (aka hyperparameter) <span class="math notranslate nohighlight">\(\alpha\)</span>, called learning rate.</p></li>
</ul>
<br/>
<p><strong>Definitions (tl;dr):</strong></p>
<ul class="simple">
<li><p>Return:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-c27b6a9b-8b64-4846-b3bb-e2647e38c142">
<span class="eqno">(435)<a class="headerlink" href="#equation-c27b6a9b-8b64-4846-b3bb-e2647e38c142" title="Permalink to this equation">¶</a></span>\[\begin{equation}
G_{t} = \sum \limits_{k = 0}^{\infty} \gamma^{k} r_{t+k+1} = r_{t+1} + \gamma G_{t+1}
\end{equation}\]</div>
<ul class="simple">
<li><p>TD-error:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-52e64114-b367-46a7-9911-59d5a427818e">
<span class="eqno">(436)<a class="headerlink" href="#equation-52e64114-b367-46a7-9911-59d5a427818e" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\delta_{t} = r_{t+1} + \gamma V(s_{t+1}) - V(s_{t})
\end{equation}\]</div>
<ul class="simple">
<li><p>Value updates:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-bcdfbc30-b753-41cf-a235-1a3d04a12bdd">
<span class="eqno">(437)<a class="headerlink" href="#equation-bcdfbc30-b753-41cf-a235-1a3d04a12bdd" title="Permalink to this equation">¶</a></span>\[\begin{equation}
V(s_{t}) \leftarrow V(s_{t}) + \alpha \delta_{t}
\end{equation}\]</div>
</div>
<div class="section" id="coding-exercise-1-td-learning-with-guaranteed-rewards">
<h2>Coding Exercise 1: TD-learning with guaranteed rewards<a class="headerlink" href="#coding-exercise-1-td-learning-with-guaranteed-rewards" title="Permalink to this headline">¶</a></h2>
<p>In this exercise, you will implement TD-learning to estimate the state-value function in the classical conditioning paradigm. Rewards have fixed magnitude and are delivered at a fixed delay after the conditioned stimulus, CS. You should save the TD-errors over learning (i.e., over trials) so we can visualize them afterwards.</p>
<p>In order to simulate the effect of the CS, you should update <span class="math notranslate nohighlight">\(V(s_{t})\)</span> only during the delay period after CS. This period is indicated by the boolean variable <code class="docutils literal notranslate"><span class="pre">is_delay</span></code>. This can be implemented by multiplying the expression for updating the value function by <code class="docutils literal notranslate"><span class="pre">is_delay</span></code>.</p>
<p>Use the provided code to estimate the value function. We will use helper class <code class="docutils literal notranslate"><span class="pre">ClassicalConditioning</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">td_learner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Temporal Difference learning</span>

<span class="sd">  Args:</span>
<span class="sd">    env (object): the environment to be learned</span>
<span class="sd">    n_trials (int): the number of trials to run</span>
<span class="sd">    gamma (float): temporal discount factor</span>
<span class="sd">    alpha (float): learning rate</span>

<span class="sd">  Returns:</span>
<span class="sd">    ndarray, ndarray: the value function and temporal difference error arrays</span>
<span class="sd">  """</span>
  <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">n_steps</span><span class="p">)</span> <span class="c1"># Array to store values over states (time)</span>
  <span class="n">TDE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">env</span><span class="o">.</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">))</span> <span class="c1"># Array to store TD errors</span>

  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>

    <span class="n">state</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Initial state</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">n_steps</span><span class="p">):</span>

      <span class="c1"># Get next state and next reward</span>
      <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_outcome</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

      <span class="c1"># Is the current state in the delay period (after CS)?</span>
      <span class="n">is_delay</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

      <span class="c1">########################################################################</span>
      <span class="c1">## TODO for students: implement TD error and value function update</span>
      <span class="c1"># Fill out function and remove</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Student exercise: implement TD error and value function update"</span><span class="p">)</span>
      <span class="c1">#################################################################################</span>
      <span class="c1"># Write an expression to compute the TD-error</span>
      <span class="n">TDE</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span>

      <span class="c1"># Write an expression to update the value function</span>
      <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="o">...</span>

      <span class="c1"># Update state</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

  <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">TDE</span>


<span class="c1"># Initialize classical conditioning class</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">ClassicalConditioning</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">reward_magnitude</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">reward_time</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Perform temporal difference learning</span>
<span class="n">V</span><span class="p">,</span> <span class="n">TDE</span> <span class="o">=</span> <span class="n">td_learner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="n">learning_summary_plot</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">TDE</span><span class="p">)</span>

</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove solution</span>
<span class="k">def</span> <span class="nf">td_learner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
<span class="w">  </span><span class="sd">""" Temporal Difference learning</span>

<span class="sd">  Args:</span>
<span class="sd">    env (object): the environment to be learned</span>
<span class="sd">    n_trials (int): the number of trials to run</span>
<span class="sd">    gamma (float): temporal discount factor</span>
<span class="sd">    alpha (float): learning rate</span>

<span class="sd">  Returns:</span>
<span class="sd">    ndarray, ndarray: the value function and temporal difference error arrays</span>
<span class="sd">  """</span>
  <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">n_steps</span><span class="p">)</span> <span class="c1"># Array to store values over states (time)</span>
  <span class="n">TDE</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">env</span><span class="o">.</span><span class="n">n_steps</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">))</span> <span class="c1"># Array to store TD errors</span>

  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>

    <span class="n">state</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Initial state</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">n_steps</span><span class="p">):</span>

      <span class="c1"># Get next state and next reward</span>
      <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_outcome</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

      <span class="c1"># Is the current state in the delay period (after CS)?</span>
      <span class="n">is_delay</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">state_dict</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

      <span class="c1"># Write an expression to compute the TD-error</span>
      <span class="n">TDE</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">])</span>

      <span class="c1"># Write an expression to update the value function</span>
      <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">TDE</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">n</span><span class="p">]</span> <span class="o">*</span> <span class="n">is_delay</span>

      <span class="c1"># Update state</span>
      <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

  <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">TDE</span>


<span class="c1"># Initialize classical conditioning class</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">ClassicalConditioning</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">reward_magnitude</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">reward_time</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Perform temporal difference learning</span>
<span class="n">V</span><span class="p">,</span> <span class="n">TDE</span> <span class="o">=</span> <span class="n">td_learner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>

<span class="c1"># Visualize</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">learning_summary_plot</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">TDE</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="interactive-demo-1-1-us-to-cs-transfer">
<h2>Interactive Demo 1.1: US to CS Transfer<a class="headerlink" href="#interactive-demo-1-1-us-to-cs-transfer" title="Permalink to this headline">¶</a></h2>
<p>During classical conditioning, the subject’s behavioral response (e.g., salivating) transfers from the unconditioned stimulus (US; like the smell of tasty food) to the conditioned stimulus (CS; like Pavlov ringing his bell) that predicts it. Reward prediction errors play an important role in this process by adjusting the value of states according to their expected, discounted return.</p>
<p>Recall that TD-errors are given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e437ef6e-4534-4116-8c02-6a27363a1326">
<span class="eqno">(438)<a class="headerlink" href="#equation-e437ef6e-4534-4116-8c02-6a27363a1326" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\delta_{t} = r_{t+1} + \gamma V(s_{t+1}) - V(s_{t})
\end{equation}\]</div>
<p>The delay period has zero reward, so throughout the learning phase, the TD-errors result from inconsistencies between <span class="math notranslate nohighlight">\(V(s_{t+1})\)</span> and <span class="math notranslate nohighlight">\(V(s_{t})\)</span> (note that the discount factor is set to zero in this example). The TD-errors for a given time point diminish once <span class="math notranslate nohighlight">\(V(s_{t})\)</span> approaches <span class="math notranslate nohighlight">\(V(s_{t+1})\)</span>, but that causes the TD-error for the preceding time point to increase. Thus, throughout learning, the TD-errors will tend to move backwards in time.</p>
<p>Use the widget below to examine how reward prediction errors change over time.</p>
<p>Before training (orange line), only the reward state has high reward prediction error (blue line). As training progresses (slider), the reward prediction errors shift to the conditioned stimulus, where they end up when the trial is complete (green line).</p>
<p>Dopamine neurons, which are thought to carry reward prediction errors <em>in vivo</em>, show exactly the same behavior!</p>
<div class="section" id="id1">
<h3><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Make sure you execute this cell to enable the widget!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>

<span class="c1">#@markdown Make sure you execute this cell to enable the widget!</span>

<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">20000</span>

<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_tde_by_trial</span><span class="p">(</span><span class="n">trial</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">n_trials</span><span class="o">-</span><span class="mi">1</span> <span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"Trial #"</span><span class="p">)):</span>
  <span class="k">if</span> <span class="s1">'TDE'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="nb">globals</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Complete Exercise 1 to enable this interactive demo!"</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span> <span class="c1"># Use this + basefmt=' ' to keep the legend clean.</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">TDE</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">linefmt</span><span class="o">=</span><span class="s1">'C1-'</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">'C1d'</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s1">' '</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">"Before Learning (Trial 0)"</span><span class="p">,</span>
            <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">TDE</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">linefmt</span><span class="o">=</span><span class="s1">'C2-'</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">'C2s'</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s1">' '</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">"After Learning (Trial $\infty$)"</span><span class="p">,</span>
            <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">TDE</span><span class="p">[:,</span> <span class="n">trial</span><span class="p">],</span> <span class="n">linefmt</span><span class="o">=</span><span class="s1">'C0-'</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">'C0o'</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s1">' '</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">"Trial </span><span class="si">{</span><span class="n">trial</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
            <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"State in trial"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"TD Error"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Temporal Difference Error by Trial"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="interactive-demo-1-2-learning-rates-and-discount-factors">
<h2>Interactive Demo 1.2: Learning Rates and Discount Factors<a class="headerlink" href="#interactive-demo-1-2-learning-rates-and-discount-factors" title="Permalink to this headline">¶</a></h2>
<p>Our TD-learning agent has two parameters that control how it learns: <span class="math notranslate nohighlight">\(\alpha\)</span>, the learning rate, and <span class="math notranslate nohighlight">\(\gamma\)</span>, the discount factor. In Exercise 1, we set these parameters to <span class="math notranslate nohighlight">\(\alpha=0.001\)</span> and <span class="math notranslate nohighlight">\(\gamma=0.98\)</span> for you. Here, you’ll investigate how changing these parameters alters the model that TD-learning learns.</p>
<p>Before enabling the interactive demo below, take a moment to think about the functions of these two parameters. <span class="math notranslate nohighlight">\(\alpha\)</span> controls the size of the Value function updates produced by each TD-error. In our simple, deterministic world, will this affect the final model we learn? Is a larger <span class="math notranslate nohighlight">\(\alpha\)</span> necessarily better in more complex, realistic environments?</p>
<p>The discount rate <span class="math notranslate nohighlight">\(\gamma\)</span> applies an exponentially-decaying weight to returns occuring in the future, rather than the present timestep. How does this affect the model we learn? What happens when <span class="math notranslate nohighlight">\(\gamma=0\)</span> or <span class="math notranslate nohighlight">\(\gamma \geq 1\)</span>?</p>
<p>Use the widget to test your hypotheses.</p>
<div class="section" id="id2">
<h3><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Make sure you execute this cell to enable the widget!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title</span>

<span class="c1">#@markdown Make sure you execute this cell to enable the widget!</span>

<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">plot_summary_alpha_gamma</span><span class="p">(</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">readout_format</span><span class="o">=</span><span class="s1">'.4f'</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"alpha"</span><span class="p">),</span>
                             <span class="n">gamma</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mf">0.980</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.1</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.010</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"gamma"</span><span class="p">)):</span>
  <span class="n">env</span> <span class="o">=</span> <span class="n">ClassicalConditioning</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">reward_magnitude</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">reward_time</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">V_params</span><span class="p">,</span> <span class="n">TDE_params</span> <span class="o">=</span> <span class="n">td_learner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">gamma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Finish Exercise 1 to enable this interactive demo"</span><span class="p">)</span>

  <span class="n">learning_summary_plot</span><span class="p">(</span><span class="n">V_params</span><span class="p">,</span><span class="n">TDE_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">Alpha determines how fast the model learns. In the simple, deterministic world</span>
<span class="sd">we're using here, this allows the model to quickly converge onto the "true"</span>
<span class="sd">model that heavily values the conditioned stimulus. In more complex environments,</span>
<span class="sd">however, excessively large values of alpha can slow, or even prevent, learning,</span>
<span class="sd">as we'll see later.</span>

<span class="sd">Gamma effectively controls how much the model cares about the future: larger values of</span>
<span class="sd">gamma cause the model to weigh future rewards nearly as much as present ones. At gamma=1,</span>
<span class="sd">the model weights all rewards, regardless of when they occur, equally and when greater than one, it</span>
<span class="sd">starts to *prefer* rewards in the future, rather than the present (this is rarely good).</span>
<span class="sd">When gamma=0, however, the model becomes greedy and only considers rewards that</span>
<span class="sd">can be obtained immediately.</span>
<span class="sd"> """</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-2-td-learning-with-varying-reward-magnitudes">
<h1>Section 2: TD-learning with varying reward magnitudes<a class="headerlink" href="#section-2-td-learning-with-varying-reward-magnitudes" title="Permalink to this headline">¶</a></h1>
<p><em>Estimated timing to here from start of tutorial: 30 min</em></p>
<p>In the previous exercise, the environment was as simple as possible. On every trial, the CS predicted the same reward, at the same time, with 100% certainty. In the next few exercises, we will make the environment more progressively more complicated and examine the TD-learner’s behavior.</p>
<div class="section" id="interactive-demo-2-match-the-value-functions">
<h2>Interactive Demo 2: Match the Value Functions<a class="headerlink" href="#interactive-demo-2-match-the-value-functions" title="Permalink to this headline">¶</a></h2>
<p>First, will replace the environment with one that dispenses one of several rewards, chosen at random. Shown below is the final value function <span class="math notranslate nohighlight">\(V\)</span> for a TD learner that was trained in an environment where the CS predicted a reward of 6 or 14 units; both rewards were equally likely).</p>
<p>Can you find another pair of rewards that cause the agent to learn the same value function? Assume each reward will be dispensed 50% of the time.</p>
<p>Hints:</p>
<ul class="simple">
<li><p>Carefully consider the definition of the value function <span class="math notranslate nohighlight">\(V\)</span>. This can be solved analytically.</p></li>
<li><p>There is no need to change <span class="math notranslate nohighlight">\(\alpha\)</span> or <span class="math notranslate nohighlight">\(\gamma\)</span>.</p></li>
<li><p>Due to the randomness, there may be a small amount of variation.</p></li>
</ul>
<p>Make sure you execute this cell to enable the widget! Please allow some time for the new figure to load</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Make sure you execute this cell to enable the widget! Please allow some time for the new figure to load</span>

<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2020</span><span class="p">)</span>
<span class="n">rng_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">MultiRewardCC</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">14</span><span class="p">],</span> <span class="n">reward_time</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">V_multi</span><span class="p">,</span> <span class="n">TDE_multi</span> <span class="o">=</span> <span class="n">td_learner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="nd">@widgets</span><span class="o">.</span><span class="n">interact</span>
<span class="k">def</span> <span class="nf">reward_guesser_interaction</span><span class="p">(</span><span class="n">r1</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">IntText</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"Reward 1"</span><span class="p">),</span>
                               <span class="n">r2</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">IntText</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">"Reward 2"</span><span class="p">)):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">env2</span> <span class="o">=</span> <span class="n">MultiRewardCC</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="p">[</span><span class="n">r1</span><span class="p">,</span> <span class="n">r2</span><span class="p">],</span> <span class="n">reward_time</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">V_guess</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">td_learner</span><span class="p">(</span><span class="n">env2</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">V_multi</span><span class="p">,</span> <span class="n">linefmt</span><span class="o">=</span><span class="s1">'y-'</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">'yo'</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s1">' '</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Target"</span><span class="p">,</span>
            <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">set_markersize</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">set_markerfacecolor</span><span class="p">(</span><span class="s1">'none'</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">set_linewidth</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">V_guess</span><span class="p">,</span> <span class="n">linefmt</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">markerfmt</span><span class="o">=</span><span class="s1">'rx'</span><span class="p">,</span> <span class="n">basefmt</span><span class="o">=</span><span class="s1">' '</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Guess"</span><span class="p">,</span>
                      <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">set_markersize</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">"State"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">"Value"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">"Guess V(s)</span><span class="se">\n</span><span class="s2">"</span> <span class="o">+</span> <span class="n">reward_guesser_title_hint</span><span class="p">(</span><span class="n">r1</span><span class="p">,</span> <span class="n">r2</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Please finish Exercise 1 first!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="think-2-examining-the-td-error">
<h2>Think! 2: Examining the TD Error<a class="headerlink" href="#think-2-examining-the-td-error" title="Permalink to this headline">¶</a></h2>
<p>Run the cell below to plot the TD errors from our multi-reward environment. A new feature appears in this plot? What is it? Why does it happen?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_tde_trace</span><span class="p">(</span><span class="n">TDE_multi</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">The TD trace now takes on negative values because the reward delivered is</span>
<span class="sd">sometimes larger than the expected reward and sometimes smaller.</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="section-3-td-learning-with-probabilistic-rewards">
<h1>Section 3: TD-learning with probabilistic rewards<a class="headerlink" href="#section-3-td-learning-with-probabilistic-rewards" title="Permalink to this headline">¶</a></h1>
<p><em>Estimated timing to here from start of tutorial: 40 min</em></p>
<div class="section" id="think-3-probabilistic-rewards">
<h2>Think! 3: Probabilistic rewards<a class="headerlink" href="#think-3-probabilistic-rewards" title="Permalink to this headline">¶</a></h2>
<p>In this environment, we’ll return to delivering a single reward of ten units. However, it will be delivered intermittently: on 20 percent of trials, the CS will be shown but the agent will not receive the usual reward; the remaining 80% will proceed as usual.</p>
<p>Run the cell below to simulate. Recall that earlier in the notebook, we saw that changing <span class="math notranslate nohighlight">\(\alpha\)</span> had little effect on learning in a deterministic environment. In the simulation below, <span class="math notranslate nohighlight">\(\alpha\)</span> is set to 1. What happens when the learning rate is set to such a large value in a probability reward setting? Does it seem like it will <em>ever</em> converge?</p>
<p>With a high learning rate, the value function tracks each observed reward, changing quickly whenever there is a reward prediction error. In a probabilistic scenario case, this behavior results in the value function changing too quickly and never stabilizing (converging). Using a low learning rate can stabilize the value function by smoothing out any variation in the reward signal, leading the value function to converge to the average reward over time. However, using a low learning rate can result in slow learning.</p>
<p>To get the best of all worls, it is often useful to use a high learning rate early on (producing fast learning), and to reduce the learning rate gradually throughout learning (so that the value function converges to the average reward). This is sometimes called “learning rate schedule”.</p>
<p>Execute this cell to visualize the value function and TD-errors when <code class="docutils literal notranslate"><span class="pre">alpha=1</span></code></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to visualize the value function and TD-errors when `alpha=1`</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">rng_state</span><span class="p">)</span>  <span class="c1"># Resynchronize everyone's notebooks</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="k">try</span><span class="p">:</span>
  <span class="n">env</span> <span class="o">=</span> <span class="n">ProbabilisticCC</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">reward_magnitude</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">reward_time</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">p_reward</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
  <span class="n">V_stochastic</span><span class="p">,</span> <span class="n">TDE_stochastic</span> <span class="o">=</span> <span class="n">td_learner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">learning_summary_plot</span><span class="p">(</span><span class="n">V_stochastic</span><span class="p">,</span> <span class="n">TDE_stochastic</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Please finish Exercise 1 first"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Execute this cell to visualize the value function and TD-errors when <code class="docutils literal notranslate"><span class="pre">alpha=0.2</span></code></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Execute this cell to visualize the value function and TD-errors when `alpha=0.2`</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">rng_state</span><span class="p">)</span>  <span class="c1"># Resynchronize everyone's notebooks</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="k">try</span><span class="p">:</span>
  <span class="n">env</span> <span class="o">=</span> <span class="n">ProbabilisticCC</span><span class="p">(</span><span class="n">n_steps</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">reward_magnitude</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">reward_time</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">p_reward</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
  <span class="n">V_stochastic</span><span class="p">,</span> <span class="n">TDE_stochastic</span> <span class="o">=</span> <span class="n">td_learner</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.2</span><span class="p">)</span>
  <span class="n">learning_summary_plot</span><span class="p">(</span><span class="n">V_stochastic</span><span class="p">,</span> <span class="n">TDE_stochastic</span><span class="p">)</span>
<span class="k">except</span> <span class="ne">NotImplementedError</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">"Please finish Exercise 1 first"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">The multi-reward and probabilistic reward environments are the same. You</span>
<span class="sd">could simulate a probabilistic reward of 10 units, delivered 50% of the time,</span>
<span class="sd">by having a mixture of 10 and 0 unit rewards, or vice versa. The take home message</span>
<span class="sd">from these last three exercises is that the *average* or expected reward is</span>
<span class="sd">what matters during TD learning.</span>

<span class="sd">Large values of alpha prevent the TD Learner from converging. As a result, the</span>
<span class="sd">value function seems implausible: one state may have an extremely low value while</span>
<span class="sd">the neighboring ones remain high. This pattern persists even if training continues</span>
<span class="sd">for hundreds of thousands of trials.</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p><em>Estimated timing of tutorial: 50 min</em></p>
<p>In this notebook, we have developed a simple TD Learner and examined how its state representations and reward prediction errors evolve during training. By manipulating its environment and parameters (<span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\gamma\)</span>), you developed an intuition for how it behaves.</p>
<p>This simple model closely resembles the behavior of subjects undergoing classical conditioning tasks and the dopamine neurons that may underlie that behavior. You may have implemented TD-reset or used the model to recreate a common experimental error. The update rule used here has been extensively studied for <a class="reference external" href="https://www.pnas.org/content/108/Supplement_3/15647">more than 70 years</a> as a possible explanation for artificial and biological learning.</p>
<p>However, you may have noticed that something is missing from this notebook. We carefully calculated the value of each state, but did not use it to actually do anything. Using values to plan <em><strong>Actions</strong></em> is coming up next!</p>
</div>
<hr class="docutils"/>
<div class="section" id="bonus">
<h1>Bonus<a class="headerlink" href="#bonus" title="Permalink to this headline">¶</a></h1>
<div class="section" id="bonus-think-1-removing-the-cs">
<h2>Bonus Think! 1: Removing the CS<a class="headerlink" href="#bonus-think-1-removing-the-cs" title="Permalink to this headline">¶</a></h2>
<p>In Coding Exercise 1, you (should have) included a term that depends on the conditioned stimulus. Remove it and see what happens. Do you understand why?
This phenomena often fools people attempting to train animals–beware!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># to_remove explanation</span>

<span class="sd">"""</span>
<span class="sd">You should only be updating the V[state] once the conditioned stimulus appears.</span>

<span class="sd">If you remove this term the Value Function becomes periodic, dropping towards zero</span>
<span class="sd">right after the reward and gradually rising towards the end of the trial. This</span>
<span class="sd">behavior is actually correct, because the model is learning the time until the</span>
<span class="sd">*next* reward, and State 37 is closer to a reward than State 21 or 22.</span>

<span class="sd">In an actual experiment, the animal often just wants rewards; it doesn't care about</span>
<span class="sd">your experiment or trial structure!</span>
<span class="sd">"""</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./tutorials/W3D4_ReinforcementLearning/instructor"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="W3D4_Intro.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title">Intro</p>
</div>
</a>
<a class="right-next" href="W3D4_Tutorial2.html" id="next-link" title="next page">
<div class="prev-next-info">
<p class="prev-next-subtitle">next</p>
<p class="prev-next-title">Tutorial 2: Learning to Act: Multi-Armed Bandits</p>
</div>
<i class="fas fa-angle-right"></i>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
  
    By Neuromatch<br>
<div class="extra_footer">
<div>
<a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png"/></a>
<a href="https://opensource.org/licenses/BSD-3-Clause"><img src="https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667"/></a>
The contents of this repository are shared under the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Software elements are additionally licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD (3-Clause) License</a>.
</div>
</div>
</br></p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>